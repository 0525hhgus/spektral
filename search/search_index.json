{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WARNING Spektral is still a work in progress and may change substantially before the first proper release. The API is not mature enough to be considered stable, but we'll try to keep breaking changes to a minimum. Drop me an email if you want to help out with the development. Welcome to Spektral Spektral is a framework for relational representation learning, built in Python and based on the Keras API. The main purpose of this project is to provide a simple, fast, and scalable environment for fast experimentation. Spektral contains a wide set of tools to build graph neural networks, and implements some of the most popular layers for graph deep learning so that you only need to worry about creating your models. Spektral is an open source project available on Github . Read the documentation on the official website . Relational Representation Learning An important trait of human intelligence is the ability to model the world in terms of entities and relations, exploiting the knowledge that entities are connected (called the relational inductive bias ) in order to make informed decisions. This relational representation of the world allows us to model a great deal of phenomena using graphs, from social networks to sentences, from the interactions of bodies in a gravitational field to the bonds between atoms in a molecule. Given their flexibility for representing knowledge in a structured way, graphs have been historically ubiquitous in computer science, and early efforts in artificial intelligence relied heavily on relational representations (e.g., Monte Carlo methods, Bayesian networks, etc.). Modern machine learning approaches, on the other hand, seemed to have diverted from such hand-engineered representations in favour of learning from raw data, with deep learning methods leading the way on the quasi-totality of learning tasks. However, by placing artificial intelligence in the framework of relational inductive bias, as proposed by Battaglia et al. , we see that even the most modern deep learning methods are designed to exploit the relational inductive biases of particular types of data. For instance, convolutional neural networks are based on exploiting the relation of locality in grid-structured data, and recurrent neural networks are designed to exploit the sequentiality of time series (i.e., chains of time steps). Adding to this, in recent years graph neural networks (GNNs) have been proposed, in several formulations, as a general framework for exploiting arbitrary relational inductive biases on arbitrarily defined entities and relations, giving rise to the field of relational representation learning (RRL). In other words, RRL consists of developing models that are able to deal with graphs natively, taking their topology and attributes into account when making a prediction, exactly like we do when we reason about the universe. To read more about RRL and recent developments in the literature, an excellent starting point is this paper by DeepMind . Why Keras? The core GNN modules of Spektral are based on Keras, rather than libraries like TensorFlow or PyTorch. Because it's built as a Keras extension, Spektral works with all the different backends offered by Keras, so that you can quickly start experimenting with RRL without having to deal with the distracting low-level details. This also means that Spektral inherits all the core design principles of Keras, and that it can be seamlessly integrated in your Keras or TensorFlow projects. While Keras has a slightly higher computational overhead with respect to the \"pure\" deep learning frameworks, the speed of implementation of Keras's models largely makes up for the disadvantage. At the same time, Keras offers the same granular control as the other frameworks via direct access to the backend ( from keras import backend as K ), and native functions from the backend frameworks can directly be used at any point in a Keras model. Spektral's accessory modules are built in Numpy/Scipy, so everything should work at almost-C-like speed and without compatibility issues. Installation Spektral is developed with Python 3 in mind, although some modules may work as expected also in Python 2. However, you should consider switching to Python 3 if you haven't already. The framework is tested for Ubuntu 16.04 and 18.04, but is should also work on other Linux distros and MacOS. Core functionalities should work on Windows, as well, although it is not fully supported for now. To install the required dependencies on Ubuntu run: $ sudo apt install graphviz libgraphviz-dev libcgraph6 Some features of Spektral also require the following optional dependencies: RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda); dyfunconn , a library to build functional connectivity networks (available through PyPi); CDG , a library implementing several change detection algorithms, as well as an engine for Riemannian geometry (available on Github); The simplest way to install Spektral is with PyPi: $ pip install spektral To install Spektral from source, run this in a terminal: $ git clone https://github.com/danielegrattarola/spektral.git $ cd spektral $ python setup.py install # Or 'pip install .' Note that the setup.py script will not attempt to install a backend for Keras, in order to not mess up any previous installation. It will, however, install Keras and its dependencies via PyPi (which may include the CPU version of TensorFlow). If you are already a Keras user, this should not impact you. If you're just getting started, then you may want to install the GPU version of Tensorflow before installing Spektral. Also note that some features of Spektral may depend explicitly on TensorFlow, although this dependency will be kept to a minimum. Contributing Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework.","title":"Home"},{"location":"#welcome-to-spektral","text":"Spektral is a framework for relational representation learning, built in Python and based on the Keras API. The main purpose of this project is to provide a simple, fast, and scalable environment for fast experimentation. Spektral contains a wide set of tools to build graph neural networks, and implements some of the most popular layers for graph deep learning so that you only need to worry about creating your models. Spektral is an open source project available on Github . Read the documentation on the official website .","title":"Welcome to Spektral"},{"location":"#relational-representation-learning","text":"An important trait of human intelligence is the ability to model the world in terms of entities and relations, exploiting the knowledge that entities are connected (called the relational inductive bias ) in order to make informed decisions. This relational representation of the world allows us to model a great deal of phenomena using graphs, from social networks to sentences, from the interactions of bodies in a gravitational field to the bonds between atoms in a molecule. Given their flexibility for representing knowledge in a structured way, graphs have been historically ubiquitous in computer science, and early efforts in artificial intelligence relied heavily on relational representations (e.g., Monte Carlo methods, Bayesian networks, etc.). Modern machine learning approaches, on the other hand, seemed to have diverted from such hand-engineered representations in favour of learning from raw data, with deep learning methods leading the way on the quasi-totality of learning tasks. However, by placing artificial intelligence in the framework of relational inductive bias, as proposed by Battaglia et al. , we see that even the most modern deep learning methods are designed to exploit the relational inductive biases of particular types of data. For instance, convolutional neural networks are based on exploiting the relation of locality in grid-structured data, and recurrent neural networks are designed to exploit the sequentiality of time series (i.e., chains of time steps). Adding to this, in recent years graph neural networks (GNNs) have been proposed, in several formulations, as a general framework for exploiting arbitrary relational inductive biases on arbitrarily defined entities and relations, giving rise to the field of relational representation learning (RRL). In other words, RRL consists of developing models that are able to deal with graphs natively, taking their topology and attributes into account when making a prediction, exactly like we do when we reason about the universe. To read more about RRL and recent developments in the literature, an excellent starting point is this paper by DeepMind .","title":"Relational Representation Learning"},{"location":"#why-keras","text":"The core GNN modules of Spektral are based on Keras, rather than libraries like TensorFlow or PyTorch. Because it's built as a Keras extension, Spektral works with all the different backends offered by Keras, so that you can quickly start experimenting with RRL without having to deal with the distracting low-level details. This also means that Spektral inherits all the core design principles of Keras, and that it can be seamlessly integrated in your Keras or TensorFlow projects. While Keras has a slightly higher computational overhead with respect to the \"pure\" deep learning frameworks, the speed of implementation of Keras's models largely makes up for the disadvantage. At the same time, Keras offers the same granular control as the other frameworks via direct access to the backend ( from keras import backend as K ), and native functions from the backend frameworks can directly be used at any point in a Keras model. Spektral's accessory modules are built in Numpy/Scipy, so everything should work at almost-C-like speed and without compatibility issues.","title":"Why Keras?"},{"location":"#installation","text":"Spektral is developed with Python 3 in mind, although some modules may work as expected also in Python 2. However, you should consider switching to Python 3 if you haven't already. The framework is tested for Ubuntu 16.04 and 18.04, but is should also work on other Linux distros and MacOS. Core functionalities should work on Windows, as well, although it is not fully supported for now. To install the required dependencies on Ubuntu run: $ sudo apt install graphviz libgraphviz-dev libcgraph6 Some features of Spektral also require the following optional dependencies: RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda); dyfunconn , a library to build functional connectivity networks (available through PyPi); CDG , a library implementing several change detection algorithms, as well as an engine for Riemannian geometry (available on Github); The simplest way to install Spektral is with PyPi: $ pip install spektral To install Spektral from source, run this in a terminal: $ git clone https://github.com/danielegrattarola/spektral.git $ cd spektral $ python setup.py install # Or 'pip install .' Note that the setup.py script will not attempt to install a backend for Keras, in order to not mess up any previous installation. It will, however, install Keras and its dependencies via PyPi (which may include the CPU version of TensorFlow). If you are already a Keras user, this should not impact you. If you're just getting started, then you may want to install the GPU version of Tensorflow before installing Spektral. Also note that some features of Spektral may depend explicitly on TensorFlow, although this dependency will be kept to a minimum.","title":"Installation"},{"location":"#contributing","text":"Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework.","title":"Contributing"},{"location":"acknowledgements/","text":"Acknowledgements Spektral is released under MIT license, available on the project's Github . The framework is developed and maintained primarily by Daniele Grattarola, from Universit\u00e0 della Svizzera Italiana (USI) , under the support of the Swiss National Science Foundation grant 200021_172671.","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"Spektral is released under MIT license, available on the project's Github . The framework is developed and maintained primarily by Daniele Grattarola, from Universit\u00e0 della Svizzera Italiana (USI) , under the support of the Swiss National Science Foundation grant 200021_172671.","title":"Acknowledgements"},{"location":"brain/","text":"This module provides some functions to create functional connectivity networks, and requires the dyfunconn library to be installed on the system. get_fc spektral.brain.get_fc(x, band_freq, sampling_freq, samples_per_graph=None, fc_measure='corr', link_cutoff=0.0, percentiles=None, band_freq_hi=(20.0, 45.0), nfft=128, n_overlap=64, njobs=1) Build functional connectivity networks from the given data stream. Arguments x : numpy array of shape (n_channels, n_samples); band_freq : list with two elements, the band in which to estimate FC; sampling_freq : float, sampling frequency of the stream; samples_per_graph : number of samples to use to generate a graph. By default, the whole stream is used. If provided, 1 + (n_samples / samples_per_graph) will be generated; fc_measure : functional connectivity measure to use; link_cutoff : links with absolute FC measure below this value will be removed; percentiles : tuple of two numbers >0 and <100; links with FC measure between the two percentiles will be removed (statistics are calculated for each edge). Note that this option ignores link_cutoff . band_freq_hi : high band used to estimate FC when using 'aec'; nfft : TODO, affects 'wpli' and 'dwpli'; n_overlap : TODO, affects 'wpli' and 'dwpli'; njobs : number of processes to use (-1 to use all available cores); Return FC graph(s) in numpy format (note that node features are all ones).","title":"Brain"},{"location":"brain/#get_fc","text":"spektral.brain.get_fc(x, band_freq, sampling_freq, samples_per_graph=None, fc_measure='corr', link_cutoff=0.0, percentiles=None, band_freq_hi=(20.0, 45.0), nfft=128, n_overlap=64, njobs=1) Build functional connectivity networks from the given data stream. Arguments x : numpy array of shape (n_channels, n_samples); band_freq : list with two elements, the band in which to estimate FC; sampling_freq : float, sampling frequency of the stream; samples_per_graph : number of samples to use to generate a graph. By default, the whole stream is used. If provided, 1 + (n_samples / samples_per_graph) will be generated; fc_measure : functional connectivity measure to use; link_cutoff : links with absolute FC measure below this value will be removed; percentiles : tuple of two numbers >0 and <100; links with FC measure between the two percentiles will be removed (statistics are calculated for each edge). Note that this option ignores link_cutoff . band_freq_hi : high band used to estimate FC when using 'aec'; nfft : TODO, affects 'wpli' and 'dwpli'; n_overlap : TODO, affects 'wpli' and 'dwpli'; njobs : number of processes to use (-1 to use all available cores); Return FC graph(s) in numpy format (note that node features are all ones).","title":"get_fc"},{"location":"chem/","text":"This module provides some functions to work with molecules, and requires the RDKit library to be installed on the system. numpy_to_rdkit spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule numpy_to_smiles spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule rdkit_to_smiles spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule sdf_to_nx spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs. nx_to_sdf spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format. validate_rdkit spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise get_atomic_symbol spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol get_atomic_num spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118 valid_score spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule novel_score spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule unique_score spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules enable_rdkit_log spektral.chem.enable_rdkit_log() Enables RDkit logging. Return plot_rdkit spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return plot_rdkit_svg_grid spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"Chemistry"},{"location":"chem/#numpy_to_rdkit","text":"spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule","title":"numpy_to_rdkit"},{"location":"chem/#numpy_to_smiles","text":"spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule","title":"numpy_to_smiles"},{"location":"chem/#rdkit_to_smiles","text":"spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule","title":"rdkit_to_smiles"},{"location":"chem/#sdf_to_nx","text":"spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs.","title":"sdf_to_nx"},{"location":"chem/#nx_to_sdf","text":"spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format.","title":"nx_to_sdf"},{"location":"chem/#validate_rdkit","text":"spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise","title":"validate_rdkit"},{"location":"chem/#get_atomic_symbol","text":"spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol","title":"get_atomic_symbol"},{"location":"chem/#get_atomic_num","text":"spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118","title":"get_atomic_num"},{"location":"chem/#valid_score","text":"spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule","title":"valid_score"},{"location":"chem/#novel_score","text":"spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule","title":"novel_score"},{"location":"chem/#unique_score","text":"spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules","title":"unique_score"},{"location":"chem/#enable_rdkit_log","text":"spektral.chem.enable_rdkit_log() Enables RDkit logging. Return","title":"enable_rdkit_log"},{"location":"chem/#plot_rdkit","text":"spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return","title":"plot_rdkit"},{"location":"chem/#plot_rdkit_svg_grid","text":"spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"plot_rdkit_svg_grid"},{"location":"data/","text":"Representing graphs Spektral uses a matrix-based representation for manipulating graphs and feeding them to neural networks. This approach is one of the most commonly used in the literature on graph neural networks, and it's perfect to perform parallel computations on GPU. A graph is generally represented by three matrices: a binary adjacency matrix, A \\in \\{0, 1\\}^{N \\times N} , where A_{ij} = 1 if there is a connection between nodes i and j , and A_{ij} = 0 otherwise; a matrix encoding node attributes, X \\in \\mathbb{R}^{N \\times F} , where each row represents the F -dimensional attribute vector of a node; a matrix encoding edge attributes, E \\in \\mathbb{R}^{N \\times N \\times S} , where each entry represents the S -dimensional attribute vector of an edge; Some formulations (like the graph networks proposed by Battaglia et al.) also include a feature vector describing the global state of the graph, but this is not supported by Spektral for now. In code, the three matrices described above are Numpy arrays A , X , and E , where A.shape == (N, N) , X.shape == (N, F) , and E.shape == (N, N, S) . Sparse adjacency matrices are also supported by Spektral. Modes In Spektral, some functionalities are implemented to work on a single graph, while others consider sets (i.e., batches) of graphs. To understand the need for different settings, consider the difference between classifying the nodes of a citation network, and classifying the chemical properties of molecules. In the citation network, we are interested in the single nodes and the connections between them. Node and edge attributes are specific to each individual network, and there is no point in training a model across networks. On the other hand, when working with molecules in a dataset, we are in a much more familiar setting. Each molecule is a sample of our dataset, and the atoms and bonds that make up the molecules are repeated across the data (like pixels in images). In this case, we are interested in finding patterns that describe the properties of the molecules in general. The two settings require us to do things that are conceptually similar, but that need some minor adjustments in how the data is processed by our graph neural networks. This is why Spektral makes these differences explicit. In practice, we actually distinguish between three main modes of operation: single , where we have a single graph, with its topology and attributes; batch , where we have a collection of graphs, each with its own topology and attributes; mixed , where we have a graph with fixed topology, but a collection of different attributes (usually called graph signals in the literature); this can be seen as a particular case of the batch mode (where all adjacency matrices are the same) but it is handled separately in Spektral to improve memory efficiency. Mode Adjacency Node attr. Edge attr. Single (N, N) (N, F) (N, N, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) (batch, N, N, S) In practice, in \"single\" mode the data has no batch dimension, and describes a single graph: In [1]: from spektral.datasets import citation Using TensorFlow backend. In [2]: adj, node_features, _, _, _, _, _, _ = citation.load_data('cora') Loading cora dataset In [3]: adj.shape Out[3]: (2708, 2708) In [4]: node_features.shape Out[4]: (2708, 1433) This means that when training GNNs in single mode, we cannot batch and shuffle the data along the first axis, and the whole graph must be fed to the model at each step (see the node classification example ). In \"batch\" mode, the matrices will have a batch dimension first: In [1]: from spektral.datasets import qm9 Using TensorFlow backend. In [2]: adj, nf, ef, _ = qm9.load_data() Loading QM9 dataset. Reading SDF 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133885/133885 [00:29<00:00, 4579.22it/s] In [3]: adj.shape Out[3]: (133885, 9, 9) In [4]: nf.shape Out[4]: (133885, 9, 6) In [5]: ef.shape Out[5]: (133885, 9, 9, 1) This should not surprise you if you are familiar with classical machine learning tasks (try and load any benchmark image dataset to see a similar representation of the data). Finally, in \"mixed\" mode we consider a single adjacency matrix with no batch dimension, and collections of node and edge attributes with the batch dimension: In [1]: from spektral.datasets import mnist Using TensorFlow backend. In [2]: nf, _, _, _, _, _, adj = mnist.load_data() In [3]: adj.shape Out[3]: (784, 784) In [4]: nf.shape Out[4]: (50000, 784) Other formats To provide better compatibility with other libraries, Spektral has methods to convert graphs between the matrix representation ( 'numpy' ) and other formats. The 'networkx' format represents graphs using the Networkx library, which can then be used to convert the graphs to other formats like .dot and edge lists. Conversion methods between 'numpy' and 'networkx' are provided in spektral.utils.conversion . Molecules When working with molecules, some specific formats can be used to represent the graphs. The 'sdf' format is an internal representation format used to store an SDF file to a dictionary. A molecule in 'sdf' format will look something like this: {'atoms': [{'atomic_num': 7, 'charge': 0, 'coords': array([-0.0299, 1.2183, 0.2994]), 'index': 0, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}, ..., {'atomic_num': 1, 'charge': 0, 'coords': array([ 0.6896, -2.3002, -0.1042]), 'index': 14, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}], 'bonds': [{'end_atom': 13, 'info': array([0, 0, 0]), 'start_atom': 4, 'stereo': 0, 'type': 1}, ..., {'end_atom': 8, 'info': array([0, 0, 0]), 'start_atom': 7, 'stereo': 0, 'type': 3}], 'comment': '', 'data': [''], 'details': '-OEChem-03231823253D', 'n_atoms': 15, 'n_bonds': 15, 'name': 'gdb_54964', 'properties': []} The 'rdkit' format uses the RDKit library to represent molecules, and offers several methods to manipulate molecules with a chemistry-oriented approach. The 'smiles' format represents molecules as strings, and can be used as a space-efficient way to store molecules or perform quick checks on a dataset (e.g., counting the unique number of molecules in a dataset is quicker if all molecules are converted to SMILES first). The spektral.chem modules offers conversion methods between all of these formats, although some conversions may need more than one step to do (e.g., 'sdf' to 'networkx' to 'numpy' to 'smiles' ). Support for direct conversion between all formats will eventually be added.","title":"Data representation"},{"location":"data/#representing-graphs","text":"Spektral uses a matrix-based representation for manipulating graphs and feeding them to neural networks. This approach is one of the most commonly used in the literature on graph neural networks, and it's perfect to perform parallel computations on GPU. A graph is generally represented by three matrices: a binary adjacency matrix, A \\in \\{0, 1\\}^{N \\times N} , where A_{ij} = 1 if there is a connection between nodes i and j , and A_{ij} = 0 otherwise; a matrix encoding node attributes, X \\in \\mathbb{R}^{N \\times F} , where each row represents the F -dimensional attribute vector of a node; a matrix encoding edge attributes, E \\in \\mathbb{R}^{N \\times N \\times S} , where each entry represents the S -dimensional attribute vector of an edge; Some formulations (like the graph networks proposed by Battaglia et al.) also include a feature vector describing the global state of the graph, but this is not supported by Spektral for now. In code, the three matrices described above are Numpy arrays A , X , and E , where A.shape == (N, N) , X.shape == (N, F) , and E.shape == (N, N, S) . Sparse adjacency matrices are also supported by Spektral.","title":"Representing graphs"},{"location":"data/#modes","text":"In Spektral, some functionalities are implemented to work on a single graph, while others consider sets (i.e., batches) of graphs. To understand the need for different settings, consider the difference between classifying the nodes of a citation network, and classifying the chemical properties of molecules. In the citation network, we are interested in the single nodes and the connections between them. Node and edge attributes are specific to each individual network, and there is no point in training a model across networks. On the other hand, when working with molecules in a dataset, we are in a much more familiar setting. Each molecule is a sample of our dataset, and the atoms and bonds that make up the molecules are repeated across the data (like pixels in images). In this case, we are interested in finding patterns that describe the properties of the molecules in general. The two settings require us to do things that are conceptually similar, but that need some minor adjustments in how the data is processed by our graph neural networks. This is why Spektral makes these differences explicit. In practice, we actually distinguish between three main modes of operation: single , where we have a single graph, with its topology and attributes; batch , where we have a collection of graphs, each with its own topology and attributes; mixed , where we have a graph with fixed topology, but a collection of different attributes (usually called graph signals in the literature); this can be seen as a particular case of the batch mode (where all adjacency matrices are the same) but it is handled separately in Spektral to improve memory efficiency. Mode Adjacency Node attr. Edge attr. Single (N, N) (N, F) (N, N, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) (batch, N, N, S) In practice, in \"single\" mode the data has no batch dimension, and describes a single graph: In [1]: from spektral.datasets import citation Using TensorFlow backend. In [2]: adj, node_features, _, _, _, _, _, _ = citation.load_data('cora') Loading cora dataset In [3]: adj.shape Out[3]: (2708, 2708) In [4]: node_features.shape Out[4]: (2708, 1433) This means that when training GNNs in single mode, we cannot batch and shuffle the data along the first axis, and the whole graph must be fed to the model at each step (see the node classification example ). In \"batch\" mode, the matrices will have a batch dimension first: In [1]: from spektral.datasets import qm9 Using TensorFlow backend. In [2]: adj, nf, ef, _ = qm9.load_data() Loading QM9 dataset. Reading SDF 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133885/133885 [00:29<00:00, 4579.22it/s] In [3]: adj.shape Out[3]: (133885, 9, 9) In [4]: nf.shape Out[4]: (133885, 9, 6) In [5]: ef.shape Out[5]: (133885, 9, 9, 1) This should not surprise you if you are familiar with classical machine learning tasks (try and load any benchmark image dataset to see a similar representation of the data). Finally, in \"mixed\" mode we consider a single adjacency matrix with no batch dimension, and collections of node and edge attributes with the batch dimension: In [1]: from spektral.datasets import mnist Using TensorFlow backend. In [2]: nf, _, _, _, _, _, adj = mnist.load_data() In [3]: adj.shape Out[3]: (784, 784) In [4]: nf.shape Out[4]: (50000, 784)","title":"Modes"},{"location":"data/#other-formats","text":"To provide better compatibility with other libraries, Spektral has methods to convert graphs between the matrix representation ( 'numpy' ) and other formats. The 'networkx' format represents graphs using the Networkx library, which can then be used to convert the graphs to other formats like .dot and edge lists. Conversion methods between 'numpy' and 'networkx' are provided in spektral.utils.conversion .","title":"Other formats"},{"location":"data/#molecules","text":"When working with molecules, some specific formats can be used to represent the graphs. The 'sdf' format is an internal representation format used to store an SDF file to a dictionary. A molecule in 'sdf' format will look something like this: {'atoms': [{'atomic_num': 7, 'charge': 0, 'coords': array([-0.0299, 1.2183, 0.2994]), 'index': 0, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}, ..., {'atomic_num': 1, 'charge': 0, 'coords': array([ 0.6896, -2.3002, -0.1042]), 'index': 14, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}], 'bonds': [{'end_atom': 13, 'info': array([0, 0, 0]), 'start_atom': 4, 'stereo': 0, 'type': 1}, ..., {'end_atom': 8, 'info': array([0, 0, 0]), 'start_atom': 7, 'stereo': 0, 'type': 3}], 'comment': '', 'data': [''], 'details': '-OEChem-03231823253D', 'n_atoms': 15, 'n_bonds': 15, 'name': 'gdb_54964', 'properties': []} The 'rdkit' format uses the RDKit library to represent molecules, and offers several methods to manipulate molecules with a chemistry-oriented approach. The 'smiles' format represents molecules as strings, and can be used as a space-efficient way to store molecules or perform quick checks on a dataset (e.g., counting the unique number of molecules in a dataset is quicker if all molecules are converted to SMILES first). The spektral.chem modules offers conversion methods between all of these formats, although some conversions may need more than one step to do (e.g., 'sdf' to 'networkx' to 'numpy' to 'smiles' ). Support for direct conversion between all formats will eventually be added.","title":"Molecules"},{"location":"geometric/","text":"This module provides some functions to work with Riemannian geometry, and requires the CDG library to be installed on the system. exp_map spektral.geometric.manifold.exp_map(x, r, tangent_point=None) Let \\mathcal{M} be a CCM of radius r , and T_{p}\\mathcal{M} the tangent plane of the CCM at point p ( tangent_point ). This function maps a point x on the tangent plane to the CCM, using the Riemannian exponential map. Arguments x : np.array, point on the tangent plane (intrinsic coordinates); r : float, radius of the CCM; tangent_point : np.array, origin of the tangent plane on the CCM (extrinsic coordinates); if None , defaults to [0., ..., 0., r] . Return The exp-map of x to the CCM (extrinsic coordinates). log_map spektral.geometric.manifold.log_map(x, r, tangent_point=None) Let \\mathcal{M} be a CCM of radius r and T_{p}\\mathcal{M} the tangent plane of the CCM at point p ( tangent_point ). This function maps a point x on the CCM to the tangent plane, using the Riemannian logarithmic map. Arguments x : np.array, point on the CCM (extrinsic coordinates); r : float, radius of the CCM; tangent_point : np.array, origin of the tangent plane on the CCM (extrinsic coordinates); if 'None', defaults to [0., ..., 0., r] . Return The log-map of x to the tangent plane (intrinsic coordinates). belongs spektral.geometric.manifold.belongs(x, r) Boolean membership to CCM of radius r . Arguments x : np.array, coordinates are assumed to be in the last axis; r : float, the radius of the CCM; Return Boolean np.array, True if the points are on the CCM. clip spektral.geometric.manifold.clip(x, r, axis=-1) Clips points in the ambient space to a CCM of radius r . Arguments x : np.array, coordinates are assumed to be in the last axis; r : float, the radius of the CCM; axis : axis along which to clip points in the hyperbolic case ( r < 0 ); Return Np.array of same shape as x. get_distance spektral.geometric.manifold.get_distance(r) Arguments r : float, the radius of the CCM; Return The callable distance function for the CCM of radius r . ccm_uniform spektral.geometric.stat.ccm_uniform(size, dim=3, r=0.0, low=-1.0, high=1.0, projection='upper') Samples points from a uniform distribution on a constant-curvature manifold. If r=0 , then points are sampled from a uniform distribution in the ambient space. If a list of radii is passed instead of a single scalar, then the sampling is repeated for each value in the list and the results are concatenated along the last axis (e.g., see Grattarola et al. (2018) ). Arguments size : number of points to sample; dim : dimension of the ambient space; r : floats or list of floats, radii of the CCMs; low : lower bound of the uniform distribution from which to sample; high : upper bound of the uniform distribution from which to sample; projection : 'upper', 'lower', or 'both'. Whether to project points always on the upper or lower branch of the hyperboloid, or on both based on the sign of the last coordinate. Return If r is a scalar, np.array of shape (size, dim). If r is a list, np.array of shape (size, len(r) * dim). ccm_normal spektral.geometric.stat.ccm_normal(size, dim=3, r=0.0, tangent_point=None, loc=0.0, scale=1.0) Samples points from a Gaussian distribution on a constant-curvature manifold. If r=0 , then points are sampled from a Gaussian distribution in the ambient space. If a list of radii is passed instead of a single scalar, then the sampling is repeated for each value in the list and the results are concatenated along the last axis (e.g., see Grattarola et al. (2018) ). Arguments size : number of points to sample; tangent_point : np.array, origin of the tangent plane on the CCM (extrinsic coordinates); if 'None', defaults to [0., ..., 0., r] . r : floats or list of floats, radii of the CCMs; dim : dimension of the ambient space; loc : mean of the Gaussian on the tangent plane; scale : standard deviation of the Gaussian on the tangent plane; Return If r is a scalar, np.array of shape (size, dim). If r is a list, np.array of shape (size, len(r) * dim). get_ccm_distribution spektral.geometric.stat.get_ccm_distribution(name) Arguments name : 'uniform' or 'normal', name of the distribution. Return The callable function for sampling on a generic CCM;","title":"Geometric"},{"location":"geometric/#exp_map","text":"spektral.geometric.manifold.exp_map(x, r, tangent_point=None) Let \\mathcal{M} be a CCM of radius r , and T_{p}\\mathcal{M} the tangent plane of the CCM at point p ( tangent_point ). This function maps a point x on the tangent plane to the CCM, using the Riemannian exponential map. Arguments x : np.array, point on the tangent plane (intrinsic coordinates); r : float, radius of the CCM; tangent_point : np.array, origin of the tangent plane on the CCM (extrinsic coordinates); if None , defaults to [0., ..., 0., r] . Return The exp-map of x to the CCM (extrinsic coordinates).","title":"exp_map"},{"location":"geometric/#log_map","text":"spektral.geometric.manifold.log_map(x, r, tangent_point=None) Let \\mathcal{M} be a CCM of radius r and T_{p}\\mathcal{M} the tangent plane of the CCM at point p ( tangent_point ). This function maps a point x on the CCM to the tangent plane, using the Riemannian logarithmic map. Arguments x : np.array, point on the CCM (extrinsic coordinates); r : float, radius of the CCM; tangent_point : np.array, origin of the tangent plane on the CCM (extrinsic coordinates); if 'None', defaults to [0., ..., 0., r] . Return The log-map of x to the tangent plane (intrinsic coordinates).","title":"log_map"},{"location":"geometric/#belongs","text":"spektral.geometric.manifold.belongs(x, r) Boolean membership to CCM of radius r . Arguments x : np.array, coordinates are assumed to be in the last axis; r : float, the radius of the CCM; Return Boolean np.array, True if the points are on the CCM.","title":"belongs"},{"location":"geometric/#clip","text":"spektral.geometric.manifold.clip(x, r, axis=-1) Clips points in the ambient space to a CCM of radius r . Arguments x : np.array, coordinates are assumed to be in the last axis; r : float, the radius of the CCM; axis : axis along which to clip points in the hyperbolic case ( r < 0 ); Return Np.array of same shape as x.","title":"clip"},{"location":"geometric/#get_distance","text":"spektral.geometric.manifold.get_distance(r) Arguments r : float, the radius of the CCM; Return The callable distance function for the CCM of radius r .","title":"get_distance"},{"location":"geometric/#ccm_uniform","text":"spektral.geometric.stat.ccm_uniform(size, dim=3, r=0.0, low=-1.0, high=1.0, projection='upper') Samples points from a uniform distribution on a constant-curvature manifold. If r=0 , then points are sampled from a uniform distribution in the ambient space. If a list of radii is passed instead of a single scalar, then the sampling is repeated for each value in the list and the results are concatenated along the last axis (e.g., see Grattarola et al. (2018) ). Arguments size : number of points to sample; dim : dimension of the ambient space; r : floats or list of floats, radii of the CCMs; low : lower bound of the uniform distribution from which to sample; high : upper bound of the uniform distribution from which to sample; projection : 'upper', 'lower', or 'both'. Whether to project points always on the upper or lower branch of the hyperboloid, or on both based on the sign of the last coordinate. Return If r is a scalar, np.array of shape (size, dim). If r is a list, np.array of shape (size, len(r) * dim).","title":"ccm_uniform"},{"location":"geometric/#ccm_normal","text":"spektral.geometric.stat.ccm_normal(size, dim=3, r=0.0, tangent_point=None, loc=0.0, scale=1.0) Samples points from a Gaussian distribution on a constant-curvature manifold. If r=0 , then points are sampled from a Gaussian distribution in the ambient space. If a list of radii is passed instead of a single scalar, then the sampling is repeated for each value in the list and the results are concatenated along the last axis (e.g., see Grattarola et al. (2018) ). Arguments size : number of points to sample; tangent_point : np.array, origin of the tangent plane on the CCM (extrinsic coordinates); if 'None', defaults to [0., ..., 0., r] . r : floats or list of floats, radii of the CCMs; dim : dimension of the ambient space; loc : mean of the Gaussian on the tangent plane; scale : standard deviation of the Gaussian on the tangent plane; Return If r is a scalar, np.array of shape (size, dim). If r is a list, np.array of shape (size, len(r) * dim).","title":"ccm_normal"},{"location":"geometric/#get_ccm_distribution","text":"spektral.geometric.stat.get_ccm_distribution(name) Arguments name : 'uniform' or 'normal', name of the distribution. Return The callable function for sampling on a generic CCM;","title":"get_ccm_distribution"},{"location":"papers/","text":"Featured papers This is a list of papers that use Spektral to run experiments. If you want to have your paper listed on this page, feel free to send a Bibtex citation to Daniele Grattarola to have it added to the list. Autoregressive Models for Sequences of Graphs Zambon et al. @article{zambon2019autoregressive, title={Autoregressive Models for Sequences of Graphs}, author={Zambon, Daniele and Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare}, journal={International Joint Conference on Neural Networks}, year={2019} } Graph Neural Networks with Convolutional ARMA Filters Bianchi et al. @article{bianchi2019graph, title={Graph Neural Networks with Convolutional ARMA Filters}, author={Bianchi, Filippo Maria and Grattarola, Daniele and Alippi, Cesare and Livi, Lorenzo}, journal={arXiv preprint arXiv:1901.01343}, year={2019} } Adversarial Autoencoders with Constant Curvature Latent Manifolds Grattarola et al. @article{grattarola2018adversarial, title={Adversarial Autoencoders with Constant Curvature Latent Manifolds}, author={Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare}, journal={arXiv preprint arXiv:1812.04314}, year={2018} } Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature Manifolds Grattarola et al. @article{grattarola2018learning, title={Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature Manifolds}, author={Grattarola, Daniele and Zambon, Daniele and Alippi, Cesare and Livi, Lorenzo}, journal={arXiv preprint arXiv:1805.06299}, year={2018} }","title":"Featured papers"},{"location":"papers/#featured-papers","text":"This is a list of papers that use Spektral to run experiments. If you want to have your paper listed on this page, feel free to send a Bibtex citation to Daniele Grattarola to have it added to the list. Autoregressive Models for Sequences of Graphs Zambon et al. @article{zambon2019autoregressive, title={Autoregressive Models for Sequences of Graphs}, author={Zambon, Daniele and Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare}, journal={International Joint Conference on Neural Networks}, year={2019} } Graph Neural Networks with Convolutional ARMA Filters Bianchi et al. @article{bianchi2019graph, title={Graph Neural Networks with Convolutional ARMA Filters}, author={Bianchi, Filippo Maria and Grattarola, Daniele and Alippi, Cesare and Livi, Lorenzo}, journal={arXiv preprint arXiv:1901.01343}, year={2019} } Adversarial Autoencoders with Constant Curvature Latent Manifolds Grattarola et al. @article{grattarola2018adversarial, title={Adversarial Autoencoders with Constant Curvature Latent Manifolds}, author={Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare}, journal={arXiv preprint arXiv:1812.04314}, year={2018} } Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature Manifolds Grattarola et al. @article{grattarola2018learning, title={Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature Manifolds}, author={Grattarola, Daniele and Zambon, Daniele and Alippi, Cesare and Livi, Lorenzo}, journal={arXiv preprint arXiv:1805.06299}, year={2018} }","title":"Featured papers"},{"location":"datasets/citation/","text":"load_data spektral.datasets.citation.load_data(dataset_name='cora') Loads a citation dataset using the public splits as defined in Kipf & Welling (2016) . Arguments dataset_name : name of the dataset to load ('cora', 'citeseer', or 'pubmed'). Return The citation network in numpy format, with train, test, and validation splits for the targets and masks.","title":"Citation"},{"location":"datasets/citation/#load_data","text":"spektral.datasets.citation.load_data(dataset_name='cora') Loads a citation dataset using the public splits as defined in Kipf & Welling (2016) . Arguments dataset_name : name of the dataset to load ('cora', 'citeseer', or 'pubmed'). Return The citation network in numpy format, with train, test, and validation splits for the targets and masks.","title":"load_data"},{"location":"datasets/delaunay/","text":"generate_data spektral.datasets.delaunay.generate_data(return_type='numpy', classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None) Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Note that this function is basically deprecated and will change soon. Arguments return_type : 'numpy' or 'networkx', data format to return; classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; Return If return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"Delaunay"},{"location":"datasets/delaunay/#generate_data","text":"spektral.datasets.delaunay.generate_data(return_type='numpy', classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None) Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Note that this function is basically deprecated and will change soon. Arguments return_type : 'numpy' or 'networkx', data format to return; classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; Return If return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"generate_data"},{"location":"datasets/mnist/","text":"load_data spektral.datasets.mnist.load_data() Loads the MNIST dataset and the associated grid. This code is largely taken from Micha\u00ebl Defferrard's Github . Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"MNIST"},{"location":"datasets/mnist/#load_data","text":"spektral.datasets.mnist.load_data() Loads the MNIST dataset and the associated grid. This code is largely taken from Micha\u00ebl Defferrard's Github . Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"load_data"},{"location":"datasets/qm9/","text":"load_data spektral.datasets.qm9.load_data(return_type='numpy', nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None) Loads the QM9 molecules dataset. Arguments return_type : 'networkx', 'numpy', or 'sdf', data format to return; nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in order). Return If return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"QM9"},{"location":"datasets/qm9/#load_data","text":"spektral.datasets.qm9.load_data(return_type='numpy', nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None) Loads the QM9 molecules dataset. Arguments return_type : 'networkx', 'numpy', or 'sdf', data format to return; nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in order). Return If return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"load_data"},{"location":"layers/base/","text":"[source] InnerProduct spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = xx^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments trainable_kernel : add a trainable square matrix between the inner product (i.e., x.dot(w).dot(x.T) ); activation : activation function to use; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel; [source] MinkowskiProduct spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None, activity_regularizer=None) Computes the hyperbolic inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = x \\, \\begin{pmatrix} I_{d\\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\,x^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments input_dim_1 : first dimension of the input tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use; activity_regularizer : regularization applied to the output; [source] CCMProjection spektral.layers.CCMProjection(r=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Projects a tensor to a CCM depending on the value of r . Optionally, r can be learned via backpropagation. Input tensor of shape (batch_size, input_dim) . Output tensor of shape (batch_size, input_dim) , where each sample along the 0th axis is projected to the CCM. Arguments r : radius of the CCM. If r is a number, then use it as fixed radius. If r='spherical' , use a trainable weight as radius, with a positivity constraint. If r='hyperbolic' , use a trainable weight as radius, with a negativity constraint. If r=None , use a trainable weight as radius, with no constraints (points will be projected to the correct manifold based on the sign of the weight). kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix. [source] CCMMembership spektral.layers.CCMMembership(r=1.0, mode='average', sigma=1.0) Computes the membership of the given points to a constant-curvature manifold of radius r , as: \\mu(x) = \\mathrm{exp}\\left(\\cfrac{-\\big( \\langle \\vec x, \\vec x \\rangle - r^2 \\big)^2}{2\\sigma^2}\\right). If r=0 , then \\mu(x) = 1 . If more than one radius is given, inputs are evenly split across the last dimension and membership is computed for each radius-slice pair. The output membership is returned according to the mode option. Input tensor of shape (batch_size, input_dim) ; Output tensor of shape (batch_size, output_size) , where output_size is computed according to the mode option;. Arguments r : int ot list, radia of the CCMs. mode : 'average' to return the average membership across CCMs, or 'concat' to return the membership for each CCM concatenated; sigma : spread of the membership curve;","title":"Base Layers"},{"location":"layers/base/#innerproduct","text":"spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = xx^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments trainable_kernel : add a trainable square matrix between the inner product (i.e., x.dot(w).dot(x.T) ); activation : activation function to use; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel; [source]","title":"InnerProduct"},{"location":"layers/base/#minkowskiproduct","text":"spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None, activity_regularizer=None) Computes the hyperbolic inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = x \\, \\begin{pmatrix} I_{d\\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\,x^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments input_dim_1 : first dimension of the input tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use; activity_regularizer : regularization applied to the output; [source]","title":"MinkowskiProduct"},{"location":"layers/base/#ccmprojection","text":"spektral.layers.CCMProjection(r=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Projects a tensor to a CCM depending on the value of r . Optionally, r can be learned via backpropagation. Input tensor of shape (batch_size, input_dim) . Output tensor of shape (batch_size, input_dim) , where each sample along the 0th axis is projected to the CCM. Arguments r : radius of the CCM. If r is a number, then use it as fixed radius. If r='spherical' , use a trainable weight as radius, with a positivity constraint. If r='hyperbolic' , use a trainable weight as radius, with a negativity constraint. If r=None , use a trainable weight as radius, with no constraints (points will be projected to the correct manifold based on the sign of the weight). kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix. [source]","title":"CCMProjection"},{"location":"layers/base/#ccmmembership","text":"spektral.layers.CCMMembership(r=1.0, mode='average', sigma=1.0) Computes the membership of the given points to a constant-curvature manifold of radius r , as: \\mu(x) = \\mathrm{exp}\\left(\\cfrac{-\\big( \\langle \\vec x, \\vec x \\rangle - r^2 \\big)^2}{2\\sigma^2}\\right). If r=0 , then \\mu(x) = 1 . If more than one radius is given, inputs are evenly split across the last dimension and membership is computed for each radius-slice pair. The output membership is returned according to the mode option. Input tensor of shape (batch_size, input_dim) ; Output tensor of shape (batch_size, output_size) , where output_size is computed according to the mode option;. Arguments r : int ot list, radia of the CCMs. mode : 'average' to return the average membership across CCMs, or 'concat' to return the membership for each CCM concatenated; sigma : spread of the membership curve;","title":"CCMMembership"},{"location":"layers/convolution/","text":"[source] GraphConv spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) . Mode : single, mixed, batch. This layer computes the transformation: Z = \\sigma(AXW + b) where X is the node features matrix, A is the normalized Laplacian, W is the convolution kernel, b is a bias vector, and \\sigma is the activation function. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. The Laplacians can be computed from the adjacency matrices like in the original paper using utils.convolution.localpooling_filter . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage fltr = localpooling_filter(adj) # Can be any pre-processing ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = GraphConv(channels, activation='relu')([X, filter]) ... model.fit([node_features, fltr], y) [source] ChebConv spektral.layers.ChebConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, mixed, batch. Given a list of Chebyshev polynomials T = [T_{1}, ..., T_{K}] , this layer computes the transformation: Z = \\sigma( \\sum \\limits_{k=1}^{K} T_{k} X W + b) where X is the node features matrix, W is the convolution kernel, b is the bias vector, and \\sigma is the activation function. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; a list of Chebyshev filters of shape (batch, num_nodes, num_nodes) , depending on the mode. The filters can be generated from the adjacency matrices using utils.convolution.chebyshev_filter . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage fltr = chebyshev_filter(adj, K) ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = GraphConv(channels, activation='relu')([X, filter]) ... model.fit([node_features, fltr], y) [source] ARMAConv spektral.layers.ARMAConv(channels, ARMA_D, ARMA_K=None, recurrent=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA(H, K) filters, as presented by Bianchi et al. (2019) . Mode : single, mixed, batch. This layer computes the transformation: X^{out} = \\text{avgpool}\\left(\\sum \\limits_{k=1}^K \\bar{X}_k^{(T)} \\right), where: \\bar{X}_k^{(t + 1)} = \\sigma\\left(\\tilde{L}\\bar{X}^{(t)}W^{(t)} + XV^{(t)}\\right) is a graph convolutional skip layer implementing the recursive update to approximate the ARMA filter, \\tilde{L} is the Laplacian modified to have a spectrum in [0,,2] , \\bar{X}^{(0)} = X , and W, V are trainable kernels. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; normalized Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; ARMA_K : order of the ARMA filter (combination of K ARMA_1 filters); ARMA_D : depth of each ARMA_1 filter (number of recursive updates); recurrent : whether to share each head's weights like a recurrent net; gcn_activation : activation function to use to compute the ARMA filter; dropout_rate : dropout rate for laplacian and output layer activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage fltr = localpooling_filter(adj) ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = ARMAConv(channels, activation='relu')([X, filter]) ... model.fit([node_features, fltr], y) [source] EdgeConditionedConv spektral.layers.EdgeConditionedConv(channels, kernel_network=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer as presented by Simonovsky and Komodakis (2017) . Mode : batch. This layer computes a transformation of the input X , s.t. for each node i we have: X^{out}_i = \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} F(E_{ji}) X_{j} + b where \\mathcal{N}(i) represents the one-step neighbourhood of node i , F is a neural network that outputs the convolution kernel as a function of edge attributes, E is the edge attributes matrix, and b is a bias vector. Input node features of shape (batch, num_nodes, num_node_features) , depending on the mode; adjacency matrices of shape (batch, num_nodes, num_nodes) , depending on the mode. edge features of shape (batch, num_nodes, num_nodes, num_edge_features) , depending on the mode. Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers describing the hidden structure of the kernel-generating network (i.e., the ReLU layers before the linear output); activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage adj = add_eye_batch(adj) ... nf = Input(shape=(num_nodes, num_node_features)) a = Input(shape=(num_nodes, num_nodes)) ef = Input(shape=(num_nodes, num_nodes, num_edge_features)) Z = EdgeConditionedConv(32, num_nodes, num_edge_features)([nf, a, ef]) ... model.fit([node_features, adj, edge_features], y) [source] GraphAttention spektral.layers.GraphAttention(channels, attn_heads=1, attn_heads_reduction='concat', dropout_rate=0.5, activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer as presented by Velickovic et al. (2017) . Mode : single, mixed, batch. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian. Input node features of shape (num_nodes, num_features) ; adjacency matrices of shape (num_nodes, num_nodes) ; Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; attn_heads : number of attention heads to use; attn_heads_reduction : how to reduce the outputs of the attention heads (can be either 'concat' or 'average'); dropout_rate : internal dropout rate; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; attn_kernel_initializer : initializer for the attention kernel matrices; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrices; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrices; bias_constraint : constraint applied to the bias vector. Usage adj = normalize_sum_to_unity(adj) ... X = Input(shape=(num_nodes, num_features)) A = Input((num_nodes, num_nodes)) Z = GraphAttention(channels, activation='relu')([X, A]) ... model.fit([node_features, fltr], y) [source] GraphConvSkip spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) , with the addition of skip connections. Mode : single, mixed, batch. This layer computes the transformation: Z = \\sigma(A X W_1 + X_0 W_2 + b) where X is the node features matrix, X_0 is the node features matrix for the skip connection, A is the normalized laplacian, W_1 and W_2 are the convolution kernels, b is a bias vector, and \\sigma is the activation function. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; node features for the skip connection of shape (batch, num_nodes, num_features) , depending on the mode; Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. The Laplacians can be computed from the adjacency matrices using utils.convolution.localpooling_filter . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage X = Input(shape=(num_nodes, num_features)) X_0 = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = GraphConvSkip(channels, activation='relu')([X, X_0, filter]) [source] APPNP spektral.layers.APPNP(channels, mlp_channels, alpha=0.2, H=1, K=1, mlp_activation='relu', dropout_rate=0.0, activation='softmax', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . Implementation by Filippo Bianchi. Mode : single, mixed, batch. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; normalized Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_channels : integer, number of hidden units for the MLP layers; alpha : teleport probability; H : number of MLP layers; K : number of power iterations; mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage I = sp.identity(adj.shape[0], dtype=adj.dtype) fltr = utils.normalize_adjacency(adj + I) ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = APPNP(channels, mlp_channels)([X, filter]) ... model.fit([node_features, fltr], y)","title":"Convolutional Layers"},{"location":"layers/convolution/#graphconv","text":"spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) . Mode : single, mixed, batch. This layer computes the transformation: Z = \\sigma(AXW + b) where X is the node features matrix, A is the normalized Laplacian, W is the convolution kernel, b is a bias vector, and \\sigma is the activation function. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. The Laplacians can be computed from the adjacency matrices like in the original paper using utils.convolution.localpooling_filter . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage fltr = localpooling_filter(adj) # Can be any pre-processing ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = GraphConv(channels, activation='relu')([X, filter]) ... model.fit([node_features, fltr], y) [source]","title":"GraphConv"},{"location":"layers/convolution/#chebconv","text":"spektral.layers.ChebConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, mixed, batch. Given a list of Chebyshev polynomials T = [T_{1}, ..., T_{K}] , this layer computes the transformation: Z = \\sigma( \\sum \\limits_{k=1}^{K} T_{k} X W + b) where X is the node features matrix, W is the convolution kernel, b is the bias vector, and \\sigma is the activation function. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; a list of Chebyshev filters of shape (batch, num_nodes, num_nodes) , depending on the mode. The filters can be generated from the adjacency matrices using utils.convolution.chebyshev_filter . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage fltr = chebyshev_filter(adj, K) ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = GraphConv(channels, activation='relu')([X, filter]) ... model.fit([node_features, fltr], y) [source]","title":"ChebConv"},{"location":"layers/convolution/#armaconv","text":"spektral.layers.ARMAConv(channels, ARMA_D, ARMA_K=None, recurrent=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA(H, K) filters, as presented by Bianchi et al. (2019) . Mode : single, mixed, batch. This layer computes the transformation: X^{out} = \\text{avgpool}\\left(\\sum \\limits_{k=1}^K \\bar{X}_k^{(T)} \\right), where: \\bar{X}_k^{(t + 1)} = \\sigma\\left(\\tilde{L}\\bar{X}^{(t)}W^{(t)} + XV^{(t)}\\right) is a graph convolutional skip layer implementing the recursive update to approximate the ARMA filter, \\tilde{L} is the Laplacian modified to have a spectrum in [0,,2] , \\bar{X}^{(0)} = X , and W, V are trainable kernels. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; normalized Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; ARMA_K : order of the ARMA filter (combination of K ARMA_1 filters); ARMA_D : depth of each ARMA_1 filter (number of recursive updates); recurrent : whether to share each head's weights like a recurrent net; gcn_activation : activation function to use to compute the ARMA filter; dropout_rate : dropout rate for laplacian and output layer activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage fltr = localpooling_filter(adj) ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = ARMAConv(channels, activation='relu')([X, filter]) ... model.fit([node_features, fltr], y) [source]","title":"ARMAConv"},{"location":"layers/convolution/#edgeconditionedconv","text":"spektral.layers.EdgeConditionedConv(channels, kernel_network=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer as presented by Simonovsky and Komodakis (2017) . Mode : batch. This layer computes a transformation of the input X , s.t. for each node i we have: X^{out}_i = \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} F(E_{ji}) X_{j} + b where \\mathcal{N}(i) represents the one-step neighbourhood of node i , F is a neural network that outputs the convolution kernel as a function of edge attributes, E is the edge attributes matrix, and b is a bias vector. Input node features of shape (batch, num_nodes, num_node_features) , depending on the mode; adjacency matrices of shape (batch, num_nodes, num_nodes) , depending on the mode. edge features of shape (batch, num_nodes, num_nodes, num_edge_features) , depending on the mode. Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers describing the hidden structure of the kernel-generating network (i.e., the ReLU layers before the linear output); activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage adj = add_eye_batch(adj) ... nf = Input(shape=(num_nodes, num_node_features)) a = Input(shape=(num_nodes, num_nodes)) ef = Input(shape=(num_nodes, num_nodes, num_edge_features)) Z = EdgeConditionedConv(32, num_nodes, num_edge_features)([nf, a, ef]) ... model.fit([node_features, adj, edge_features], y) [source]","title":"EdgeConditionedConv"},{"location":"layers/convolution/#graphattention","text":"spektral.layers.GraphAttention(channels, attn_heads=1, attn_heads_reduction='concat', dropout_rate=0.5, activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer as presented by Velickovic et al. (2017) . Mode : single, mixed, batch. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian. Input node features of shape (num_nodes, num_features) ; adjacency matrices of shape (num_nodes, num_nodes) ; Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; attn_heads : number of attention heads to use; attn_heads_reduction : how to reduce the outputs of the attention heads (can be either 'concat' or 'average'); dropout_rate : internal dropout rate; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; attn_kernel_initializer : initializer for the attention kernel matrices; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrices; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrices; bias_constraint : constraint applied to the bias vector. Usage adj = normalize_sum_to_unity(adj) ... X = Input(shape=(num_nodes, num_features)) A = Input((num_nodes, num_nodes)) Z = GraphAttention(channels, activation='relu')([X, A]) ... model.fit([node_features, fltr], y) [source]","title":"GraphAttention"},{"location":"layers/convolution/#graphconvskip","text":"spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) , with the addition of skip connections. Mode : single, mixed, batch. This layer computes the transformation: Z = \\sigma(A X W_1 + X_0 W_2 + b) where X is the node features matrix, X_0 is the node features matrix for the skip connection, A is the normalized laplacian, W_1 and W_2 are the convolution kernels, b is a bias vector, and \\sigma is the activation function. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; node features for the skip connection of shape (batch, num_nodes, num_features) , depending on the mode; Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. The Laplacians can be computed from the adjacency matrices using utils.convolution.localpooling_filter . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage X = Input(shape=(num_nodes, num_features)) X_0 = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = GraphConvSkip(channels, activation='relu')([X, X_0, filter]) [source]","title":"GraphConvSkip"},{"location":"layers/convolution/#appnp","text":"spektral.layers.APPNP(channels, mlp_channels, alpha=0.2, H=1, K=1, mlp_activation='relu', dropout_rate=0.0, activation='softmax', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . Implementation by Filippo Bianchi. Mode : single, mixed, batch. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; normalized Laplacians of shape (batch, num_nodes, num_nodes) , depending on the mode. Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_channels : integer, number of hidden units for the MLP layers; alpha : teleport probability; H : number of MLP layers; K : number of power iterations; mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage I = sp.identity(adj.shape[0], dtype=adj.dtype) fltr = utils.normalize_adjacency(adj + I) ... X = Input(shape=(num_nodes, num_features)) filter = Input((num_nodes, num_nodes)) Z = APPNP(channels, mlp_channels)([X, filter]) ... model.fit([node_features, fltr], y)","title":"APPNP"},{"location":"layers/pooling/","text":"[source] GlobalAttentionPool spektral.layers.GlobalAttentionPool(channels=32, kernel_regularizer=None) A gated attention global pooling layer as presented by Li et al. (2017) . Note that this layer assumes the 'channels_last' data format, and cannot be used otherwise. Mode : single, batch. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; Output a pooled feature matrix of shape (batch, channels) ; Arguments channels : integer, number of output channels; kernel_regularizer : regularization applied to the gating networks; Usage X = Input(shape=(num_nodes, num_features)) Z = GlobalAttentionPool(channels)(X) [source] NodeAttentionPool spektral.layers.NodeAttentionPool(attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. Note that this layer assumes the 'channels_last' data format, and cannot be used otherwise. Mode : single, batch. Input node features of shape (batch, num_nodes, num_features) ; Output a pooled feature matrix of shape (batch, num_features) ; Arguments attn_kernel_initializer : initializer for the attention kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix; Usage X = Input(shape=(num_nodes, num_features)) Z = NodeAttentionPool()(X)","title":"Pooling Layers"},{"location":"layers/pooling/#globalattentionpool","text":"spektral.layers.GlobalAttentionPool(channels=32, kernel_regularizer=None) A gated attention global pooling layer as presented by Li et al. (2017) . Note that this layer assumes the 'channels_last' data format, and cannot be used otherwise. Mode : single, batch. Input node features of shape (batch, num_nodes, num_features) , depending on the mode; Output a pooled feature matrix of shape (batch, channels) ; Arguments channels : integer, number of output channels; kernel_regularizer : regularization applied to the gating networks; Usage X = Input(shape=(num_nodes, num_features)) Z = GlobalAttentionPool(channels)(X) [source]","title":"GlobalAttentionPool"},{"location":"layers/pooling/#nodeattentionpool","text":"spektral.layers.NodeAttentionPool(attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. Note that this layer assumes the 'channels_last' data format, and cannot be used otherwise. Mode : single, batch. Input node features of shape (batch, num_nodes, num_features) ; Output a pooled feature matrix of shape (batch, num_features) ; Arguments attn_kernel_initializer : initializer for the attention kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix; Usage X = Input(shape=(num_nodes, num_features)) Z = NodeAttentionPool()(X)","title":"NodeAttentionPool"},{"location":"utils/conversion/","text":"nx_to_adj spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices. nx_to_node_features spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_edge_features spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_numpy spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) numpy_to_nx spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"Conversion"},{"location":"utils/conversion/#nx_to_adj","text":"spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices.","title":"nx_to_adj"},{"location":"utils/conversion/#nx_to_node_features","text":"spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_node_features"},{"location":"utils/conversion/#nx_to_edge_features","text":"spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_edge_features"},{"location":"utils/conversion/#nx_to_numpy","text":"spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim)","title":"nx_to_numpy"},{"location":"utils/conversion/#numpy_to_nx","text":"spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"numpy_to_nx"},{"location":"utils/convolution/","text":"degree spektral.utils.degree(adj) Computes the degree matrix of the given adjacency matrix. Arguments adj : rank 2 array or sparse matrix Return The degree matrix in sparse DIA format degree_power spektral.utils.degree_power(adj, pow) Computes D^{p} from the given adjacency matrix. Useful for computing normalised Laplacians. Arguments adj : rank 2 array or sparse matrix pow : exponent to which elevate the degree matrix Return The exponentiated degree matrix in sparse DIA format normalized_adjacency spektral.utils.normalized_adjacency(adj, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either D^{-1}A or D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix. laplacian spektral.utils.laplacian(adj) Computes the Laplacian of the given adjacency matrix as D - A . Arguments adj : rank 2 array or sparse matrix; Return The Laplacian. normalized_laplacian spektral.utils.normalized_laplacian(adj, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as I - D^{-1}A or I - D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian. localpooling_filter spektral.utils.localpooling_filter(adj, symmetric=True) Computes the local pooling filter from the given adjacency matrix, as described by Kipf & Welling (2017). Arguments adj : a np.array or scipy.sparse matrix of rank 2 or 3; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return The filter matrix, as dense np.array. chebyshev_filter spektral.utils.chebyshev_filter(adj, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016). Arguments adj : a np.array or scipy.sparse matrix; k : integer, the order up to which to compute the Chebyshev polynomials; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return A list of k+1 filter matrices, as np.arrays.","title":"Convolution"},{"location":"utils/convolution/#degree","text":"spektral.utils.degree(adj) Computes the degree matrix of the given adjacency matrix. Arguments adj : rank 2 array or sparse matrix Return The degree matrix in sparse DIA format","title":"degree"},{"location":"utils/convolution/#degree_power","text":"spektral.utils.degree_power(adj, pow) Computes D^{p} from the given adjacency matrix. Useful for computing normalised Laplacians. Arguments adj : rank 2 array or sparse matrix pow : exponent to which elevate the degree matrix Return The exponentiated degree matrix in sparse DIA format","title":"degree_power"},{"location":"utils/convolution/#normalized_adjacency","text":"spektral.utils.normalized_adjacency(adj, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either D^{-1}A or D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix.","title":"normalized_adjacency"},{"location":"utils/convolution/#laplacian","text":"spektral.utils.laplacian(adj) Computes the Laplacian of the given adjacency matrix as D - A . Arguments adj : rank 2 array or sparse matrix; Return The Laplacian.","title":"laplacian"},{"location":"utils/convolution/#normalized_laplacian","text":"spektral.utils.normalized_laplacian(adj, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as I - D^{-1}A or I - D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian.","title":"normalized_laplacian"},{"location":"utils/convolution/#localpooling_filter","text":"spektral.utils.localpooling_filter(adj, symmetric=True) Computes the local pooling filter from the given adjacency matrix, as described by Kipf & Welling (2017). Arguments adj : a np.array or scipy.sparse matrix of rank 2 or 3; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return The filter matrix, as dense np.array.","title":"localpooling_filter"},{"location":"utils/convolution/#chebyshev_filter","text":"spektral.utils.chebyshev_filter(adj, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016). Arguments adj : a np.array or scipy.sparse matrix; k : integer, the order up to which to compute the Chebyshev polynomials; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return A list of k+1 filter matrices, as np.arrays.","title":"chebyshev_filter"},{"location":"utils/io/","text":"load_binary spektral.utils.load_binary(filename) Loads a pickled file. Arguments filename : a string or file-like object Return The loaded object dump_binary spektral.utils.dump_binary(obj, filename) Pickles and saves an object to file. Arguments obj : the object to save filename : a string or file-like object load_csv spektral.utils.load_csv(filename) Loads a csv file with pandas. Arguments filename : a string or file-like object Return The loaded csv dump_csv spektral.utils.dump_csv(df, filename, convert=False) Dumps a pd.DataFrame to csv. Arguments df : the pd.DataFrame to save or equivalent object filename : a string or file-like object convert : whether to attempt to convert the given object to pd.DataFrame before saving the csv. load_dot spektral.utils.load_dot(filename, force_graph=True) Loads a graph saved in .dot format. Arguments filename : a string or file-like object force_graph : whether to force a conversion to nx.Graph after loading. This may be useful in the case of .dot files being loaded as nx.MultiGraph. Return The loaded graph dump_dot spektral.utils.dump_dot(obj, filename) Dumps a nx.Graph to .dot file Arguments obj : the nx.Graph (or equivalent) to save filename : a string or file-like object load_npy spektral.utils.load_npy(filename) Loads a file saved by np.save. Arguments filename : a string or file-like object Return The loaded object dump_npy spektral.utils.dump_npy(obj, filename, zipped=False) Saves an object to file using the numpy format. Arguments obj : the object to save filename : a string or file-like object zipped : boolean, whether to save the object in the zipped format .npz rather than .npy load_txt spektral.utils.load_txt(filename) Loads a txt file using np.genfromtxt. Arguments filename : a string or file-like object Return The loaded object dump_txt spektral.utils.dump_txt(obj, filename) Saves an object to text file using np.savetxt. Arguments obj : the object to save filename : a string or file-like object load_sdf spektral.utils.load_sdf(filename, amount=None) Load an .sdf file and return a list of molecules in the internal SDF format. Arguments filename : target SDF file amount : only load the first amount molecules from the file Return A list of molecules in the internal SDF format (see documentation).","title":"Input/Ouput"},{"location":"utils/io/#load_binary","text":"spektral.utils.load_binary(filename) Loads a pickled file. Arguments filename : a string or file-like object Return The loaded object","title":"load_binary"},{"location":"utils/io/#dump_binary","text":"spektral.utils.dump_binary(obj, filename) Pickles and saves an object to file. Arguments obj : the object to save filename : a string or file-like object","title":"dump_binary"},{"location":"utils/io/#load_csv","text":"spektral.utils.load_csv(filename) Loads a csv file with pandas. Arguments filename : a string or file-like object Return The loaded csv","title":"load_csv"},{"location":"utils/io/#dump_csv","text":"spektral.utils.dump_csv(df, filename, convert=False) Dumps a pd.DataFrame to csv. Arguments df : the pd.DataFrame to save or equivalent object filename : a string or file-like object convert : whether to attempt to convert the given object to pd.DataFrame before saving the csv.","title":"dump_csv"},{"location":"utils/io/#load_dot","text":"spektral.utils.load_dot(filename, force_graph=True) Loads a graph saved in .dot format. Arguments filename : a string or file-like object force_graph : whether to force a conversion to nx.Graph after loading. This may be useful in the case of .dot files being loaded as nx.MultiGraph. Return The loaded graph","title":"load_dot"},{"location":"utils/io/#dump_dot","text":"spektral.utils.dump_dot(obj, filename) Dumps a nx.Graph to .dot file Arguments obj : the nx.Graph (or equivalent) to save filename : a string or file-like object","title":"dump_dot"},{"location":"utils/io/#load_npy","text":"spektral.utils.load_npy(filename) Loads a file saved by np.save. Arguments filename : a string or file-like object Return The loaded object","title":"load_npy"},{"location":"utils/io/#dump_npy","text":"spektral.utils.dump_npy(obj, filename, zipped=False) Saves an object to file using the numpy format. Arguments obj : the object to save filename : a string or file-like object zipped : boolean, whether to save the object in the zipped format .npz rather than .npy","title":"dump_npy"},{"location":"utils/io/#load_txt","text":"spektral.utils.load_txt(filename) Loads a txt file using np.genfromtxt. Arguments filename : a string or file-like object Return The loaded object","title":"load_txt"},{"location":"utils/io/#dump_txt","text":"spektral.utils.dump_txt(obj, filename) Saves an object to text file using np.savetxt. Arguments obj : the object to save filename : a string or file-like object","title":"dump_txt"},{"location":"utils/io/#load_sdf","text":"spektral.utils.load_sdf(filename, amount=None) Load an .sdf file and return a list of molecules in the internal SDF format. Arguments filename : target SDF file amount : only load the first amount molecules from the file Return A list of molecules in the internal SDF format (see documentation).","title":"load_sdf"},{"location":"utils/misc/","text":"batch_iterator spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with equal first dimension. batch_size : number of samples in a batch epochs : number of times to iterate over the data shuffle : whether to shuffle the data at the beginning of each epoch :yield: a batch of samples (or tuple of batches if X had more than one array). set_trainable spektral.utils.set_trainable(model, toset) Sets the trainable parameters of a Keras model and all its layers to toset. Arguments model : a Keras Model toset : boolean Return None pad_jagged_array spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape add_eye spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above sub_eye spektral.utils.sub_eye(x) Subtracts the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above","title":"Miscellaneous"},{"location":"utils/misc/#batch_iterator","text":"spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with equal first dimension. batch_size : number of samples in a batch epochs : number of times to iterate over the data shuffle : whether to shuffle the data at the beginning of each epoch :yield: a batch of samples (or tuple of batches if X had more than one array).","title":"batch_iterator"},{"location":"utils/misc/#set_trainable","text":"spektral.utils.set_trainable(model, toset) Sets the trainable parameters of a Keras model and all its layers to toset. Arguments model : a Keras Model toset : boolean Return None","title":"set_trainable"},{"location":"utils/misc/#pad_jagged_array","text":"spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape","title":"pad_jagged_array"},{"location":"utils/misc/#add_eye","text":"spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above","title":"add_eye"},{"location":"utils/misc/#sub_eye","text":"spektral.utils.sub_eye(x) Subtracts the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above","title":"sub_eye"},{"location":"utils/plotting/","text":"plot_numpy spektral.utils.plot_numpy(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Converts a graph in matrix format (i.e. with adjacency matrix, node features matrix, and edge features matrix) to the Networkx format, then plots it with plot_nx(). Arguments adj : np.array, adjacency matrix of the graph node_features : np.array, node features matrix of the graph edge_features : np.array, edge features matrix of the graph nf_name : name to assign to the node features ef_name : name to assign to the edge features layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None plot_nx spektral.utils.plot_nx(nx_graph, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Plot the given Networkx graph. Arguments nx_graph : a Networkx graph nf_name : name of the relevant node feature to plot ef_name : name of the relevant edgee feature to plot layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None","title":"Plotting"},{"location":"utils/plotting/#plot_numpy","text":"spektral.utils.plot_numpy(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Converts a graph in matrix format (i.e. with adjacency matrix, node features matrix, and edge features matrix) to the Networkx format, then plots it with plot_nx(). Arguments adj : np.array, adjacency matrix of the graph node_features : np.array, node features matrix of the graph edge_features : np.array, edge features matrix of the graph nf_name : name to assign to the node features ef_name : name to assign to the edge features layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None","title":"plot_numpy"},{"location":"utils/plotting/#plot_nx","text":"spektral.utils.plot_nx(nx_graph, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Plot the given Networkx graph. Arguments nx_graph : a Networkx graph nf_name : name of the relevant node feature to plot ef_name : name of the relevant edgee feature to plot layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None","title":"plot_nx"}]}