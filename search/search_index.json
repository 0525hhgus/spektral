{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Spektral Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the nodes of a network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph Convolutional Networks (GCN) Chebyshev networks (ChebNets) GraphSAGE ARMA convolutions Edge-Conditioned Convolutions (ECC) Graph attention networks (GAT) Approximated Personalized Propagation of Neural Predictions (APPNP) Graph Isomorphism Networks (GIN) You can also find pooling layers , including: DiffPool MinCUT pooling Top-K pooling Self-Attention Graph (SAG) pooling Global sum, average, and max pooling Global gated attention pooling Spektral also includes lots of utilities for your graph deep learning projects. See how to get started with Spektral and have a look at the examples for some templates. The source code of the project is available on Github . Read the documentation here . Installation Spektral is compatible with Python 3.5+, and is tested on Ubuntu 16.04+ and MacOS. Other Linux distros should work as well, but Windows is not supported for now. To install the required dependencies on Ubuntu run: sudo apt install graphviz libgraphviz-dev libcgraph6 Some optional features of Spektral also depend on RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda). The simplest way to install Spektral is from PyPi: pip install spektral To install Spektral from source, run this in a terminal: git clone https://github.com/danielegrattarola/spektral.git cd spektral python setup.py install # Or 'pip install .' To install Spektral on Google Colab : ! apt install graphviz libgraphviz-dev libcgraph6 ! pip install spektral TensorFlow 1 and Keras Starting from version 0.3, Spektral only supports TensorFlow 2 and tf.keras . The old version of Spektral, which is based on TensorFlow 1 and the stand-alone Keras library, is still available on the tf1 branch on GitHub and can be installed from source: git clone https://github.com/danielegrattarola/spektral.git cd spektral git checkout tf1 python setup.py install # Or 'pip install .' In the future, the TF1-compatible version of Spektral (<0.2) will receive bug fixes, but all new features will only support TensorFlow 2. Contributing Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework.","title":"Home"},{"location":"#welcome-to-spektral","text":"Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the nodes of a network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph Convolutional Networks (GCN) Chebyshev networks (ChebNets) GraphSAGE ARMA convolutions Edge-Conditioned Convolutions (ECC) Graph attention networks (GAT) Approximated Personalized Propagation of Neural Predictions (APPNP) Graph Isomorphism Networks (GIN) You can also find pooling layers , including: DiffPool MinCUT pooling Top-K pooling Self-Attention Graph (SAG) pooling Global sum, average, and max pooling Global gated attention pooling Spektral also includes lots of utilities for your graph deep learning projects. See how to get started with Spektral and have a look at the examples for some templates. The source code of the project is available on Github . Read the documentation here .","title":"Welcome to Spektral"},{"location":"#installation","text":"Spektral is compatible with Python 3.5+, and is tested on Ubuntu 16.04+ and MacOS. Other Linux distros should work as well, but Windows is not supported for now. To install the required dependencies on Ubuntu run: sudo apt install graphviz libgraphviz-dev libcgraph6 Some optional features of Spektral also depend on RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda). The simplest way to install Spektral is from PyPi: pip install spektral To install Spektral from source, run this in a terminal: git clone https://github.com/danielegrattarola/spektral.git cd spektral python setup.py install # Or 'pip install .' To install Spektral on Google Colab : ! apt install graphviz libgraphviz-dev libcgraph6 ! pip install spektral","title":"Installation"},{"location":"#tensorflow-1-and-keras","text":"Starting from version 0.3, Spektral only supports TensorFlow 2 and tf.keras . The old version of Spektral, which is based on TensorFlow 1 and the stand-alone Keras library, is still available on the tf1 branch on GitHub and can be installed from source: git clone https://github.com/danielegrattarola/spektral.git cd spektral git checkout tf1 python setup.py install # Or 'pip install .' In the future, the TF1-compatible version of Spektral (<0.2) will receive bug fixes, but all new features will only support TensorFlow 2.","title":"TensorFlow 1 and Keras"},{"location":"#contributing","text":"Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework.","title":"Contributing"},{"location":"about/","text":"About Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"about/#about","text":"Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"chem/","text":"This module provides some functions to work with molecules, and requires the RDKit library to be installed on the system. numpy_to_rdkit spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule numpy_to_smiles spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule rdkit_to_smiles spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule sdf_to_nx spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs. nx_to_sdf spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format. validate_rdkit spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise get_atomic_symbol spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol get_atomic_num spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118 valid_score spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule novel_score spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule unique_score spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules enable_rdkit_log spektral.chem.enable_rdkit_log() Enables RDkit logging. Return plot_rdkit spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return The image as np.array plot_rdkit_svg_grid spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"Chemistry"},{"location":"chem/#numpy_to_rdkit","text":"spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule","title":"numpy_to_rdkit"},{"location":"chem/#numpy_to_smiles","text":"spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule","title":"numpy_to_smiles"},{"location":"chem/#rdkit_to_smiles","text":"spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule","title":"rdkit_to_smiles"},{"location":"chem/#sdf_to_nx","text":"spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs.","title":"sdf_to_nx"},{"location":"chem/#nx_to_sdf","text":"spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format.","title":"nx_to_sdf"},{"location":"chem/#validate_rdkit","text":"spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise","title":"validate_rdkit"},{"location":"chem/#get_atomic_symbol","text":"spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol","title":"get_atomic_symbol"},{"location":"chem/#get_atomic_num","text":"spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118","title":"get_atomic_num"},{"location":"chem/#valid_score","text":"spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule","title":"valid_score"},{"location":"chem/#novel_score","text":"spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule","title":"novel_score"},{"location":"chem/#unique_score","text":"spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules","title":"unique_score"},{"location":"chem/#enable_rdkit_log","text":"spektral.chem.enable_rdkit_log() Enables RDkit logging. Return","title":"enable_rdkit_log"},{"location":"chem/#plot_rdkit","text":"spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return The image as np.array","title":"plot_rdkit"},{"location":"chem/#plot_rdkit_svg_grid","text":"spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"plot_rdkit_svg_grid"},{"location":"data/","text":"Representing graphs Spektral uses a matrix-based representation for manipulating graphs and feeding them to neural networks. This approach is one of the most commonly used in the literature on graph neural networks, and it's perfect to perform parallel computations on GPU. A graph is generally represented by three matrices: A \\in \\{0, 1\\}^{N \\times N} , a square adjacency matrix where A_{ij} = 1 if there is a connection between nodes i and j , and A_{ij} = 0 otherwise; X \\in \\mathbb{R}^{N \\times F} , a matrix encoding node attributes, where each row represents the F -dimensional attribute vector of a node; E \\in \\mathbb{R}^{N \\times N \\times S} , a matrix encoding edge attributes, where each entry represents the S -dimensional attribute vector of an edge; Some frameworks (like the graph networks proposed by Battaglia et al.) also include a feature vector describing the global state of the graph, but this is not supported by Spektral for now. In code, and in this documentation, we use the following convention to refer to the formulation above: A is the adjacency matrix, N is the number of nodes; X is the node attributes matrix, F is the size of the node attributes; E is the edge attributes matrix, S is the size of the edge attributes; See the table below for how these matrices are represented in Numpy. Modes In Spektral, some functionalities are implemented to work on a single graph, while others consider batches of graphs. To understand the difference between the two settings, consider the difference between classifying the nodes of a citation network, and classifying the chemical properties of molecules. For the citation network, we are interested in the individual nodes and the connections between them. Node and edge attributes are specific to each individual network, and we are usually not interested in training models that work on different networks. The nodes themselves are our data. On the other hand, when working with molecules in a dataset, we are in a much more familiar setting. Each molecule is a sample of our dataset, and the atoms and bonds that make up the molecules are the constituent part of each data point (like pixels in images). In this case, we are interested in finding patterns that describe the properties of the molecules in general. The two settings require us to do things that are conceptually similar, but that need some minor adjustments in how the data is processed by our graph neural networks. This is why Spektral makes these differences explicit. In practice, we actually distinguish between three main modes of operation: single , where we have a single graph, with fixed topology and attributes; batch , where we have a set of different graphs, each with its own topology and attributes; mixed , where we have a graph with fixed topology, but a set of different attributes (usually called graph signals ); this can be seen as a particular case of the batch mode, but it is handled separately in Spektral to improve memory efficiency. We also have the disjoint mode, which is a simple trick to represent a batch of graphs in single mode. This requires an additional data structure to keep track of the graphs, and is explained in detail at the end of this section. The difference between the three main modes can be easily seen in how A , X , and E have different shapes in each case: Mode A.shape X.shape E.shape Single (N, N) (N, F) (N, N, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) (batch, N, N, S) Single mode In single mode the data describes a single graph. Three very popular datasets in this setting are the citation networks, Cora, Citeseer, and Pubmed. To load a citation network, you can use the built-in loader: In [1]: from spektral.datasets import citation Using TensorFlow backend. In [2]: A, X, _, _, _, _ = citation.load_data('cora') Loading cora dataset In [3]: A.shape Out[3]: (2708, 2708) In [4]: X.shape Out[4]: (2708, 1433) When training GNNs in single mode, we cannot batch and shuffle the data along the first axis, and the whole graph must be fed to the model at each step (see the node classification example ). Batch mode In batch mode , the matrices will have a batch dimension first. For instance, we can load the QM9 chemical database of small molecules as follows: In [1]: from spektral.datasets import qm9 Using TensorFlow backend. In [2]: A, X, E, _ = qm9.load_data() Loading QM9 dataset. Reading SDF 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133885/133885 [00:29<00:00, 4579.22it/s] In [3]: A.shape Out[3]: (133885, 9, 9) In [4]: X.shape Out[4]: (133885, 9, 6) In [5]: E.shape Out[5]: (133885, 9, 9, 1) Note that the graphs in QM9 have variable order (i.e., a different number of nodes for each graph), and that by default load_data() pads them with zeros in order to store the data in Numpy arrays. See the disjoint mode section for an alternative to zero-padding. Mixed mode In mixed mode we consider a single adjacency matrix, and different node and edge attributes matrices. An example of a mixed mode dataset is the MNIST random grid proposed by Defferrard et al. : In [1]: from spektral.datasets import mnist Using TensorFlow backend. In [2]: X, _, _, _, _, _, A = mnist.load_data() In [3]: A.shape Out[3]: (784, 784) In [4]: X.shape Out[4]: (50000, 784, 1) Disjoint mode When dealing with graphs with a variable number of nodes, representing a group of graphs in batch mode requires padding A , X , and E to a fixed dimension. In order to avoid this issue, a common approach is to represent a batch of graphs with their disjoint union, leading us back to single mode. The disjoint union of a batch of graphs is a graph where: A is a block diagonal matrix, constructed from the adjacency matrices of the batch; X is obtained by stacking the node attributes of the batch; E is a block diagonal tensor of rank 3, obtained from the edge attributes; In order to keep track of different graphs in the disjoint union, we use an additional array of integers I , that maps each node to a graph with a progressive zero-based index (color coded in the image above). Utilities for creating the disjoint union of a list of graphs are provided in spektral.utils.data : In [1]: from spektral.utils.data import Batch Using TensorFlow backend. In [2]: A_list = [np.ones((2, 2))] * 3 In [3]: X_list = [np.random.normal(size=(2, 4))] * 3 In [4]: b = Batch(A_list, X_list) In [5]: b.A.todense() Out[5]: matrix([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1.]]) In [6]: b.X Out[6]: array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]) In [7]: b.I Out[7]: array([0, 0, 1, 1, 2, 2]) In [8]: b.get('AXI') Out[8]: (<6x6 sparse matrix of type '<class 'numpy.float64'>' with 12 stored elements in COOrdinate format>, array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]), array([0, 0, 1, 1, 2, 2])) Convolutional layers that work in single mode will work for this type of data representation, without any modification. Pooling layers, on the other hand, require the index vector I to know which nodes to pool together. Global pooling layers will consume I and reduce the graphs to single vectors. Standard pooling layers will return a reduced version of I along with the reduced graphs. Conversion methods To provide better compatibility with other libraries, Spektral has methods to convert graphs between the matrix representation ( 'numpy' ) and other formats. The 'networkx' format represents graphs using the Networkx library, which can then be used to convert the graphs to other formats like .dot and edge lists. Conversion utils between 'numpy' and 'networkx' are provided in spektral.utils.conversion . Molecules When working with molecules, some specific formats can be used to represent the graphs. The 'sdf' format is an internal representation format used to store an SDF file as a dictionary. A molecule in 'sdf' format will look like this: {'atoms': [{'atomic_num': 7, 'charge': 0, 'coords': array([-0.0299, 1.2183, 0.2994]), 'index': 0, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}, ..., {'atomic_num': 1, 'charge': 0, 'coords': array([ 0.6896, -2.3002, -0.1042]), 'index': 14, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}], 'bonds': [{'end_atom': 13, 'info': array([0, 0, 0]), 'start_atom': 4, 'stereo': 0, 'type': 1}, ..., {'end_atom': 8, 'info': array([0, 0, 0]), 'start_atom': 7, 'stereo': 0, 'type': 3}], 'comment': '', 'data': [''], 'details': '-OEChem-03231823253D', 'n_atoms': 15, 'n_bonds': 15, 'name': 'gdb_54964', 'properties': []} The 'rdkit' format uses the RDKit library to represent molecules, and offers several methods to manipulate molecules with a chemistry-oriented approach. The 'smiles' format represents molecules as strings, and can be used as a space-efficient way to store molecules or perform quick checks on a dataset (e.g., counting the unique number of molecules in a dataset is quicker if all molecules are converted to SMILES first). The spektral.chem and spektral.utils modules offer conversion methods between all of these formats, although some conversions may need more than one step (e.g., 'sdf' to 'networkx' to 'numpy' to 'smiles' ).","title":"Data representation"},{"location":"data/#representing-graphs","text":"Spektral uses a matrix-based representation for manipulating graphs and feeding them to neural networks. This approach is one of the most commonly used in the literature on graph neural networks, and it's perfect to perform parallel computations on GPU. A graph is generally represented by three matrices: A \\in \\{0, 1\\}^{N \\times N} , a square adjacency matrix where A_{ij} = 1 if there is a connection between nodes i and j , and A_{ij} = 0 otherwise; X \\in \\mathbb{R}^{N \\times F} , a matrix encoding node attributes, where each row represents the F -dimensional attribute vector of a node; E \\in \\mathbb{R}^{N \\times N \\times S} , a matrix encoding edge attributes, where each entry represents the S -dimensional attribute vector of an edge; Some frameworks (like the graph networks proposed by Battaglia et al.) also include a feature vector describing the global state of the graph, but this is not supported by Spektral for now. In code, and in this documentation, we use the following convention to refer to the formulation above: A is the adjacency matrix, N is the number of nodes; X is the node attributes matrix, F is the size of the node attributes; E is the edge attributes matrix, S is the size of the edge attributes; See the table below for how these matrices are represented in Numpy.","title":"Representing graphs"},{"location":"data/#modes","text":"In Spektral, some functionalities are implemented to work on a single graph, while others consider batches of graphs. To understand the difference between the two settings, consider the difference between classifying the nodes of a citation network, and classifying the chemical properties of molecules. For the citation network, we are interested in the individual nodes and the connections between them. Node and edge attributes are specific to each individual network, and we are usually not interested in training models that work on different networks. The nodes themselves are our data. On the other hand, when working with molecules in a dataset, we are in a much more familiar setting. Each molecule is a sample of our dataset, and the atoms and bonds that make up the molecules are the constituent part of each data point (like pixels in images). In this case, we are interested in finding patterns that describe the properties of the molecules in general. The two settings require us to do things that are conceptually similar, but that need some minor adjustments in how the data is processed by our graph neural networks. This is why Spektral makes these differences explicit. In practice, we actually distinguish between three main modes of operation: single , where we have a single graph, with fixed topology and attributes; batch , where we have a set of different graphs, each with its own topology and attributes; mixed , where we have a graph with fixed topology, but a set of different attributes (usually called graph signals ); this can be seen as a particular case of the batch mode, but it is handled separately in Spektral to improve memory efficiency. We also have the disjoint mode, which is a simple trick to represent a batch of graphs in single mode. This requires an additional data structure to keep track of the graphs, and is explained in detail at the end of this section. The difference between the three main modes can be easily seen in how A , X , and E have different shapes in each case: Mode A.shape X.shape E.shape Single (N, N) (N, F) (N, N, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) (batch, N, N, S)","title":"Modes"},{"location":"data/#single-mode","text":"In single mode the data describes a single graph. Three very popular datasets in this setting are the citation networks, Cora, Citeseer, and Pubmed. To load a citation network, you can use the built-in loader: In [1]: from spektral.datasets import citation Using TensorFlow backend. In [2]: A, X, _, _, _, _ = citation.load_data('cora') Loading cora dataset In [3]: A.shape Out[3]: (2708, 2708) In [4]: X.shape Out[4]: (2708, 1433) When training GNNs in single mode, we cannot batch and shuffle the data along the first axis, and the whole graph must be fed to the model at each step (see the node classification example ).","title":"Single mode"},{"location":"data/#batch-mode","text":"In batch mode , the matrices will have a batch dimension first. For instance, we can load the QM9 chemical database of small molecules as follows: In [1]: from spektral.datasets import qm9 Using TensorFlow backend. In [2]: A, X, E, _ = qm9.load_data() Loading QM9 dataset. Reading SDF 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133885/133885 [00:29<00:00, 4579.22it/s] In [3]: A.shape Out[3]: (133885, 9, 9) In [4]: X.shape Out[4]: (133885, 9, 6) In [5]: E.shape Out[5]: (133885, 9, 9, 1) Note that the graphs in QM9 have variable order (i.e., a different number of nodes for each graph), and that by default load_data() pads them with zeros in order to store the data in Numpy arrays. See the disjoint mode section for an alternative to zero-padding.","title":"Batch mode"},{"location":"data/#mixed-mode","text":"In mixed mode we consider a single adjacency matrix, and different node and edge attributes matrices. An example of a mixed mode dataset is the MNIST random grid proposed by Defferrard et al. : In [1]: from spektral.datasets import mnist Using TensorFlow backend. In [2]: X, _, _, _, _, _, A = mnist.load_data() In [3]: A.shape Out[3]: (784, 784) In [4]: X.shape Out[4]: (50000, 784, 1)","title":"Mixed mode"},{"location":"data/#disjoint-mode","text":"When dealing with graphs with a variable number of nodes, representing a group of graphs in batch mode requires padding A , X , and E to a fixed dimension. In order to avoid this issue, a common approach is to represent a batch of graphs with their disjoint union, leading us back to single mode. The disjoint union of a batch of graphs is a graph where: A is a block diagonal matrix, constructed from the adjacency matrices of the batch; X is obtained by stacking the node attributes of the batch; E is a block diagonal tensor of rank 3, obtained from the edge attributes; In order to keep track of different graphs in the disjoint union, we use an additional array of integers I , that maps each node to a graph with a progressive zero-based index (color coded in the image above). Utilities for creating the disjoint union of a list of graphs are provided in spektral.utils.data : In [1]: from spektral.utils.data import Batch Using TensorFlow backend. In [2]: A_list = [np.ones((2, 2))] * 3 In [3]: X_list = [np.random.normal(size=(2, 4))] * 3 In [4]: b = Batch(A_list, X_list) In [5]: b.A.todense() Out[5]: matrix([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1.]]) In [6]: b.X Out[6]: array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]) In [7]: b.I Out[7]: array([0, 0, 1, 1, 2, 2]) In [8]: b.get('AXI') Out[8]: (<6x6 sparse matrix of type '<class 'numpy.float64'>' with 12 stored elements in COOrdinate format>, array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]), array([0, 0, 1, 1, 2, 2])) Convolutional layers that work in single mode will work for this type of data representation, without any modification. Pooling layers, on the other hand, require the index vector I to know which nodes to pool together. Global pooling layers will consume I and reduce the graphs to single vectors. Standard pooling layers will return a reduced version of I along with the reduced graphs.","title":"Disjoint mode"},{"location":"data/#conversion-methods","text":"To provide better compatibility with other libraries, Spektral has methods to convert graphs between the matrix representation ( 'numpy' ) and other formats. The 'networkx' format represents graphs using the Networkx library, which can then be used to convert the graphs to other formats like .dot and edge lists. Conversion utils between 'numpy' and 'networkx' are provided in spektral.utils.conversion .","title":"Conversion methods"},{"location":"data/#molecules","text":"When working with molecules, some specific formats can be used to represent the graphs. The 'sdf' format is an internal representation format used to store an SDF file as a dictionary. A molecule in 'sdf' format will look like this: {'atoms': [{'atomic_num': 7, 'charge': 0, 'coords': array([-0.0299, 1.2183, 0.2994]), 'index': 0, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}, ..., {'atomic_num': 1, 'charge': 0, 'coords': array([ 0.6896, -2.3002, -0.1042]), 'index': 14, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}], 'bonds': [{'end_atom': 13, 'info': array([0, 0, 0]), 'start_atom': 4, 'stereo': 0, 'type': 1}, ..., {'end_atom': 8, 'info': array([0, 0, 0]), 'start_atom': 7, 'stereo': 0, 'type': 3}], 'comment': '', 'data': [''], 'details': '-OEChem-03231823253D', 'n_atoms': 15, 'n_bonds': 15, 'name': 'gdb_54964', 'properties': []} The 'rdkit' format uses the RDKit library to represent molecules, and offers several methods to manipulate molecules with a chemistry-oriented approach. The 'smiles' format represents molecules as strings, and can be used as a space-efficient way to store molecules or perform quick checks on a dataset (e.g., counting the unique number of molecules in a dataset is quicker if all molecules are converted to SMILES first). The spektral.chem and spektral.utils modules offer conversion methods between all of these formats, although some conversions may need more than one step (e.g., 'sdf' to 'networkx' to 'numpy' to 'smiles' ).","title":"Molecules"},{"location":"examples/","text":"Examples This is a collection of example scripts that you can use as template to solve your own tasks. Node classification Node classification on citation networks with GCN ; Node classification on citation networks with ChebNets ; Node classification on citation networks with GAT ; Node classification on citation networks with ARMA ; Node classification on citation networks with SimpleGCN ; Node classification on the Open Graph Benchmark dataset (ogbn-proteins) ; Graph-level prediction Batch mode: Classification of synthetic graphs with GAT ; Regression of molecular properties on QM9 with ECC ; Disjoint mode: Classification of synthetic graphs with TopK pooling ; Regression of molecular properties on QM9 with ECC ; Graph signal classification Graph signal classification on MNIST (mixed mode) ; Other applications Node clustering on citation networks with minCUT pooling (unsupervised) ; The following notebooks are available on Kaggle with more visualizations (maintained by @kmader ): MNIST Graph Deep Learning ; MNIST Graph Pooling ;","title":"Examples"},{"location":"examples/#examples","text":"This is a collection of example scripts that you can use as template to solve your own tasks.","title":"Examples"},{"location":"examples/#node-classification","text":"Node classification on citation networks with GCN ; Node classification on citation networks with ChebNets ; Node classification on citation networks with GAT ; Node classification on citation networks with ARMA ; Node classification on citation networks with SimpleGCN ; Node classification on the Open Graph Benchmark dataset (ogbn-proteins) ;","title":"Node classification"},{"location":"examples/#graph-level-prediction","text":"Batch mode: Classification of synthetic graphs with GAT ; Regression of molecular properties on QM9 with ECC ; Disjoint mode: Classification of synthetic graphs with TopK pooling ; Regression of molecular properties on QM9 with ECC ;","title":"Graph-level prediction"},{"location":"examples/#graph-signal-classification","text":"Graph signal classification on MNIST (mixed mode) ;","title":"Graph signal classification"},{"location":"examples/#other-applications","text":"Node clustering on citation networks with minCUT pooling (unsupervised) ; The following notebooks are available on Kaggle with more visualizations (maintained by @kmader ): MNIST Graph Deep Learning ; MNIST Graph Pooling ;","title":"Other applications"},{"location":"getting-started/","text":"Getting started Spektral is designed according to the guiding principles of the Keras API to make things extremely simple for beginners while maintaining flexibility for experts and researchers. The most important modules of Spektral are layers.convolutional and layers.pooling , which offer a number of popular layers to start building graph neural networks (GNNs) right away. Because Spektral is designed as an extension of Keras, you can plug any Spektral layer into an existing Keras Model without modifications. Node classification on citation networks In this example, we will build a simple Graph Convolutional Network for semi-supervised classification of nodes. This is a simple but challenging task that consists of classifying text documents in a citation network . In this type of graph, each node represents a document and is associated to a binary bag-of-words attribute (1 if a given word appears in the text, 0 otherwise). If a document cites another, then there exist an undirected edge between the two corresponding nodes. Finally, each node has a class label that we want to predict. This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels. The datasets.citation module of Spektral lets you to download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as: from spektral.datasets import citation data = citation.load_data('cora') A, X, y, train_mask, val_mask, test_mask = data X = X.toarray() N = A.shape[0] F = X.shape[-1] n_classes = y.shape[-1] This will load the network's adjacency matrix A as a Scipy sparse matrix of shape (N, N) , the node features X of shape (N, F) , and the labels y of shape (N, n_classes) . The loader will also return some boolean masks to know which nodes belong to the training, validation and test sets ( train_mask, val_mask, test_mask ). Creating a GNN To create a GCN, we will use the GraphConv layer and the functional API of Keras: from spektral.layers import GraphConv from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dropout Building the model is no different than building any Keras model, but we will need to provide multiple inputs to the GraphConv layers (namely A and X ): # Model definition X_in = Input(shape=(F, )) # Input layer for X A_in = Input((N, ), sparse=True) # Input layer for A graph_conv_1 = GraphConv(16, activation='relu')([X_in, A_in]) dropout = Dropout(0.5)(graph_conv_1) graph_conv_2 = GraphConv(n_classes, activation='softmax')([dropout, A_in]) # Build model model = Model(inputs=[X_in, A_in], outputs=graph_conv_2) And that's it. We just built our first GNN in Spektral and Keras. Note how we used the familiar API of Keras to create the GCN layers, as well as the standard Dropout layer to regularize our model. All features of Keras are also supported by Spektral (including initializers, regularizers, etc.). An important thing to notice at this point is how we defined the Input layers of our model. Because the \"elements\" of our dataset are the node themselves, we are telling Keras to consider each node as a separate sample, so that the \"batch\" axis is implicitly defined as None . In other words, a sample of the node attributes will be a vector of shape (F, ) and a sample of the adjacency matrix will be one of its rows of shape (N, ) . Keep this detail in mind for later. Training the GNN Before training the model, we have to pre-process the adjacency matrix to scale the weights of a node's connections according to its degree. In other words, the more a node is connected to others, the less relative importance those connections have. Most GNN layers available in Spektral require their own type of pre-processing in order to work correctly. You can find all necessary tools for pre-processing A in spektral.utils . In our example, the pre-processing required by GCN is: from spektral import utils A = utils.localpooling_filter(A).astype('f4') And that's all! What's left now for us is to compile and train our model: model.compile(optimizer='adam', loss='categorical_crossentropy', weighted_metrics=['acc']) model.summary() Note that we used the weighted_metrics argument instead of the usual metrics . This is due to the particular semi-supervised problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later). We can now train the model using the native fit() method of Keras, no modifications needed: # Train model validation_data = ([X, A], y, val_mask) model.fit([X, A], y, sample_weight=train_mask, epochs=100, batch_size=N, validation_data=validation_data, shuffle=False) # Shuffling data means shuffling the whole graph There are a couple of things to note here. We have set batch_size=N and shuffle=False . This is because, since we are considering our nodes as samples in a dataset, the default behaviour of Keras will be to split our graph into batches of 32, and shuffle the batches at each epoch. However, shuffling the adjacency matrix along one axis and not the other means that row i will represent a different node than column i . At the same time, if we split the graph in batches, we can easily end up in a situation where we need to use a node attribute that is not part of the batch. The only solution is to take all the node features at the same time, hence batch_size=N . Finally, we used train_mask and val_mask as sample_weight . This means that, during training, the training nodes will have a weight of 1 and the validation nodes will have a weight of 0. Then, in validation, we will set the training nodes to have a weight of 0 and the validation nodes to have a weight of 1. This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X , A , and y for both training and valdation? The only thing that changes is the mask. This is also why we used the weighted_metrics keyword when compiling the model, so that our accuracy is calculated only on the correct nodes at each phase. Evaluating the model Once again, evaluation is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training (note that in model.evaluate() , by default, shuffle=False ): # Evaluate model eval_results = model.evaluate([X, A], y, sample_weight=test_mask, batch_size=N) print('Done.\\n' 'Test loss: {}\\n' 'Test accuracy: {}'.format(*eval_results)) Done! Our model has been defined, trained, and evaluated. Go create! You are now ready to use Spektral to create your own models. If you want to build a GNN for a specific task, chances are that the things you need to define the model and pre-process the data are already part of Spektral. Check the examples for some ideas and practical tips. Remember to read the data representation section to learn about how GNNs can be used to solve different machine learning problems on graphs. Make sure to check the documentation, and leave a comment on Github if you have a feature that you want to see implemented.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Spektral is designed according to the guiding principles of the Keras API to make things extremely simple for beginners while maintaining flexibility for experts and researchers. The most important modules of Spektral are layers.convolutional and layers.pooling , which offer a number of popular layers to start building graph neural networks (GNNs) right away. Because Spektral is designed as an extension of Keras, you can plug any Spektral layer into an existing Keras Model without modifications.","title":"Getting started"},{"location":"getting-started/#node-classification-on-citation-networks","text":"In this example, we will build a simple Graph Convolutional Network for semi-supervised classification of nodes. This is a simple but challenging task that consists of classifying text documents in a citation network . In this type of graph, each node represents a document and is associated to a binary bag-of-words attribute (1 if a given word appears in the text, 0 otherwise). If a document cites another, then there exist an undirected edge between the two corresponding nodes. Finally, each node has a class label that we want to predict. This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels. The datasets.citation module of Spektral lets you to download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as: from spektral.datasets import citation data = citation.load_data('cora') A, X, y, train_mask, val_mask, test_mask = data X = X.toarray() N = A.shape[0] F = X.shape[-1] n_classes = y.shape[-1] This will load the network's adjacency matrix A as a Scipy sparse matrix of shape (N, N) , the node features X of shape (N, F) , and the labels y of shape (N, n_classes) . The loader will also return some boolean masks to know which nodes belong to the training, validation and test sets ( train_mask, val_mask, test_mask ).","title":"Node classification on citation networks"},{"location":"getting-started/#creating-a-gnn","text":"To create a GCN, we will use the GraphConv layer and the functional API of Keras: from spektral.layers import GraphConv from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dropout Building the model is no different than building any Keras model, but we will need to provide multiple inputs to the GraphConv layers (namely A and X ): # Model definition X_in = Input(shape=(F, )) # Input layer for X A_in = Input((N, ), sparse=True) # Input layer for A graph_conv_1 = GraphConv(16, activation='relu')([X_in, A_in]) dropout = Dropout(0.5)(graph_conv_1) graph_conv_2 = GraphConv(n_classes, activation='softmax')([dropout, A_in]) # Build model model = Model(inputs=[X_in, A_in], outputs=graph_conv_2) And that's it. We just built our first GNN in Spektral and Keras. Note how we used the familiar API of Keras to create the GCN layers, as well as the standard Dropout layer to regularize our model. All features of Keras are also supported by Spektral (including initializers, regularizers, etc.). An important thing to notice at this point is how we defined the Input layers of our model. Because the \"elements\" of our dataset are the node themselves, we are telling Keras to consider each node as a separate sample, so that the \"batch\" axis is implicitly defined as None . In other words, a sample of the node attributes will be a vector of shape (F, ) and a sample of the adjacency matrix will be one of its rows of shape (N, ) . Keep this detail in mind for later.","title":"Creating a GNN"},{"location":"getting-started/#training-the-gnn","text":"Before training the model, we have to pre-process the adjacency matrix to scale the weights of a node's connections according to its degree. In other words, the more a node is connected to others, the less relative importance those connections have. Most GNN layers available in Spektral require their own type of pre-processing in order to work correctly. You can find all necessary tools for pre-processing A in spektral.utils . In our example, the pre-processing required by GCN is: from spektral import utils A = utils.localpooling_filter(A).astype('f4') And that's all! What's left now for us is to compile and train our model: model.compile(optimizer='adam', loss='categorical_crossentropy', weighted_metrics=['acc']) model.summary() Note that we used the weighted_metrics argument instead of the usual metrics . This is due to the particular semi-supervised problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later). We can now train the model using the native fit() method of Keras, no modifications needed: # Train model validation_data = ([X, A], y, val_mask) model.fit([X, A], y, sample_weight=train_mask, epochs=100, batch_size=N, validation_data=validation_data, shuffle=False) # Shuffling data means shuffling the whole graph There are a couple of things to note here. We have set batch_size=N and shuffle=False . This is because, since we are considering our nodes as samples in a dataset, the default behaviour of Keras will be to split our graph into batches of 32, and shuffle the batches at each epoch. However, shuffling the adjacency matrix along one axis and not the other means that row i will represent a different node than column i . At the same time, if we split the graph in batches, we can easily end up in a situation where we need to use a node attribute that is not part of the batch. The only solution is to take all the node features at the same time, hence batch_size=N . Finally, we used train_mask and val_mask as sample_weight . This means that, during training, the training nodes will have a weight of 1 and the validation nodes will have a weight of 0. Then, in validation, we will set the training nodes to have a weight of 0 and the validation nodes to have a weight of 1. This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X , A , and y for both training and valdation? The only thing that changes is the mask. This is also why we used the weighted_metrics keyword when compiling the model, so that our accuracy is calculated only on the correct nodes at each phase.","title":"Training the GNN"},{"location":"getting-started/#evaluating-the-model","text":"Once again, evaluation is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training (note that in model.evaluate() , by default, shuffle=False ): # Evaluate model eval_results = model.evaluate([X, A], y, sample_weight=test_mask, batch_size=N) print('Done.\\n' 'Test loss: {}\\n' 'Test accuracy: {}'.format(*eval_results)) Done! Our model has been defined, trained, and evaluated.","title":"Evaluating the model"},{"location":"getting-started/#go-create","text":"You are now ready to use Spektral to create your own models. If you want to build a GNN for a specific task, chances are that the things you need to define the model and pre-process the data are already part of Spektral. Check the examples for some ideas and practical tips. Remember to read the data representation section to learn about how GNNs can be used to solve different machine learning problems on graphs. Make sure to check the documentation, and leave a comment on Github if you have a feature that you want to see implemented.","title":"Go create!"},{"location":"datasets/citation/","text":"load_data spektral.datasets.citation.load_data(dataset_name='cora', normalize_features=True, random_split=False) Loads a citation dataset (Cora, Citeseer or Pubmed) using the \"Planetoid\" splits intialliy defined in Yang et al. (2016) . The train, test, and validation splits are given as binary masks. Node attributes are bag-of-words vectors representing the most common words in the text document associated to each node. Two papers are connected if either one cites the other. Labels represent the class of the paper. Arguments dataset_name : name of the dataset to load ( 'cora' , 'citeseer' , or 'pubmed' ); normalize_features : if True, the node features are normalized; random_split : if True, return a randomized split (20 nodes per class for training, 30 nodes per class for validation and the remaining nodes for testing, Shchur et al. (2018) ). Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits.","title":"Citation"},{"location":"datasets/citation/#load_data","text":"spektral.datasets.citation.load_data(dataset_name='cora', normalize_features=True, random_split=False) Loads a citation dataset (Cora, Citeseer or Pubmed) using the \"Planetoid\" splits intialliy defined in Yang et al. (2016) . The train, test, and validation splits are given as binary masks. Node attributes are bag-of-words vectors representing the most common words in the text document associated to each node. Two papers are connected if either one cites the other. Labels represent the class of the paper. Arguments dataset_name : name of the dataset to load ( 'cora' , 'citeseer' , or 'pubmed' ); normalize_features : if True, the node features are normalized; random_split : if True, return a randomized split (20 nodes per class for training, 30 nodes per class for validation and the remaining nodes for testing, Shchur et al. (2018) ). Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits.","title":"load_data"},{"location":"datasets/delaunay/","text":"generate_data spektral.datasets.delaunay.generate_data(classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None, return_type='numpy') Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Node attributes are the 2D coordinates of the points. Two nodes are connected if they share an edge in the Delaunay triangulation. Labels represent the class of the graph (0 to 20, each class index i represent the \"difficulty\" of the classification problem 0 v. i. In other words, the higher the class index, the more similar the class is to class 0). Arguments classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; return_type : 'numpy' or 'networkx' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"Delaunay"},{"location":"datasets/delaunay/#generate_data","text":"spektral.datasets.delaunay.generate_data(classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None, return_type='numpy') Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Node attributes are the 2D coordinates of the points. Two nodes are connected if they share an edge in the Delaunay triangulation. Labels represent the class of the graph (0 to 20, each class index i represent the \"difficulty\" of the classification problem 0 v. i. In other words, the higher the class index, the more similar the class is to class 0). Arguments classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; return_type : 'numpy' or 'networkx' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"generate_data"},{"location":"datasets/graphsage/","text":"load_data spektral.datasets.graphsage.load_data(dataset_name, max_degree=-1, normalize_features=True) Loads one of the datasets (PPI or Reddit) used in Hamilton & Ying (2017) . The PPI dataset (originally Stark et al. (2006) ) for inductive node classification uses positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels. The Reddit dataset consists of a graph made of Reddit posts in the month of September, 2014. The label for each node is the community that a post belongs to. The graph is built by sampling 50 large communities and two nodes are connected if the same user commented on both. Node features are obtained by concatenating the average GloVe CommonCrawl vectors of the title and comments, the post's score and the number of comments. The train, test, and validation splits are returned as binary masks. Arguments dataset_name : name of the dataset to load ( 'ppi' , or 'reddit' ); max_degree : int, if positive, subsample edges so that each node has the specified maximum degree. normalize_features : if True, the node features are normalized; Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits.","title":"GraphSage"},{"location":"datasets/graphsage/#load_data","text":"spektral.datasets.graphsage.load_data(dataset_name, max_degree=-1, normalize_features=True) Loads one of the datasets (PPI or Reddit) used in Hamilton & Ying (2017) . The PPI dataset (originally Stark et al. (2006) ) for inductive node classification uses positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels. The Reddit dataset consists of a graph made of Reddit posts in the month of September, 2014. The label for each node is the community that a post belongs to. The graph is built by sampling 50 large communities and two nodes are connected if the same user commented on both. Node features are obtained by concatenating the average GloVe CommonCrawl vectors of the title and comments, the post's score and the number of comments. The train, test, and validation splits are returned as binary masks. Arguments dataset_name : name of the dataset to load ( 'ppi' , or 'reddit' ); max_degree : int, if positive, subsample edges so that each node has the specified maximum degree. normalize_features : if True, the node features are normalized; Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits.","title":"load_data"},{"location":"datasets/mnist/","text":"load_data spektral.datasets.mnist.load_data(k=8, noise_level=0.0) Loads the MNIST dataset and a K-NN graph to perform graph signal classification, as described by Defferrard et al. (2016) . The K-NN graph is statically determined from a regular grid of pixels using the 2d coordinates. The node features of each graph are the MNIST digits vectorized and rescaled to [0, 1]. Two nodes are connected if they are neighbours according to the K-NN graph. Labels are the MNIST class associated to each sample. Arguments k : int, number of neighbours for each node; noise_level : fraction of edges to flip (from 0 to 1 and vice versa); Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"MNIST"},{"location":"datasets/mnist/#load_data","text":"spektral.datasets.mnist.load_data(k=8, noise_level=0.0) Loads the MNIST dataset and a K-NN graph to perform graph signal classification, as described by Defferrard et al. (2016) . The K-NN graph is statically determined from a regular grid of pixels using the 2d coordinates. The node features of each graph are the MNIST digits vectorized and rescaled to [0, 1]. Two nodes are connected if they are neighbours according to the K-NN graph. Labels are the MNIST class associated to each sample. Arguments k : int, number of neighbours for each node; noise_level : fraction of edges to flip (from 0 to 1 and vice versa); Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"load_data"},{"location":"datasets/ogb/","text":"graph_to_numpy spektral.datasets.ogb.graph_to_numpy(graph, dtype=None) Converts a graph in OGB's library-agnostic format to a representation in Numpy/Scipy. See the Open Graph Benchmark's website for more information. Arguments graph : OGB library-agnostic graph; dtype : if set, all output arrays will be cast to this dtype. Return X: np.array of shape (N, F) with the node features; A: scipy.sparse adjacency matrix of shape (N, N) in COOrdinate format; E: if edge features are available, np.array of shape (n_edges, S), None otherwise. dataset_to_numpy spektral.datasets.ogb.dataset_to_numpy(dataset, indices=None, dtype=None) Converts a dataset in OGB's library-agnostic version to lists of Numpy/Scipy arrays. See the Open Graph Benchmark's website for more information. Arguments dataset : OGB library-agnostic dataset (e.g., GraphPropPredDataset); indices : optional, a list of integer indices; if provided, only these graphs will be converted; dtype : if set, the arrays in the returned lists will have this dtype. Return X_list: list of np.arrays of (variable) shape (N, F) with node features; A_list: list of scipy.sparse adjacency matrices of (variable) shape (N, N); E_list: list of np.arrays of (variable) shape (n_nodes, S) with edge attributes. If edge attributes are not available, a list of None. y_list: np.array of shape (n_graphs, n_tasks) with the task labels;","title":"OGB"},{"location":"datasets/ogb/#graph_to_numpy","text":"spektral.datasets.ogb.graph_to_numpy(graph, dtype=None) Converts a graph in OGB's library-agnostic format to a representation in Numpy/Scipy. See the Open Graph Benchmark's website for more information. Arguments graph : OGB library-agnostic graph; dtype : if set, all output arrays will be cast to this dtype. Return X: np.array of shape (N, F) with the node features; A: scipy.sparse adjacency matrix of shape (N, N) in COOrdinate format; E: if edge features are available, np.array of shape (n_edges, S), None otherwise.","title":"graph_to_numpy"},{"location":"datasets/ogb/#dataset_to_numpy","text":"spektral.datasets.ogb.dataset_to_numpy(dataset, indices=None, dtype=None) Converts a dataset in OGB's library-agnostic version to lists of Numpy/Scipy arrays. See the Open Graph Benchmark's website for more information. Arguments dataset : OGB library-agnostic dataset (e.g., GraphPropPredDataset); indices : optional, a list of integer indices; if provided, only these graphs will be converted; dtype : if set, the arrays in the returned lists will have this dtype. Return X_list: list of np.arrays of (variable) shape (N, F) with node features; A_list: list of scipy.sparse adjacency matrices of (variable) shape (N, N); E_list: list of np.arrays of (variable) shape (n_nodes, S) with edge attributes. If edge attributes are not available, a list of None. y_list: np.array of shape (n_graphs, n_tasks) with the task labels;","title":"dataset_to_numpy"},{"location":"datasets/qm9/","text":"load_data spektral.datasets.qm9.load_data(nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None, return_type='numpy') Loads the QM9 chemical data set of small molecules. Nodes represent heavy atoms (hydrogens are discarded), edges represent chemical bonds. The node features represent the chemical properties of each atom, and are loaded according to the nf_keys argument. See spektral.datasets.qm9.NODE_FEATURES for possible node features, and see this link for the meaning of each property. Usually, it is sufficient to load the atomic number. The edge features represent the type and stereoscopy of each chemical bond between two atoms. See spektral.datasets.qm9.EDGE_FEATURES for possible edge features, and see this link for the meaning of each property. Usually, it is sufficient to load the type of bond. Arguments nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in ascending order by number of atoms). return_type : 'numpy' , 'networkx' , or 'sdf' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"QM9"},{"location":"datasets/qm9/#load_data","text":"spektral.datasets.qm9.load_data(nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None, return_type='numpy') Loads the QM9 chemical data set of small molecules. Nodes represent heavy atoms (hydrogens are discarded), edges represent chemical bonds. The node features represent the chemical properties of each atom, and are loaded according to the nf_keys argument. See spektral.datasets.qm9.NODE_FEATURES for possible node features, and see this link for the meaning of each property. Usually, it is sufficient to load the atomic number. The edge features represent the type and stereoscopy of each chemical bond between two atoms. See spektral.datasets.qm9.EDGE_FEATURES for possible edge features, and see this link for the meaning of each property. Usually, it is sufficient to load the type of bond. Arguments nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in ascending order by number of atoms). return_type : 'numpy' , 'networkx' , or 'sdf' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"load_data"},{"location":"datasets/tud/","text":"load_data spektral.datasets.tud.load_data(dataset_name, clean=False) Loads one of the Benchmark Data Sets for Graph Kernels from TU Dortmund ( link ). The node features are computed by concatenating the following features for each node: node attributes, if available, normalized as specified in normalize_features ; clustering coefficient, normalized with z-score; node degrees, normalized as specified in normalize_features ; node labels, if available, one-hot encoded. Arguments dataset_name : name of the dataset to load (see spektral.datasets.tud.AVAILABLE_DATASETS ). normalize_features : None , 'zscore' or 'ohe' , how to normalize the node features (only works for node attributes). clean : if True, return a version of the dataset with no isomorphic graphs. Return a list of adjacency matrices; a list of node feature matrices; a numpy array containing the one-hot encoded targets.","title":"TU Dortmund"},{"location":"datasets/tud/#load_data","text":"spektral.datasets.tud.load_data(dataset_name, clean=False) Loads one of the Benchmark Data Sets for Graph Kernels from TU Dortmund ( link ). The node features are computed by concatenating the following features for each node: node attributes, if available, normalized as specified in normalize_features ; clustering coefficient, normalized with z-score; node degrees, normalized as specified in normalize_features ; node labels, if available, one-hot encoded. Arguments dataset_name : name of the dataset to load (see spektral.datasets.tud.AVAILABLE_DATASETS ). normalize_features : None , 'zscore' or 'ohe' , how to normalize the node features (only works for node attributes). clean : if True, return a version of the dataset with no isomorphic graphs. Return a list of adjacency matrices; a list of node feature matrices; a numpy array containing the one-hot encoded targets.","title":"load_data"},{"location":"layers/base/","text":"[source] InnerProduct spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a 2d Tensor: \\langle \\x, \\x \\rangle = \\x\\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments trainable_kernel : add a trainable square matrix between the inner product (e.g., X @ W @ X.T ); activation : activation function to use; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel; kernel_constraint : constraint applied to the kernel; [source] MinkowskiProduct spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None) Computes the hyperbolic inner product between elements of a rank 2 Tensor: \\langle \\x, \\x \\rangle = \\x \\, \\begin{pmatrix} \\I_{d \\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\, \\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments input_dim_1 : first dimension of the input Tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use;","title":"Base Layers"},{"location":"layers/base/#innerproduct","text":"spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a 2d Tensor: \\langle \\x, \\x \\rangle = \\x\\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments trainable_kernel : add a trainable square matrix between the inner product (e.g., X @ W @ X.T ); activation : activation function to use; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel; kernel_constraint : constraint applied to the kernel; [source]","title":"InnerProduct"},{"location":"layers/base/#minkowskiproduct","text":"spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None) Computes the hyperbolic inner product between elements of a rank 2 Tensor: \\langle \\x, \\x \\rangle = \\x \\, \\begin{pmatrix} \\I_{d \\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\, \\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments input_dim_1 : first dimension of the input Tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use;","title":"MinkowskiProduct"},{"location":"layers/convolution/","text":"The message-passing layers from these papers are available in Spektral: Semi-Supervised Classification with Graph Convolutional Networks Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering Inductive Representation Learning on Large Graphs Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs Graph Attention Networks Graph Neural Networks with convolutional ARMA filters Predict then Propagate: Graph Neural Networks meet Personalized PageRank How Powerful are Graph Neural Networks? Notation: N : number of nodes in the graph; F : dimension of the node attributes (i.e., each node has an attribute in \\mathbb{R}^F ); S : dimension of the edge attributes (i.e., each edge has an attribute in \\mathbb{R}^S ); \\A \\in \\{0, 1\\}^{N \\times N} : binary adjacency matrix; \\X \\in \\mathbb{R}^{ N \\times F } : node attributes matrix; \\E \\in \\mathbb{R}^{ N \\times N \\times S } : edge attributes matrix; \\D = \\textrm{diag} ( \\sum\\limits_{j=0} \\A_{ij} ) : degree matrix; \\W, \\V : trainable kernels; \\b : trainable bias vector; \\mathcal{N}(i) : the one-hop neighbourhood of node i ; F' : dimension of the node attributes after a message-passing layer; [source] GraphConv spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer (GCN) as presented by Kipf & Welling (2016) . Mode : single, mixed, batch. This layer computes: \\Z = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b where \\hat \\A = \\A + \\I is the adjacency matrix with added self-loops and \\hat\\D is its degree matrix. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] ChebConv spektral.layers.ChebConv(channels, K=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, mixed, batch. This layer computes: \\Z = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)} + \\b^{(k)}, where \\T^{(0)}, ..., \\T^{(K - 1)} are Chebyshev polynomials of \\tilde \\L defined as \\T^{(0)} = \\X \\\\ \\T^{(1)} = \\tilde \\L \\X \\\\ \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)}, where \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; A list of K Chebyshev polynomials of shape [([batch], N, N), ..., ([batch], N, N)] ; can be computed with spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : order of the Chebyshev polynomials; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] GraphSageConv spektral.layers.GraphSageConv(channels, aggregate_op='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSAGE layer as presented by Hamilton et al. (2017) . Mode : single. This layer computes: \\Z = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\ \\Z = \\frac{\\Z}{\\|\\Z\\|} where \\textrm{AGGREGATE} is a function to aggregate a node's neighbourhood. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; aggregate_op : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] ARMAConv spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA _K filters, as presented by Bianchi et al. (2019) . Mode : single, mixed, batch. This layer computes: \\Z = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)}, where K is the order of the ARMA _K filter, and where: \\bar \\X_k^{(t + 1)} = \\sigma \\left(\\tilde \\L \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right) is a recursive approximation of an ARMA _1 filter, where \\bar \\X^{(0)} = \\X and \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; Normalized and rescaled Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_laplacian and spektral.utils.convolution.rescale_laplacian . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; order : order of the full ARMA _K filter, i.e., the number of parallel stacks in the layer; iterations : number of iterations to compute each ARMA _1 approximation; share_weights : share the weights in each ARMA _1 stack. gcn_activation : activation function to use to compute each ARMA _1 stack; dropout_rate : dropout rate for skip connection; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] EdgeConditionedConv spektral.layers.EdgeConditionedConv(channels, kernel_network=None, root=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer (ECC) as presented by Simonovsky & Komodakis (2017) . Mode : single, batch. This layer expects dense inputs and self-loops when working in batch mode. For each node i , this layer computes: \\Z_i = \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}(\\E_{ji}) \\X_{j} + \\b where \\textrm{MLP} is a multi-layer perceptron that outputs the convolutional kernel \\W as a function of edge attributes. Input Node features of shape ([batch], N, F) ; Binary adjacency matrices with self-loops, of shape ([batch], N, N) ; Edge features of shape ([batch], N, N, S) ; Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers describing the hidden structure of the kernel-generating network (i.e., the ReLU layers before the linear output); activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] GraphAttention spektral.layers.GraphAttention(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer (GAT) as presented by Velickovic et al. (2017) . Mode : single, mixed, batch. This layer expects dense inputs when working in batch mode. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian: \\Z = \\mathbf{\\alpha}\\X\\W + \\b where \\mathbf{\\alpha}_{ij} = \\frac{ \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j] \\right) \\right) } {\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k] \\right) \\right) } where \\a \\in \\mathbb{R}^{2F'} is a trainable attention kernel. Dropout is also applied to \\alpha before computing \\Z . Parallel attention heads are computed in parallel and their results are aggregated by concatenation or average. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Node features with the same shape as the input, but with the last dimension changed to channels ; if return_attn_coef=True , a list with the attention coefficients for each attention head. Each attention coefficient matrix has shape ([batch], N, N) . Arguments channels : number of output channels; attn_heads : number of attention heads to use; concat_heads : bool, whether to concatenate the output of the attention heads instead of averaging; dropout_rate : internal dropout rate for attention coefficients; return_attn_coef : if True, return the attention coefficients for the given input (one N x N matrix for each head). activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; attn_kernel_initializer : initializer for the attention kernels; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernels; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; attn_kernel_constraint : constraint applied to the attention kernels; bias_constraint : constraint applied to the bias vector. [source] GraphConvSkip spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A simple convolutional layer with a skip connection. Mode : single, mixed, batch. This layer computes: \\Z = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b where \\A does not have self-loops (unlike in GraphConv). Input Node features of shape ([batch], N, F) ; Normalized adjacency matrix of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_adjacency . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] APPNP spektral.layers.APPNP(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . This layer computes: \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\ \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} + \\alpha \\Z^{(0)}, where \\alpha is the teleport probability and \\textrm{MLP} is a multi-layer perceptron. Mode : single, mixed, batch. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; alpha : teleport probability during propagation; propagations : number of propagation steps; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] GINConv spektral.layers.GINConv(channels, epsilon=None, mlp_hidden=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) as presented by Xu et al. (2018) . Mode : single. This layer expects sparse inputs. This layer computes for each node i : \\Z_i = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_j \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; epsilon : unnamed parameter, see Xu et al. (2018) , and the equation above. This parameter can be learned by setting epsilon=None , or it can be set to a constant value, which is what happens by default (0). In practice, it is safe to leave it to 0. mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector.","title":"Convolutional Layers"},{"location":"layers/convolution/#graphconv","text":"spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer (GCN) as presented by Kipf & Welling (2016) . Mode : single, mixed, batch. This layer computes: \\Z = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b where \\hat \\A = \\A + \\I is the adjacency matrix with added self-loops and \\hat\\D is its degree matrix. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphConv"},{"location":"layers/convolution/#chebconv","text":"spektral.layers.ChebConv(channels, K=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, mixed, batch. This layer computes: \\Z = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)} + \\b^{(k)}, where \\T^{(0)}, ..., \\T^{(K - 1)} are Chebyshev polynomials of \\tilde \\L defined as \\T^{(0)} = \\X \\\\ \\T^{(1)} = \\tilde \\L \\X \\\\ \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)}, where \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; A list of K Chebyshev polynomials of shape [([batch], N, N), ..., ([batch], N, N)] ; can be computed with spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : order of the Chebyshev polynomials; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"ChebConv"},{"location":"layers/convolution/#graphsageconv","text":"spektral.layers.GraphSageConv(channels, aggregate_op='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSAGE layer as presented by Hamilton et al. (2017) . Mode : single. This layer computes: \\Z = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\ \\Z = \\frac{\\Z}{\\|\\Z\\|} where \\textrm{AGGREGATE} is a function to aggregate a node's neighbourhood. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; aggregate_op : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphSageConv"},{"location":"layers/convolution/#armaconv","text":"spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA _K filters, as presented by Bianchi et al. (2019) . Mode : single, mixed, batch. This layer computes: \\Z = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)}, where K is the order of the ARMA _K filter, and where: \\bar \\X_k^{(t + 1)} = \\sigma \\left(\\tilde \\L \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right) is a recursive approximation of an ARMA _1 filter, where \\bar \\X^{(0)} = \\X and \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; Normalized and rescaled Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_laplacian and spektral.utils.convolution.rescale_laplacian . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; order : order of the full ARMA _K filter, i.e., the number of parallel stacks in the layer; iterations : number of iterations to compute each ARMA _1 approximation; share_weights : share the weights in each ARMA _1 stack. gcn_activation : activation function to use to compute each ARMA _1 stack; dropout_rate : dropout rate for skip connection; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"ARMAConv"},{"location":"layers/convolution/#edgeconditionedconv","text":"spektral.layers.EdgeConditionedConv(channels, kernel_network=None, root=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer (ECC) as presented by Simonovsky & Komodakis (2017) . Mode : single, batch. This layer expects dense inputs and self-loops when working in batch mode. For each node i , this layer computes: \\Z_i = \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}(\\E_{ji}) \\X_{j} + \\b where \\textrm{MLP} is a multi-layer perceptron that outputs the convolutional kernel \\W as a function of edge attributes. Input Node features of shape ([batch], N, F) ; Binary adjacency matrices with self-loops, of shape ([batch], N, N) ; Edge features of shape ([batch], N, N, S) ; Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers describing the hidden structure of the kernel-generating network (i.e., the ReLU layers before the linear output); activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"EdgeConditionedConv"},{"location":"layers/convolution/#graphattention","text":"spektral.layers.GraphAttention(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer (GAT) as presented by Velickovic et al. (2017) . Mode : single, mixed, batch. This layer expects dense inputs when working in batch mode. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian: \\Z = \\mathbf{\\alpha}\\X\\W + \\b where \\mathbf{\\alpha}_{ij} = \\frac{ \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j] \\right) \\right) } {\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k] \\right) \\right) } where \\a \\in \\mathbb{R}^{2F'} is a trainable attention kernel. Dropout is also applied to \\alpha before computing \\Z . Parallel attention heads are computed in parallel and their results are aggregated by concatenation or average. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Node features with the same shape as the input, but with the last dimension changed to channels ; if return_attn_coef=True , a list with the attention coefficients for each attention head. Each attention coefficient matrix has shape ([batch], N, N) . Arguments channels : number of output channels; attn_heads : number of attention heads to use; concat_heads : bool, whether to concatenate the output of the attention heads instead of averaging; dropout_rate : internal dropout rate for attention coefficients; return_attn_coef : if True, return the attention coefficients for the given input (one N x N matrix for each head). activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; attn_kernel_initializer : initializer for the attention kernels; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernels; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; attn_kernel_constraint : constraint applied to the attention kernels; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphAttention"},{"location":"layers/convolution/#graphconvskip","text":"spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A simple convolutional layer with a skip connection. Mode : single, mixed, batch. This layer computes: \\Z = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b where \\A does not have self-loops (unlike in GraphConv). Input Node features of shape ([batch], N, F) ; Normalized adjacency matrix of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_adjacency . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphConvSkip"},{"location":"layers/convolution/#appnp","text":"spektral.layers.APPNP(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . This layer computes: \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\ \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} + \\alpha \\Z^{(0)}, where \\alpha is the teleport probability and \\textrm{MLP} is a multi-layer perceptron. Mode : single, mixed, batch. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; alpha : teleport probability during propagation; propagations : number of propagation steps; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"APPNP"},{"location":"layers/convolution/#ginconv","text":"spektral.layers.GINConv(channels, epsilon=None, mlp_hidden=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) as presented by Xu et al. (2018) . Mode : single. This layer expects sparse inputs. This layer computes for each node i : \\Z_i = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_j \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; epsilon : unnamed parameter, see Xu et al. (2018) , and the equation above. This parameter can be learned by setting epsilon=None , or it can be set to a constant value, which is what happens by default (0). In practice, it is safe to leave it to 0. mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector.","title":"GINConv"},{"location":"layers/pooling/","text":"The pooling layers from these papers are available in Spektral: Hierarchical Graph Representation Learning with Differentiable Pooling Mincut pooling in Graph Neural Networks Graph U-Nets Self-Attention Graph Pooling Gated Graph Sequence Neural Networks Additionally, sum, average, and max global pooling are implemented, as well as a simple global weighted sum pooling where weights are calculated with an attention mechanism. See the convolutional layers page for the notation. [source] DiffPool spektral.layers.DiffPool(k, channels=None, return_mask=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A DiffPool layer as presented by Ying et al. . Mode : single, batch. This layer computes a soft clustering \\S of the input graphs using a GNN, and reduces graphs as follows: \\S = \\textrm{GNN}(\\A, \\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where GNN consists of one GraphConv layer with softmax activation. Two auxiliary loss terms are also added to the model: the link prediction loss \\big\\| \\A - \\S\\S^\\top \\big\\|_F and the entropy loss - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S). The layer also applies a 1-layer GCN to the input features, and returns the updated graph signal (the number of output channels is controlled by the channels parameter). The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, channels) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; channels : number of output channels (if None, the number of output channels is assumed to be the same as the input); return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source] MinCutPool spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_mask=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A minCUT pooling layer as presented by Bianchi et al. . Mode : single, batch. This layer computes a soft clustering \\S of the input graphs using a MLP, and reduces graphs as follows: \\S = \\textrm{MLP}(\\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where MLP is a multi-layer perceptron with softmax output. Two auxiliary loss terms are also added to the model: the minCUT loss - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) } and the orthogonality loss \\left\\| \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F} - \\frac{\\I_K}{\\sqrt{K}} \\right\\|_F. The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, F) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP used to compute cluster assignments (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source] TopKPool spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A gPool/Top-K layer as presented by Gao & Ji and Cangea et al. . This layer computes the following operations: \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A^2_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\p is a learnable parameter vector of size F . Note that the the gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). The paper by Gao & Ji originally used a tanh as well, but was later updated to use a sigmoid activation. Due to the lack of sparse-sparse matrix multiplication support, this layer temporarily makes the adjacency matrix dense in order to compute \\A^2 (needed to preserve connectivity after pooling). If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Mode : single, disjoint. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source] SAGPool spektral.layers.SAGPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A self-attention graph pooling layer as presented by Lee et al. (2019) and Knyazev et al. (2019) . This layer computes the following operations: \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A^2_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\textrm{GNN} consists of one GraphConv layer with no activation. Due to the lack of sparse-sparse matrix multiplication support, this layer temporarily makes the adjacency matrix dense in order to compute \\A^2 (needed to preserve connectivity after pooling). If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Mode : single, disjoint. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source] GlobalSumPool spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments None. [source] GlobalAvgPool spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments None. [source] GlobalMaxPool spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments None. [source] GlobalAttentionPool spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer as presented by Li et al. (2017) . This layer computes: \\X' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i where \\sigma is the sigmoid activation function. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], channels) (if single mode, shape will be (1, channels) ). Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vectors; kernel_regularizer : regularization applied to the kernel matrices; bias_regularizer : regularization applied to the bias vectors; kernel_constraint : constraint applied to the kernel matrices; bias_constraint : constraint applied to the bias vectors. [source] GlobalAttnSumPool spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. This layer computes: \\alpha = \\textrm{softmax}( \\X \\a); \\\\ \\X' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i where \\a \\in \\mathbb{R}^F is a trainable vector. Note that the softmax is applied across nodes, and not across features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments attn_kernel_initializer : initializer for the attention kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix;","title":"Pooling Layers"},{"location":"layers/pooling/#diffpool","text":"spektral.layers.DiffPool(k, channels=None, return_mask=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A DiffPool layer as presented by Ying et al. . Mode : single, batch. This layer computes a soft clustering \\S of the input graphs using a GNN, and reduces graphs as follows: \\S = \\textrm{GNN}(\\A, \\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where GNN consists of one GraphConv layer with softmax activation. Two auxiliary loss terms are also added to the model: the link prediction loss \\big\\| \\A - \\S\\S^\\top \\big\\|_F and the entropy loss - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S). The layer also applies a 1-layer GCN to the input features, and returns the updated graph signal (the number of output channels is controlled by the channels parameter). The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, channels) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; channels : number of output channels (if None, the number of output channels is assumed to be the same as the input); return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source]","title":"DiffPool"},{"location":"layers/pooling/#mincutpool","text":"spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_mask=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A minCUT pooling layer as presented by Bianchi et al. . Mode : single, batch. This layer computes a soft clustering \\S of the input graphs using a MLP, and reduces graphs as follows: \\S = \\textrm{MLP}(\\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where MLP is a multi-layer perceptron with softmax output. Two auxiliary loss terms are also added to the model: the minCUT loss - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) } and the orthogonality loss \\left\\| \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F} - \\frac{\\I_K}{\\sqrt{K}} \\right\\|_F. The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, F) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP used to compute cluster assignments (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source]","title":"MinCutPool"},{"location":"layers/pooling/#topkpool","text":"spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A gPool/Top-K layer as presented by Gao & Ji and Cangea et al. . This layer computes the following operations: \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A^2_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\p is a learnable parameter vector of size F . Note that the the gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). The paper by Gao & Ji originally used a tanh as well, but was later updated to use a sigmoid activation. Due to the lack of sparse-sparse matrix multiplication support, this layer temporarily makes the adjacency matrix dense in order to compute \\A^2 (needed to preserve connectivity after pooling). If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Mode : single, disjoint. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source]","title":"TopKPool"},{"location":"layers/pooling/#sagpool","text":"spektral.layers.SAGPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A self-attention graph pooling layer as presented by Lee et al. (2019) and Knyazev et al. (2019) . This layer computes the following operations: \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A^2_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\textrm{GNN} consists of one GraphConv layer with no activation. Due to the lack of sparse-sparse matrix multiplication support, this layer temporarily makes the adjacency matrix dense in order to compute \\A^2 (needed to preserve connectivity after pooling). If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Mode : single, disjoint. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix; [source]","title":"SAGPool"},{"location":"layers/pooling/#globalsumpool","text":"spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments None. [source]","title":"GlobalSumPool"},{"location":"layers/pooling/#globalavgpool","text":"spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments None. [source]","title":"GlobalAvgPool"},{"location":"layers/pooling/#globalmaxpool","text":"spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments None. [source]","title":"GlobalMaxPool"},{"location":"layers/pooling/#globalattentionpool","text":"spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer as presented by Li et al. (2017) . This layer computes: \\X' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i where \\sigma is the sigmoid activation function. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], channels) (if single mode, shape will be (1, channels) ). Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vectors; kernel_regularizer : regularization applied to the kernel matrices; bias_regularizer : regularization applied to the bias vectors; kernel_constraint : constraint applied to the kernel matrices; bias_constraint : constraint applied to the bias vectors. [source]","title":"GlobalAttentionPool"},{"location":"layers/pooling/#globalattnsumpool","text":"spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. This layer computes: \\alpha = \\textrm{softmax}( \\X \\a); \\\\ \\X' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i where \\a \\in \\mathbb{R}^F is a trainable vector. Note that the softmax is applied across nodes, and not across features. Mode : single, mixed, batch, disjoint. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape ([batch], F) (if single mode, shape will be (1, F) ). Arguments attn_kernel_initializer : initializer for the attention kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix;","title":"GlobalAttnSumPool"},{"location":"utils/conversion/","text":"nx_to_adj spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices. nx_to_node_features spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_edge_features spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_numpy spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) numpy_to_nx spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"Conversion"},{"location":"utils/conversion/#nx_to_adj","text":"spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices.","title":"nx_to_adj"},{"location":"utils/conversion/#nx_to_node_features","text":"spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_node_features"},{"location":"utils/conversion/#nx_to_edge_features","text":"spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_edge_features"},{"location":"utils/conversion/#nx_to_numpy","text":"spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim)","title":"nx_to_numpy"},{"location":"utils/conversion/#numpy_to_nx","text":"spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"numpy_to_nx"},{"location":"utils/convolution/","text":"degree_matrix spektral.utils.degree_matrix(A) Computes the degree matrix of the given adjacency matrix. Arguments A : rank 2 array or sparse matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format. degree_power spektral.utils.degree_power(A, k) Computes \\D^{k} from the given adjacency matrix. Useful for computing normalised Laplacian. Arguments A : rank 2 array or sparse matrix. k : exponent to which elevate the degree matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format. normalized_adjacency spektral.utils.normalized_adjacency(A, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either \\D^{-1}\\A or \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix. laplacian spektral.utils.laplacian(A) Computes the Laplacian of the given adjacency matrix as \\D - \\A . Arguments A : rank 2 array or sparse matrix; Return The Laplacian. normalized_laplacian spektral.utils.normalized_laplacian(A, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as \\I - \\D^{-1}\\A or \\I - \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian. rescale_laplacian spektral.utils.rescale_laplacian(L, lmax=None) Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue. Arguments L : rank 2 array or sparse matrix; lmax : if None, compute largest eigenvalue with scipy.linalg.eisgh. If the eigendecomposition fails, lmax is set to 2 automatically. If scalar, use this value as largest eignevalue when rescaling. Return localpooling_filter spektral.utils.localpooling_filter(A, symmetric=True) Computes the graph filter described in Kipf & Welling (2017) . Arguments A : array or sparse matrix with rank 2 or 3; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return Array or sparse matrix with rank 2 or 3, same as A; chebyshev_polynomial spektral.utils.chebyshev_polynomial(X, k) Calculates Chebyshev polynomials of X, up to order k. Arguments X : rank 2 array or sparse matrix; k : the order up to which compute the polynomials, Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial. chebyshev_filter spektral.utils.chebyshev_filter(A, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016) . Arguments A : rank 2 array or sparse matrix; k : integer, the order of the Chebyshev polynomial; symmetric : boolean, whether to normalize the adjacency matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"Convolution"},{"location":"utils/convolution/#degree_matrix","text":"spektral.utils.degree_matrix(A) Computes the degree matrix of the given adjacency matrix. Arguments A : rank 2 array or sparse matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format.","title":"degree_matrix"},{"location":"utils/convolution/#degree_power","text":"spektral.utils.degree_power(A, k) Computes \\D^{k} from the given adjacency matrix. Useful for computing normalised Laplacian. Arguments A : rank 2 array or sparse matrix. k : exponent to which elevate the degree matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format.","title":"degree_power"},{"location":"utils/convolution/#normalized_adjacency","text":"spektral.utils.normalized_adjacency(A, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either \\D^{-1}\\A or \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix.","title":"normalized_adjacency"},{"location":"utils/convolution/#laplacian","text":"spektral.utils.laplacian(A) Computes the Laplacian of the given adjacency matrix as \\D - \\A . Arguments A : rank 2 array or sparse matrix; Return The Laplacian.","title":"laplacian"},{"location":"utils/convolution/#normalized_laplacian","text":"spektral.utils.normalized_laplacian(A, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as \\I - \\D^{-1}\\A or \\I - \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian.","title":"normalized_laplacian"},{"location":"utils/convolution/#rescale_laplacian","text":"spektral.utils.rescale_laplacian(L, lmax=None) Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue. Arguments L : rank 2 array or sparse matrix; lmax : if None, compute largest eigenvalue with scipy.linalg.eisgh. If the eigendecomposition fails, lmax is set to 2 automatically. If scalar, use this value as largest eignevalue when rescaling. Return","title":"rescale_laplacian"},{"location":"utils/convolution/#localpooling_filter","text":"spektral.utils.localpooling_filter(A, symmetric=True) Computes the graph filter described in Kipf & Welling (2017) . Arguments A : array or sparse matrix with rank 2 or 3; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return Array or sparse matrix with rank 2 or 3, same as A;","title":"localpooling_filter"},{"location":"utils/convolution/#chebyshev_polynomial","text":"spektral.utils.chebyshev_polynomial(X, k) Calculates Chebyshev polynomials of X, up to order k. Arguments X : rank 2 array or sparse matrix; k : the order up to which compute the polynomials, Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"chebyshev_polynomial"},{"location":"utils/convolution/#chebyshev_filter","text":"spektral.utils.chebyshev_filter(A, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016) . Arguments A : rank 2 array or sparse matrix; k : integer, the order of the Chebyshev polynomial; symmetric : boolean, whether to normalize the adjacency matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"chebyshev_filter"},{"location":"utils/data/","text":"numpy_to_disjoint spektral.utils.numpy_to_disjoint(X_list, A_list, E_list=None) Converts a batch of graphs stored in lists (X, A, and optionally E) to the disjoint mode . Note that the number of nodes N can vary between graphs, although each entry i of each list should be associated to the same graph, i.e., X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0] . Arguments X_list : a list of np.arrays of shape (N, F); A_list : a list of np.arrays or sparse matrices of shape (N, N); E_list : a list of np.arrays of shape (N, N, S); Return batch_iterator spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with equal first dimension. batch_size : number of samples in a batch epochs : number of times to iterate over the data shuffle : whether to shuffle the data at the beginning of each epoch :yield: a batch of samples (or tuple of batches if X had more than one array).","title":"Data"},{"location":"utils/data/#numpy_to_disjoint","text":"spektral.utils.numpy_to_disjoint(X_list, A_list, E_list=None) Converts a batch of graphs stored in lists (X, A, and optionally E) to the disjoint mode . Note that the number of nodes N can vary between graphs, although each entry i of each list should be associated to the same graph, i.e., X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0] . Arguments X_list : a list of np.arrays of shape (N, F); A_list : a list of np.arrays or sparse matrices of shape (N, N); E_list : a list of np.arrays of shape (N, N, S); Return","title":"numpy_to_disjoint"},{"location":"utils/data/#batch_iterator","text":"spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with equal first dimension. batch_size : number of samples in a batch epochs : number of times to iterate over the data shuffle : whether to shuffle the data at the beginning of each epoch :yield: a batch of samples (or tuple of batches if X had more than one array).","title":"batch_iterator"},{"location":"utils/misc/","text":"pad_jagged_array spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a list or np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape add_eye spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix sub_eye spektral.utils.sub_eye(x) Subtracts the identity matrix from the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix add_eye_batch spektral.utils.add_eye_batch(x) Adds the identity matrix to each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array sub_eye_batch spektral.utils.sub_eye_batch(x) Subtracts the identity matrix from each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array add_eye_jagged spektral.utils.add_eye_jagged(x) Adds the identity matrix to each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array sub_eye_jagged spektral.utils.sub_eye_jagged(x) Subtracts the identity matrix from each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array","title":"Miscellaneous"},{"location":"utils/misc/#pad_jagged_array","text":"spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a list or np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape","title":"pad_jagged_array"},{"location":"utils/misc/#add_eye","text":"spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix","title":"add_eye"},{"location":"utils/misc/#sub_eye","text":"spektral.utils.sub_eye(x) Subtracts the identity matrix from the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix","title":"sub_eye"},{"location":"utils/misc/#add_eye_batch","text":"spektral.utils.add_eye_batch(x) Adds the identity matrix to each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array","title":"add_eye_batch"},{"location":"utils/misc/#sub_eye_batch","text":"spektral.utils.sub_eye_batch(x) Subtracts the identity matrix from each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array","title":"sub_eye_batch"},{"location":"utils/misc/#add_eye_jagged","text":"spektral.utils.add_eye_jagged(x) Adds the identity matrix to each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array","title":"add_eye_jagged"},{"location":"utils/misc/#sub_eye_jagged","text":"spektral.utils.sub_eye_jagged(x) Subtracts the identity matrix from each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array","title":"sub_eye_jagged"},{"location":"utils/plotting/","text":"plot_numpy spektral.utils.plot_numpy(A, X=None, E=None, nf_name=None, ef_name=None, layout='spring_layout', labels=True) Plots a graph in matrix format (adjacency matrix, node features matrix, and edge features matrix). Arguments A : np.array, adjacency matrix of the graph; X : np.array, node features matrix of the graph; E : np.array, edge features matrix of the graph; nf_name : string, name of the node features to plot; ef_name : string, name of the edge features to plot; layout : string, type of layout for networkx (see nx.layout.__all__ ); labels : bool, plot node and edge labels; kwargs : extra arguments for nx.draw; Return None plot_nx spektral.utils.plot_nx(nx_graph, nf_name=None, ef_name=None, layout='spring_layout', labels=True) Plot a Networkx graph. Arguments nx_graph : a Networkx graph; nf_name : string, name of the node features to plot; ef_name : string, name of the edge features to plot; layout : string, type of layout for networkx (see nx.layout.__all__ ); labels : bool, plot node and edge labels; kwargs : extra arguments for nx.draw; Return None","title":"Plotting"},{"location":"utils/plotting/#plot_numpy","text":"spektral.utils.plot_numpy(A, X=None, E=None, nf_name=None, ef_name=None, layout='spring_layout', labels=True) Plots a graph in matrix format (adjacency matrix, node features matrix, and edge features matrix). Arguments A : np.array, adjacency matrix of the graph; X : np.array, node features matrix of the graph; E : np.array, edge features matrix of the graph; nf_name : string, name of the node features to plot; ef_name : string, name of the edge features to plot; layout : string, type of layout for networkx (see nx.layout.__all__ ); labels : bool, plot node and edge labels; kwargs : extra arguments for nx.draw; Return None","title":"plot_numpy"},{"location":"utils/plotting/#plot_nx","text":"spektral.utils.plot_nx(nx_graph, nf_name=None, ef_name=None, layout='spring_layout', labels=True) Plot a Networkx graph. Arguments nx_graph : a Networkx graph; nf_name : string, name of the node features to plot; ef_name : string, name of the edge features to plot; layout : string, type of layout for networkx (see nx.layout.__all__ ); labels : bool, plot node and edge labels; kwargs : extra arguments for nx.draw; Return None","title":"plot_nx"}]}