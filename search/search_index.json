{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Spektral Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the nodes of a network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph Convolutional Networks (GCN) Chebyshev networks (ChebNets) GraphSAGE ARMA convolutions Edge-Conditioned Convolutions (ECC) Graph attention networks (GAT) Approximated Personalized Propagation of Neural Predictions (APPNP) Graph Isomorphism Networks (GIN) Diffusional Convolutions and many others (see convolutional layers ). You can also find pooling layers , including: DiffPool MinCUT pooling Top-K pooling Self-Attention Graph (SAG) pooling Global sum, average, and max pooling Global gated attention pooling SortPool Spektral also includes lots of utilities for your graph deep learning projects. See how to get started with Spektral and have a look at the examples for some templates. The source code of the project is available on Github . Read the documentation here . You can also cite the paper introducing Spektral: Graph Neural Networks in TensorFlow and Keras with Spektral (ICML 2020 - GRL+ Workshop). Installation Spektral is compatible with Python 3.5+, and is tested on Ubuntu 16.04+ and MacOS. Other Linux distros should work as well, but Windows is not supported for now. Some optional features of Spektral depend on RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda). The simplest way to install Spektral is from PyPi: pip install spektral To install Spektral from source, run this in a terminal: git clone https://github.com/danielegrattarola/spektral.git cd spektral python setup.py install # Or 'pip install .' To install Spektral on Google Colab : ! pip install spektral TensorFlow 1 and Keras Starting from version 0.3, Spektral only supports TensorFlow 2 and tf.keras . The old version of Spektral, which is based on TensorFlow 1 and the stand-alone Keras library, is still available on the tf1 branch on GitHub and can be installed from source: git clone https://github.com/danielegrattarola/spektral.git cd spektral git checkout tf1 python setup.py install # Or 'pip install .' In the future, the TF1-compatible version of Spektral (<0.2) will receive bug fixes, but all new features will only support TensorFlow 2. Contributing Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework. The contribution guidelines are available here and a list of feature requests is available here .","title":"Home"},{"location":"#welcome-to-spektral","text":"Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the nodes of a network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph Convolutional Networks (GCN) Chebyshev networks (ChebNets) GraphSAGE ARMA convolutions Edge-Conditioned Convolutions (ECC) Graph attention networks (GAT) Approximated Personalized Propagation of Neural Predictions (APPNP) Graph Isomorphism Networks (GIN) Diffusional Convolutions and many others (see convolutional layers ). You can also find pooling layers , including: DiffPool MinCUT pooling Top-K pooling Self-Attention Graph (SAG) pooling Global sum, average, and max pooling Global gated attention pooling SortPool Spektral also includes lots of utilities for your graph deep learning projects. See how to get started with Spektral and have a look at the examples for some templates. The source code of the project is available on Github . Read the documentation here . You can also cite the paper introducing Spektral: Graph Neural Networks in TensorFlow and Keras with Spektral (ICML 2020 - GRL+ Workshop).","title":"Welcome to Spektral"},{"location":"#installation","text":"Spektral is compatible with Python 3.5+, and is tested on Ubuntu 16.04+ and MacOS. Other Linux distros should work as well, but Windows is not supported for now. Some optional features of Spektral depend on RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda). The simplest way to install Spektral is from PyPi: pip install spektral To install Spektral from source, run this in a terminal: git clone https://github.com/danielegrattarola/spektral.git cd spektral python setup.py install # Or 'pip install .' To install Spektral on Google Colab : ! pip install spektral","title":"Installation"},{"location":"#tensorflow-1-and-keras","text":"Starting from version 0.3, Spektral only supports TensorFlow 2 and tf.keras . The old version of Spektral, which is based on TensorFlow 1 and the stand-alone Keras library, is still available on the tf1 branch on GitHub and can be installed from source: git clone https://github.com/danielegrattarola/spektral.git cd spektral git checkout tf1 python setup.py install # Or 'pip install .' In the future, the TF1-compatible version of Spektral (<0.2) will receive bug fixes, but all new features will only support TensorFlow 2.","title":"TensorFlow 1 and Keras"},{"location":"#contributing","text":"Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework. The contribution guidelines are available here and a list of feature requests is available here .","title":"Contributing"},{"location":"about/","text":"About Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"about/#about","text":"Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"chem/","text":"Chemistry This module provides some functions to work with molecules, and requires the RDKit library to be installed on the system. numpy_to_rdkit spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule numpy_to_smiles spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule rdkit_to_smiles spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule sdf_to_nx spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs. nx_to_sdf spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format. validate_rdkit spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise get_atomic_symbol spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol get_atomic_num spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118 valid_score spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule novel_score spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule unique_score spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules enable_rdkit_log spektral.chem.enable_rdkit_log() Enables RDkit logging. Return plot_rdkit spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return The image as np.array plot_rdkit_svg_grid spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"Chemistry"},{"location":"chem/#chemistry","text":"This module provides some functions to work with molecules, and requires the RDKit library to be installed on the system.","title":"Chemistry"},{"location":"chem/#numpy_to_rdkit","text":"spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule","title":"numpy_to_rdkit"},{"location":"chem/#numpy_to_smiles","text":"spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule","title":"numpy_to_smiles"},{"location":"chem/#rdkit_to_smiles","text":"spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule","title":"rdkit_to_smiles"},{"location":"chem/#sdf_to_nx","text":"spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs.","title":"sdf_to_nx"},{"location":"chem/#nx_to_sdf","text":"spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format.","title":"nx_to_sdf"},{"location":"chem/#validate_rdkit","text":"spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise","title":"validate_rdkit"},{"location":"chem/#get_atomic_symbol","text":"spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol","title":"get_atomic_symbol"},{"location":"chem/#get_atomic_num","text":"spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118","title":"get_atomic_num"},{"location":"chem/#valid_score","text":"spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule","title":"valid_score"},{"location":"chem/#novel_score","text":"spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule","title":"novel_score"},{"location":"chem/#unique_score","text":"spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules","title":"unique_score"},{"location":"chem/#enable_rdkit_log","text":"spektral.chem.enable_rdkit_log() Enables RDkit logging. Return","title":"enable_rdkit_log"},{"location":"chem/#plot_rdkit","text":"spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return The image as np.array","title":"plot_rdkit"},{"location":"chem/#plot_rdkit_svg_grid","text":"spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"plot_rdkit_svg_grid"},{"location":"data/","text":"Representing graphs In Spektral, graphs are represented as matrices: A is the adjacency matrix of shape (N, N) , where N is the number of nodes. A is a binary matrix where A[i, j] = 1 if there is an edge between nodes i and j , and 0 otherwise. X is the node attributes matrix of shape (N, F) , where F is the size of the node attributes. Sometimes, we can also have edge attributes of size S , which we store in a matrix E of shape (n_edges, S) where each row is associated to a non-zero entry of A : assuming that A is a Scipy sparse matrix, we have that E[i] is the attribute associated to A.data[i] . Modes Spektral supports four different ways of representing graphs or batches of graphs, which we refer to as data modes . In single mode , we have one graph with its adjacency matrix and attributes; Disjoint mode is a special case of single mode, where the graph is the disjoint union of a set of graphs; In batch mode , a set of graphs is represented by stacking their adjacency and node attributes matrices in higher order tensors of shape (batch, N, ...) ; In mixed mode , we have a single adjacency matrix shared by a set of graphs; the adjacency matrix will be in single mode, but the node attributes will be in batch mode. The difference between the four data modes can be easily seen in how A , X , and E have different shapes in each case: Mode A.shape X.shape E.shape Single (N, N) (N, F) (n_edges, S) Disjoint (N, N) (N, F) (n_edges, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) N/A Single mode In single mode the data describes a single graph where: A is a sparse matrix of shape (N, N) ; X is a matrix of shape (N, F) ; When edge attributes are present, we represent them as a matrix E of shape (n_edges, S) so that there is a correspondence between E[i] and A.data[i] . Three very popular datasets in this setting are the citation networks: Cora, Citeseer, and Pubmed. To load a citation network, you can use the built-in loader: >>> from spektral.datasets import citation >>> A, X, _, _, _, _ = citation.load_data('cora') Loading cora dataset >>> A.shape (2708, 2708) >>> X.shape (2708, 1433) Disjoint mode Disjoint mode is a smart way of representing a set of graphs as a single graph. In particular, the disjoint union of a batch is a graph where A is a sparse block diagonal matrix, where each block is the adjacency matrix A_i of the i-th graph; X is obtained by stacking the node attributes matrices of the graphs. When edge attributes are present, we represent them as a matrix E of shape (n_edges, S) so that there is a correspondence between E[i] and A.data[i] . In order to keep track of different graphs in the disjoint union, we use an additional array of integers I that identifies which nodes belong to the same graph. For convolutional layers, disjoint mode is indistinguishable from single mode because it is not possible to exchange messages between the components of the graph, so I is not needed to compute the output. Pooling layers, on the other hand, require I to know which nodes can be pooled together. Hierarchical pooling layers will return a reduced version of I along with the reduced graphs. Global pooling layers will consume I and reduce the graphs to single vectors. Utilities for creating the disjoint union of a list of graphs are provided in spektral.utils.data : >>> from spektral.utils.data import numpy_to_disjoint >>> A_list = [np.ones((2, 2)), np.ones((3, 3))] # One graph has 2 nodes, the other has 3 >>> X_list = [np.random.randn(2, 4), np.random.randn(3, 4)] # F = 4 >>> X, A, I = numpy_to_disjoint(X_list, A_list) >>> X.shape (5, 4) >>> A.shape (5, 5) >>> A.toarray() array([[1., 1., 0., 0., 0.], [1., 1., 0., 0., 0.], [0., 0., 1., 1., 1.], [0., 0., 1., 1., 1.], [0., 0., 1., 1., 1.]]) >>> I array([0, 0, 1, 1, 1]) Batch mode In batch mode , graphs have the same number of nodes and are stacked in tensors of shape (batch, N, ...) . Due to the general lack of support for sparse higher-order tensors both in Scipy and TensorFlow, A and X will be dense tensors. In this case, edge attributes must also be reshaped and made dense, so that E has shape (batch, N, N, S) (the attributes of non-existing edges are usually all zeros). Note that if the graphs have variable number of nodes, the matrices must be zero-padded so that they have the same N . If you don't want to zero-pad the graphs or work with dense inputs, it is better to work in disjoint mode instead. The advantage of batch mode is that it is more intuitive and it allows to use the training loop of tf.keras without any modifications. Also, some pooling layers like DiffPool and MinCutPool will only work in batch mode. For example, the QM9 dataset of small molecules will be loaded in batch mode by default: >>> from spektral.datasets import qm9 >>> A, X, E, y = qm9.load_data() Loading QM9 dataset. Reading SDF >>> A.shape (133885, 9, 9) >>> X.shape (133885, 9, 6) >>> E.shape (133885, 9, 9, 5) Mixed mode In mixed mode we consider a single adjacency matrix that acts as the support for different node attributes (also sometimes called \"signals\"). In this case we have that: A is a sparse matrix of shape (N, N) ; X is a tensor in batch mode, of shape (batch, N, F) ; Currently, there are no layers in Spektral that support mixed mode and edge attributes. An example of a mixed mode dataset is the MNIST random grid ( Defferrard et al., 2016 ): >>> from spektral.datasets import mnist >>> X_tr, y_tr, X_va, y_va, X_te, y_te, A = mnist.load_data() >>> A.shape (784, 784) >>> X_tr.shape (50000, 784, 1)","title":"Data representation"},{"location":"data/#representing-graphs","text":"In Spektral, graphs are represented as matrices: A is the adjacency matrix of shape (N, N) , where N is the number of nodes. A is a binary matrix where A[i, j] = 1 if there is an edge between nodes i and j , and 0 otherwise. X is the node attributes matrix of shape (N, F) , where F is the size of the node attributes. Sometimes, we can also have edge attributes of size S , which we store in a matrix E of shape (n_edges, S) where each row is associated to a non-zero entry of A : assuming that A is a Scipy sparse matrix, we have that E[i] is the attribute associated to A.data[i] .","title":"Representing graphs"},{"location":"data/#modes","text":"Spektral supports four different ways of representing graphs or batches of graphs, which we refer to as data modes . In single mode , we have one graph with its adjacency matrix and attributes; Disjoint mode is a special case of single mode, where the graph is the disjoint union of a set of graphs; In batch mode , a set of graphs is represented by stacking their adjacency and node attributes matrices in higher order tensors of shape (batch, N, ...) ; In mixed mode , we have a single adjacency matrix shared by a set of graphs; the adjacency matrix will be in single mode, but the node attributes will be in batch mode. The difference between the four data modes can be easily seen in how A , X , and E have different shapes in each case: Mode A.shape X.shape E.shape Single (N, N) (N, F) (n_edges, S) Disjoint (N, N) (N, F) (n_edges, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) N/A","title":"Modes"},{"location":"data/#single-mode","text":"In single mode the data describes a single graph where: A is a sparse matrix of shape (N, N) ; X is a matrix of shape (N, F) ; When edge attributes are present, we represent them as a matrix E of shape (n_edges, S) so that there is a correspondence between E[i] and A.data[i] . Three very popular datasets in this setting are the citation networks: Cora, Citeseer, and Pubmed. To load a citation network, you can use the built-in loader: >>> from spektral.datasets import citation >>> A, X, _, _, _, _ = citation.load_data('cora') Loading cora dataset >>> A.shape (2708, 2708) >>> X.shape (2708, 1433)","title":"Single mode"},{"location":"data/#disjoint-mode","text":"Disjoint mode is a smart way of representing a set of graphs as a single graph. In particular, the disjoint union of a batch is a graph where A is a sparse block diagonal matrix, where each block is the adjacency matrix A_i of the i-th graph; X is obtained by stacking the node attributes matrices of the graphs. When edge attributes are present, we represent them as a matrix E of shape (n_edges, S) so that there is a correspondence between E[i] and A.data[i] . In order to keep track of different graphs in the disjoint union, we use an additional array of integers I that identifies which nodes belong to the same graph. For convolutional layers, disjoint mode is indistinguishable from single mode because it is not possible to exchange messages between the components of the graph, so I is not needed to compute the output. Pooling layers, on the other hand, require I to know which nodes can be pooled together. Hierarchical pooling layers will return a reduced version of I along with the reduced graphs. Global pooling layers will consume I and reduce the graphs to single vectors. Utilities for creating the disjoint union of a list of graphs are provided in spektral.utils.data : >>> from spektral.utils.data import numpy_to_disjoint >>> A_list = [np.ones((2, 2)), np.ones((3, 3))] # One graph has 2 nodes, the other has 3 >>> X_list = [np.random.randn(2, 4), np.random.randn(3, 4)] # F = 4 >>> X, A, I = numpy_to_disjoint(X_list, A_list) >>> X.shape (5, 4) >>> A.shape (5, 5) >>> A.toarray() array([[1., 1., 0., 0., 0.], [1., 1., 0., 0., 0.], [0., 0., 1., 1., 1.], [0., 0., 1., 1., 1.], [0., 0., 1., 1., 1.]]) >>> I array([0, 0, 1, 1, 1])","title":"Disjoint mode"},{"location":"data/#batch-mode","text":"In batch mode , graphs have the same number of nodes and are stacked in tensors of shape (batch, N, ...) . Due to the general lack of support for sparse higher-order tensors both in Scipy and TensorFlow, A and X will be dense tensors. In this case, edge attributes must also be reshaped and made dense, so that E has shape (batch, N, N, S) (the attributes of non-existing edges are usually all zeros). Note that if the graphs have variable number of nodes, the matrices must be zero-padded so that they have the same N . If you don't want to zero-pad the graphs or work with dense inputs, it is better to work in disjoint mode instead. The advantage of batch mode is that it is more intuitive and it allows to use the training loop of tf.keras without any modifications. Also, some pooling layers like DiffPool and MinCutPool will only work in batch mode. For example, the QM9 dataset of small molecules will be loaded in batch mode by default: >>> from spektral.datasets import qm9 >>> A, X, E, y = qm9.load_data() Loading QM9 dataset. Reading SDF >>> A.shape (133885, 9, 9) >>> X.shape (133885, 9, 6) >>> E.shape (133885, 9, 9, 5)","title":"Batch mode"},{"location":"data/#mixed-mode","text":"In mixed mode we consider a single adjacency matrix that acts as the support for different node attributes (also sometimes called \"signals\"). In this case we have that: A is a sparse matrix of shape (N, N) ; X is a tensor in batch mode, of shape (batch, N, F) ; Currently, there are no layers in Spektral that support mixed mode and edge attributes. An example of a mixed mode dataset is the MNIST random grid ( Defferrard et al., 2016 ): >>> from spektral.datasets import mnist >>> X_tr, y_tr, X_va, y_va, X_te, y_te, A = mnist.load_data() >>> A.shape (784, 784) >>> X_tr.shape (50000, 784, 1)","title":"Mixed mode"},{"location":"datasets/","text":"Datasets Citation networks load_data spektral.datasets.citation.load_data(dataset_name='cora', normalize_features=True, random_split=False) Loads a citation dataset (Cora, Citeseer or Pubmed) using the \"Planetoid\" splits intialliy defined in Yang et al. (2016) . The train, test, and validation splits are given as binary masks. Node attributes are bag-of-words vectors representing the most common words in the text document associated to each node. Two papers are connected if either one cites the other. Labels represent the class of the paper. Arguments dataset_name : name of the dataset to load ( 'cora' , 'citeseer' , or 'pubmed' ); normalize_features : if True, the node features are normalized; random_split : if True, return a randomized split (20 nodes per class for training, 30 nodes per class for validation and the remaining nodes for testing, Shchur et al. (2018) ). Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits. GraphSAGE datasets load_data spektral.datasets.graphsage.load_data(dataset_name, max_degree=-1, normalize_features=True) Loads one of the datasets (PPI or Reddit) used in Hamilton & Ying (2017) . The PPI dataset (originally Stark et al. (2006) ) for inductive node classification uses positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels. The Reddit dataset consists of a graph made of Reddit posts in the month of September, 2014. The label for each node is the community that a post belongs to. The graph is built by sampling 50 large communities and two nodes are connected if the same user commented on both. Node features are obtained by concatenating the average GloVe CommonCrawl vectors of the title and comments, the post's score and the number of comments. The train, test, and validation splits are returned as binary masks. Arguments dataset_name : name of the dataset to load ( 'ppi' , or 'reddit' ); max_degree : int, if positive, subsample edges so that each node has the specified maximum degree. normalize_features : if True, the node features are normalized; Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits. TU Dortmund Benchmark Datasets for Graph Kernels load_data spektral.datasets.tud.load_data(dataset_name, clean=False) Loads one of the Benchmark Data Sets for Graph Kernels from TU Dortmund ( link ). The node features are computed by concatenating the following features for each node: node attributes, if available, normalized as specified in normalize_features ; clustering coefficient, normalized with z-score; node degrees, normalized as specified in normalize_features ; node labels, if available, one-hot encoded. Arguments dataset_name : name of the dataset to load (see spektral.datasets.tud.AVAILABLE_DATASETS ). clean : if True, return a version of the dataset with no isomorphic graphs. Return a list of adjacency matrices; a list of node feature matrices; a numpy array containing the one-hot encoded targets. Open Graph Benchmark (OGB) graph_to_numpy spektral.datasets.ogb.graph_to_numpy(graph, dtype=None) Converts a graph in OGB's library-agnostic format to a representation in Numpy/Scipy. See the Open Graph Benchmark's website for more information. Arguments graph : OGB library-agnostic graph; dtype : if set, all output arrays will be cast to this dtype. Return X: np.array of shape (N, F) with the node features; A: scipy.sparse adjacency matrix of shape (N, N) in COOrdinate format; E: if edge features are available, np.array of shape (n_edges, S), None otherwise. dataset_to_numpy spektral.datasets.ogb.dataset_to_numpy(dataset, indices=None, dtype=None) Converts a dataset in OGB's library-agnostic version to lists of Numpy/Scipy arrays. See the Open Graph Benchmark's website for more information. Arguments dataset : OGB library-agnostic dataset (e.g., GraphPropPredDataset); indices : optional, a list of integer indices; if provided, only these graphs will be converted; dtype : if set, the arrays in the returned lists will have this dtype. Return X_list: list of np.arrays of (variable) shape (N, F) with node features; A_list: list of scipy.sparse adjacency matrices of (variable) shape (N, N); E_list: list of np.arrays of (variable) shape (n_nodes, S) with edge attributes. If edge attributes are not available, a list of None. y_list: np.array of shape (n_graphs, n_tasks) with the task labels; QM9 Small Molecules load_data spektral.datasets.qm9.load_data(nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None, return_type='numpy') Loads the QM9 chemical data set of small molecules. Nodes represent heavy atoms (hydrogens are discarded), edges represent chemical bonds. The node features represent the chemical properties of each atom, and are loaded according to the nf_keys argument. See spektral.datasets.qm9.NODE_FEATURES for possible node features, and see this link for the meaning of each property. Usually, it is sufficient to load the atomic number. The edge features represent the type and stereoscopy of each chemical bond between two atoms. See spektral.datasets.qm9.EDGE_FEATURES for possible edge features, and see this link for the meaning of each property. Usually, it is sufficient to load the type of bond. Arguments nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in ascending order by number of atoms). return_type : 'numpy' , 'networkx' , or 'sdf' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels. MNIST KNN Grid load_data spektral.datasets.mnist.load_data(k=8, noise_level=0.0) Loads the MNIST dataset and a K-NN graph to perform graph signal classification, as described by Defferrard et al. (2016) . The K-NN graph is statically determined from a regular grid of pixels using the 2d coordinates. The node features of each graph are the MNIST digits vectorized and rescaled to [0, 1]. Two nodes are connected if they are neighbours according to the K-NN graph. Labels are the MNIST class associated to each sample. Arguments k : int, number of neighbours for each node; noise_level : fraction of edges to flip (from 0 to 1 and vice versa); Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid; Delaunay Triangulations generate_data spektral.datasets.delaunay.generate_data(classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None, return_type='numpy') Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Node attributes are the 2D coordinates of the points. Two nodes are connected if they share an edge in the Delaunay triangulation. Labels represent the class of the graph (0 to 20, each class index i represent the \"difficulty\" of the classification problem 0 v. i. In other words, the higher the class index, the more similar the class is to class 0). Arguments classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; return_type : 'numpy' or 'networkx' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/#citation-networks","text":"","title":"Citation networks"},{"location":"datasets/#load_data","text":"spektral.datasets.citation.load_data(dataset_name='cora', normalize_features=True, random_split=False) Loads a citation dataset (Cora, Citeseer or Pubmed) using the \"Planetoid\" splits intialliy defined in Yang et al. (2016) . The train, test, and validation splits are given as binary masks. Node attributes are bag-of-words vectors representing the most common words in the text document associated to each node. Two papers are connected if either one cites the other. Labels represent the class of the paper. Arguments dataset_name : name of the dataset to load ( 'cora' , 'citeseer' , or 'pubmed' ); normalize_features : if True, the node features are normalized; random_split : if True, return a randomized split (20 nodes per class for training, 30 nodes per class for validation and the remaining nodes for testing, Shchur et al. (2018) ). Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits.","title":"load_data"},{"location":"datasets/#graphsage-datasets","text":"","title":"GraphSAGE datasets"},{"location":"datasets/#load_data_1","text":"spektral.datasets.graphsage.load_data(dataset_name, max_degree=-1, normalize_features=True) Loads one of the datasets (PPI or Reddit) used in Hamilton & Ying (2017) . The PPI dataset (originally Stark et al. (2006) ) for inductive node classification uses positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels. The Reddit dataset consists of a graph made of Reddit posts in the month of September, 2014. The label for each node is the community that a post belongs to. The graph is built by sampling 50 large communities and two nodes are connected if the same user commented on both. Node features are obtained by concatenating the average GloVe CommonCrawl vectors of the title and comments, the post's score and the number of comments. The train, test, and validation splits are returned as binary masks. Arguments dataset_name : name of the dataset to load ( 'ppi' , or 'reddit' ); max_degree : int, if positive, subsample edges so that each node has the specified maximum degree. normalize_features : if True, the node features are normalized; Return Adjacency matrix; Node features; Labels; Three binary masks for train, validation, and test splits.","title":"load_data"},{"location":"datasets/#tu-dortmund-benchmark-datasets-for-graph-kernels","text":"","title":"TU Dortmund Benchmark Datasets for Graph Kernels"},{"location":"datasets/#load_data_2","text":"spektral.datasets.tud.load_data(dataset_name, clean=False) Loads one of the Benchmark Data Sets for Graph Kernels from TU Dortmund ( link ). The node features are computed by concatenating the following features for each node: node attributes, if available, normalized as specified in normalize_features ; clustering coefficient, normalized with z-score; node degrees, normalized as specified in normalize_features ; node labels, if available, one-hot encoded. Arguments dataset_name : name of the dataset to load (see spektral.datasets.tud.AVAILABLE_DATASETS ). clean : if True, return a version of the dataset with no isomorphic graphs. Return a list of adjacency matrices; a list of node feature matrices; a numpy array containing the one-hot encoded targets.","title":"load_data"},{"location":"datasets/#open-graph-benchmark-ogb","text":"","title":"Open Graph Benchmark (OGB)"},{"location":"datasets/#graph_to_numpy","text":"spektral.datasets.ogb.graph_to_numpy(graph, dtype=None) Converts a graph in OGB's library-agnostic format to a representation in Numpy/Scipy. See the Open Graph Benchmark's website for more information. Arguments graph : OGB library-agnostic graph; dtype : if set, all output arrays will be cast to this dtype. Return X: np.array of shape (N, F) with the node features; A: scipy.sparse adjacency matrix of shape (N, N) in COOrdinate format; E: if edge features are available, np.array of shape (n_edges, S), None otherwise.","title":"graph_to_numpy"},{"location":"datasets/#dataset_to_numpy","text":"spektral.datasets.ogb.dataset_to_numpy(dataset, indices=None, dtype=None) Converts a dataset in OGB's library-agnostic version to lists of Numpy/Scipy arrays. See the Open Graph Benchmark's website for more information. Arguments dataset : OGB library-agnostic dataset (e.g., GraphPropPredDataset); indices : optional, a list of integer indices; if provided, only these graphs will be converted; dtype : if set, the arrays in the returned lists will have this dtype. Return X_list: list of np.arrays of (variable) shape (N, F) with node features; A_list: list of scipy.sparse adjacency matrices of (variable) shape (N, N); E_list: list of np.arrays of (variable) shape (n_nodes, S) with edge attributes. If edge attributes are not available, a list of None. y_list: np.array of shape (n_graphs, n_tasks) with the task labels;","title":"dataset_to_numpy"},{"location":"datasets/#qm9-small-molecules","text":"","title":"QM9 Small Molecules"},{"location":"datasets/#load_data_3","text":"spektral.datasets.qm9.load_data(nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None, return_type='numpy') Loads the QM9 chemical data set of small molecules. Nodes represent heavy atoms (hydrogens are discarded), edges represent chemical bonds. The node features represent the chemical properties of each atom, and are loaded according to the nf_keys argument. See spektral.datasets.qm9.NODE_FEATURES for possible node features, and see this link for the meaning of each property. Usually, it is sufficient to load the atomic number. The edge features represent the type and stereoscopy of each chemical bond between two atoms. See spektral.datasets.qm9.EDGE_FEATURES for possible edge features, and see this link for the meaning of each property. Usually, it is sufficient to load the type of bond. Arguments nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in ascending order by number of atoms). return_type : 'numpy' , 'networkx' , or 'sdf' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"load_data"},{"location":"datasets/#mnist-knn-grid","text":"","title":"MNIST KNN Grid"},{"location":"datasets/#load_data_4","text":"spektral.datasets.mnist.load_data(k=8, noise_level=0.0) Loads the MNIST dataset and a K-NN graph to perform graph signal classification, as described by Defferrard et al. (2016) . The K-NN graph is statically determined from a regular grid of pixels using the 2d coordinates. The node features of each graph are the MNIST digits vectorized and rescaled to [0, 1]. Two nodes are connected if they are neighbours according to the K-NN graph. Labels are the MNIST class associated to each sample. Arguments k : int, number of neighbours for each node; noise_level : fraction of edges to flip (from 0 to 1 and vice versa); Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"load_data"},{"location":"datasets/#delaunay-triangulations","text":"","title":"Delaunay Triangulations"},{"location":"datasets/#generate_data","text":"spektral.datasets.delaunay.generate_data(classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None, return_type='numpy') Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Node attributes are the 2D coordinates of the points. Two nodes are connected if they share an edge in the Delaunay triangulation. Labels represent the class of the graph (0 to 20, each class index i represent the \"difficulty\" of the classification problem 0 v. i. In other words, the higher the class index, the more similar the class is to class 0). Arguments classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; return_type : 'numpy' or 'networkx' , data format to return; Return if return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"generate_data"},{"location":"examples/","text":"Examples This is a collection of example scripts that you can use as template to solve your own tasks. Node classification Node classification on citation networks with GCN ; Node classification on citation networks with ChebNets ; Node classification on citation networks with GAT ; Node classification on citation networks with ARMA ; Node classification on citation networks with SimpleGCN ; Node classification on the Open Graph Benchmark dataset (ogbn-proteins) ; Graph-level prediction Batch mode: Classification of synthetic graphs with GAT ; Regression of molecular properties on QM9 with ECC ; Disjoint mode: Classification of synthetic graphs with TopK pooling ; Regression of molecular properties on QM9 with ECC ; Graph signal classification Graph signal classification on MNIST (mixed mode) ; Other applications Node clustering on citation networks with minCUT pooling (unsupervised) ; The following notebooks are available on Kaggle with more visualizations (maintained by @kmader ): MNIST Graph Deep Learning ; MNIST Graph Pooling ;","title":"Examples"},{"location":"examples/#examples","text":"This is a collection of example scripts that you can use as template to solve your own tasks.","title":"Examples"},{"location":"examples/#node-classification","text":"Node classification on citation networks with GCN ; Node classification on citation networks with ChebNets ; Node classification on citation networks with GAT ; Node classification on citation networks with ARMA ; Node classification on citation networks with SimpleGCN ; Node classification on the Open Graph Benchmark dataset (ogbn-proteins) ;","title":"Node classification"},{"location":"examples/#graph-level-prediction","text":"Batch mode: Classification of synthetic graphs with GAT ; Regression of molecular properties on QM9 with ECC ; Disjoint mode: Classification of synthetic graphs with TopK pooling ; Regression of molecular properties on QM9 with ECC ;","title":"Graph-level prediction"},{"location":"examples/#graph-signal-classification","text":"Graph signal classification on MNIST (mixed mode) ;","title":"Graph signal classification"},{"location":"examples/#other-applications","text":"Node clustering on citation networks with minCUT pooling (unsupervised) ; The following notebooks are available on Kaggle with more visualizations (maintained by @kmader ): MNIST Graph Deep Learning ; MNIST Graph Pooling ;","title":"Other applications"},{"location":"getting-started/","text":"Getting started Spektral is designed according to the guiding principles of the Keras API to make things extremely simple for beginners while maintaining flexibility for experts and researchers. The most important modules of Spektral are layers.convolutional and layers.pooling , which offer a number of popular layers to start building graph neural networks (GNNs) right away. Because Spektral is designed as an extension of Keras, you can plug any Spektral layer into an existing Keras Model without modifications. Node classification on citation networks In this example, we will build a simple Graph Convolutional Network for semi-supervised classification of nodes. This is a simple but challenging task that consists of classifying text documents in a citation network . In this type of graph, each node represents a document and is associated to a binary bag-of-words attribute (1 if a given word appears in the text, 0 otherwise). If a document cites another, then there exist an undirected edge between the two corresponding nodes. Finally, each node has a class label that we want to predict. This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels. The datasets.citation module of Spektral lets you download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as: from spektral.datasets import citation A, X, y, train_mask, val_mask, test_mask = citation.load_data('cora') N = A.shape[0] F = X.shape[-1] n_classes = y.shape[-1] This will load the network's adjacency matrix A as a Scipy sparse matrix of shape (N, N) , the node features X of shape (N, F) , and the labels y of shape (N, n_classes) . The loader will also return some boolean masks to know which nodes belong to the training, validation and test sets ( train_mask, val_mask, test_mask ). Creating a GNN To create a GCN, we will use the GraphConv layer and the functional API of Keras: from spektral.layers import GraphConv from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dropout Building the model is no different than building any Keras model, but we will need to provide multiple inputs ( X and A ) to the GraphConv layers: X_in = Input(shape=(F, )) A_in = Input((N, ), sparse=True) X_1 = GraphConv(16, 'relu')([X_in, A_in]) X_1 = Dropout(0.5)(X_1) X_2 = GraphConv(n_classes, 'softmax')([X_1, A_in]) model = Model(inputs=[X_in, A_in], outputs=X_2) And that's it. We just built our first GNN in Spektral and Keras. Note how we used the familiar API of Keras to create the GCN layers, as well as the standard Dropout layer to regularize our model. All features of Keras are also supported by Spektral (including initializers, regularizers, etc.). An important thing to notice at this point is how we defined the Input layers of our model. Because the \"elements\" of our dataset are the node themselves, we are telling Keras to consider each node as a separate sample so that the batch axis is implicitly defined as None . In other words, a sample of the node attributes will be a vector of shape (F, ) and a sample of the adjacency matrix will be one row of shape (N, ) . Keep this detail in mind for later. Training the GNN When training GCN, we have to pre-process the adjacency matrix to 1) add self-loops and 2) scale the weights of a node's connections according to its degree. Some layers in Spektral require a different type of pre-processing in order to work correctly, and some work out-of-the-box on the binary A . The pre-processing required by each layer is available as a static class method preprocess() . In our example, the pre-processing required by GCN is: A = GraphConv.preprocess(A).astype('f4') And that's all! What's left now for us is to compile and train our model: model.compile(optimizer='adam', loss='categorical_crossentropy', weighted_metrics=['acc']) model.summary() Note that we used the weighted_metrics argument instead of the usual metrics . This is due to the particular semi-supervised problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later). We can now train the model using the native fit() method of Keras: # Prepare data X = X.toarray() A = A.astype('f4') validation_data = ([X, A], y, val_mask) # Train model model.fit([X, A], y, sample_weight=train_mask, validation_data=validation_data, batch_size=N, shuffle=False) There are a couple of things to note here. We have set batch_size=N and shuffle=False . This is because the default behaviour of Keras is to split the data into batches of 32 and shuffle the samples at each epoch. However, shuffling the adjacency matrix along one axis and not the other means that row i will represent a different node than column i . At the same time, if we split the graph into batches we may end up in a situation where we need to use a node attribute that is not part of the batch. The only solution is to take all the node features at the same time, hence batch_size=N . Finally, we used train_mask and val_mask as sample_weight . This means that, during training, the training nodes will have a weight of 1 and the validation nodes will have a weight of 0. Then, in validation, we will set the training nodes to have a weight of 0 and the validation nodes to have a weight of 1. This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X , A , and y for both training and valdation? The only thing that changes is the mask. This is also why we used the weighted_metrics keyword when compiling the model, so that our accuracy is calculated only on the correct nodes at each phase. Evaluating the model Once again, evaluation is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training (note that in model.evaluate() by default shuffle=False ): # Evaluate model eval_results = model.evaluate([X, A], y, sample_weight=test_mask, batch_size=N) print('Done.\\n' 'Test loss: {}\\n' 'Test accuracy: {}'.format(*eval_results)) Go create! You are now ready to use Spektral to create your own models. If you want to build a GNN for a specific task, chances are that everything you need is already part of Spektral. Check the examples for some ideas and practical tips. Remember to read the data representation section to learn different ways of representing a graph or batches of different graphs. Make sure to check the documentation, and get in touch on Github if you have a feature that you want to see implemented.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Spektral is designed according to the guiding principles of the Keras API to make things extremely simple for beginners while maintaining flexibility for experts and researchers. The most important modules of Spektral are layers.convolutional and layers.pooling , which offer a number of popular layers to start building graph neural networks (GNNs) right away. Because Spektral is designed as an extension of Keras, you can plug any Spektral layer into an existing Keras Model without modifications.","title":"Getting started"},{"location":"getting-started/#node-classification-on-citation-networks","text":"In this example, we will build a simple Graph Convolutional Network for semi-supervised classification of nodes. This is a simple but challenging task that consists of classifying text documents in a citation network . In this type of graph, each node represents a document and is associated to a binary bag-of-words attribute (1 if a given word appears in the text, 0 otherwise). If a document cites another, then there exist an undirected edge between the two corresponding nodes. Finally, each node has a class label that we want to predict. This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels. The datasets.citation module of Spektral lets you download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as: from spektral.datasets import citation A, X, y, train_mask, val_mask, test_mask = citation.load_data('cora') N = A.shape[0] F = X.shape[-1] n_classes = y.shape[-1] This will load the network's adjacency matrix A as a Scipy sparse matrix of shape (N, N) , the node features X of shape (N, F) , and the labels y of shape (N, n_classes) . The loader will also return some boolean masks to know which nodes belong to the training, validation and test sets ( train_mask, val_mask, test_mask ).","title":"Node classification on citation networks"},{"location":"getting-started/#creating-a-gnn","text":"To create a GCN, we will use the GraphConv layer and the functional API of Keras: from spektral.layers import GraphConv from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dropout Building the model is no different than building any Keras model, but we will need to provide multiple inputs ( X and A ) to the GraphConv layers: X_in = Input(shape=(F, )) A_in = Input((N, ), sparse=True) X_1 = GraphConv(16, 'relu')([X_in, A_in]) X_1 = Dropout(0.5)(X_1) X_2 = GraphConv(n_classes, 'softmax')([X_1, A_in]) model = Model(inputs=[X_in, A_in], outputs=X_2) And that's it. We just built our first GNN in Spektral and Keras. Note how we used the familiar API of Keras to create the GCN layers, as well as the standard Dropout layer to regularize our model. All features of Keras are also supported by Spektral (including initializers, regularizers, etc.). An important thing to notice at this point is how we defined the Input layers of our model. Because the \"elements\" of our dataset are the node themselves, we are telling Keras to consider each node as a separate sample so that the batch axis is implicitly defined as None . In other words, a sample of the node attributes will be a vector of shape (F, ) and a sample of the adjacency matrix will be one row of shape (N, ) . Keep this detail in mind for later.","title":"Creating a GNN"},{"location":"getting-started/#training-the-gnn","text":"When training GCN, we have to pre-process the adjacency matrix to 1) add self-loops and 2) scale the weights of a node's connections according to its degree. Some layers in Spektral require a different type of pre-processing in order to work correctly, and some work out-of-the-box on the binary A . The pre-processing required by each layer is available as a static class method preprocess() . In our example, the pre-processing required by GCN is: A = GraphConv.preprocess(A).astype('f4') And that's all! What's left now for us is to compile and train our model: model.compile(optimizer='adam', loss='categorical_crossentropy', weighted_metrics=['acc']) model.summary() Note that we used the weighted_metrics argument instead of the usual metrics . This is due to the particular semi-supervised problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later). We can now train the model using the native fit() method of Keras: # Prepare data X = X.toarray() A = A.astype('f4') validation_data = ([X, A], y, val_mask) # Train model model.fit([X, A], y, sample_weight=train_mask, validation_data=validation_data, batch_size=N, shuffle=False) There are a couple of things to note here. We have set batch_size=N and shuffle=False . This is because the default behaviour of Keras is to split the data into batches of 32 and shuffle the samples at each epoch. However, shuffling the adjacency matrix along one axis and not the other means that row i will represent a different node than column i . At the same time, if we split the graph into batches we may end up in a situation where we need to use a node attribute that is not part of the batch. The only solution is to take all the node features at the same time, hence batch_size=N . Finally, we used train_mask and val_mask as sample_weight . This means that, during training, the training nodes will have a weight of 1 and the validation nodes will have a weight of 0. Then, in validation, we will set the training nodes to have a weight of 0 and the validation nodes to have a weight of 1. This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X , A , and y for both training and valdation? The only thing that changes is the mask. This is also why we used the weighted_metrics keyword when compiling the model, so that our accuracy is calculated only on the correct nodes at each phase.","title":"Training the GNN"},{"location":"getting-started/#evaluating-the-model","text":"Once again, evaluation is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training (note that in model.evaluate() by default shuffle=False ): # Evaluate model eval_results = model.evaluate([X, A], y, sample_weight=test_mask, batch_size=N) print('Done.\\n' 'Test loss: {}\\n' 'Test accuracy: {}'.format(*eval_results))","title":"Evaluating the model"},{"location":"getting-started/#go-create","text":"You are now ready to use Spektral to create your own models. If you want to build a GNN for a specific task, chances are that everything you need is already part of Spektral. Check the examples for some ideas and practical tips. Remember to read the data representation section to learn different ways of representing a graph or batches of different graphs. Make sure to check the documentation, and get in touch on Github if you have a feature that you want to see implemented.","title":"Go create!"},{"location":"layers/base/","text":"Base layers This module contains a miscellany of layers that are not specifically for graph neural networks. [source] InnerProduct spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a 2d Tensor: \\langle \\x, \\x \\rangle = \\x\\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments trainable_kernel : add a trainable square matrix between the inner product (e.g., X @ W @ X.T ); activation : activation function to use; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the kernel; kernel_constraint : constraint applied to the kernel; [source] MinkowskiProduct spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None) Computes the hyperbolic inner product between elements of a rank 2 Tensor: \\langle \\x, \\x \\rangle = \\x \\, \\begin{pmatrix} \\I_{d \\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\, \\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments input_dim_1 : first dimension of the input Tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use; [source] Disjoint2Batch spektral.layers.Disjoint2Batch() Utility layer that converts data from disjoint mode to batch mode by zero-padding the node features and adjacency matrices. Mode : disjoint. This layer expects a sparse adjacency matrix. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) ; Output Batched node features of shape (batch, N_max, F) ; Batched adjacency matrix of shape (batch, N_max, N_max) ;","title":"Base Layers"},{"location":"layers/base/#base-layers","text":"This module contains a miscellany of layers that are not specifically for graph neural networks. [source]","title":"Base layers"},{"location":"layers/base/#innerproduct","text":"spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a 2d Tensor: \\langle \\x, \\x \\rangle = \\x\\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments trainable_kernel : add a trainable square matrix between the inner product (e.g., X @ W @ X.T ); activation : activation function to use; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the kernel; kernel_constraint : constraint applied to the kernel; [source]","title":"InnerProduct"},{"location":"layers/base/#minkowskiproduct","text":"spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None) Computes the hyperbolic inner product between elements of a rank 2 Tensor: \\langle \\x, \\x \\rangle = \\x \\, \\begin{pmatrix} \\I_{d \\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\, \\x^\\top. Mode : single. Input Tensor of shape (N, M) ; Output Tensor of shape (N, N) . Arguments input_dim_1 : first dimension of the input Tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use; [source]","title":"MinkowskiProduct"},{"location":"layers/base/#disjoint2batch","text":"spektral.layers.Disjoint2Batch() Utility layer that converts data from disjoint mode to batch mode by zero-padding the node features and adjacency matrices. Mode : disjoint. This layer expects a sparse adjacency matrix. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) ; Output Batched node features of shape (batch, N_max, F) ; Batched adjacency matrix of shape (batch, N_max, N_max) ;","title":"Disjoint2Batch"},{"location":"layers/convolution/","text":"Convolutional layers The message-passing layers from these papers are available in Spektral: Semi-Supervised Classification with Graph Convolutional Networks Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering Inductive Representation Learning on Large Graphs Graph Neural Networks with convolutional ARMA filters Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs Graph Attention Networks Predict then Propagate: Graph Neural Networks meet Personalized PageRank How Powerful are Graph Neural Networks? Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting Gated Graph Sequence Neural Networks Attention-based Graph Neural Network for Semi-supervised Learning Topology Adaptive Graph Convolutional Networks Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties Dynamic Graph CNN for Learning on Point Clouds Notation: N : number of nodes in the graph; F : dimension of the node attributes (i.e., each node has an attribute in \\mathbb{R}^F ); S : dimension of the edge attributes (i.e., each edge has an attribute in \\mathbb{R}^S ); \\A \\in \\{0, 1\\}^{N \\times N} : binary adjacency matrix; \\X \\in \\mathbb{R}^{ N \\times F } : node attributes matrix; \\E \\in \\mathbb{R}^{ N \\times N \\times S } : edge attributes matrix; \\D = \\textrm{diag} ( \\sum\\limits_{j=0} \\A_{ij} ) : degree matrix; \\W, \\V : trainable kernels; \\b : trainable bias vector; \\mathcal{N}(i) : the one-hop neighbourhood of node i ; F' : dimension of the node attributes after a message-passing layer; [source] GraphConv spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer (GCN) as presented by Kipf & Welling (2016) . Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b where \\hat \\A = \\A + \\I is the adjacency matrix with added self-loops and \\hat\\D is its degree matrix. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] ChebConv spektral.layers.ChebConv(channels, K=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)} + \\b^{(k)}, where \\T^{(0)}, ..., \\T^{(K - 1)} are Chebyshev polynomials of \\tilde \\L defined as \\T^{(0)} = \\X \\\\ \\T^{(1)} = \\tilde \\L \\X \\\\ \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)}, where \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; A list of K Chebyshev polynomials of shape [([batch], N, N), ..., ([batch], N, N)] ; can be computed with spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : order of the Chebyshev polynomials; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GraphSageConv spektral.layers.GraphSageConv(channels, aggregate_op='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSAGE layer as presented by Hamilton et al. (2017) . Mode : single, disjoint. This layer computes: \\Z = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\ \\Z = \\frac{\\Z}{\\|\\Z\\|} where \\textrm{AGGREGATE} is a function to aggregate a node's neighbourhood. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; aggregate_op : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] ARMAConv spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA _K filters, as presented by Bianchi et al. (2019) . Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)}, where K is the order of the ARMA _K filter, and where: \\bar \\X_k^{(t + 1)} = \\sigma \\left(\\tilde \\L \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right) is a recursive approximation of an ARMA _1 filter, where \\bar \\X^{(0)} = \\X and \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; Normalized and rescaled Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_laplacian and spektral.utils.convolution.rescale_laplacian . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; order : order of the full ARMA _K filter, i.e., the number of parallel stacks in the layer; iterations : number of iterations to compute each ARMA _1 approximation; share_weights : share the weights in each ARMA _1 stack. gcn_activation : activation function to use to compute each ARMA _1 stack; dropout_rate : dropout rate for skip connection; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] EdgeConditionedConv spektral.layers.EdgeConditionedConv(channels, kernel_network=None, root=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer (ECC) as presented by Simonovsky & Komodakis (2017) . Mode : single, disjoint, batch. Notes : - In single mode, if the adjacency matrix is dense it will be converted to a SparseTensor automatically (which is an expensive operation). For each node i , this layer computes: \\Z_i = \\X_{i} \\W_{\\textrm{root}} + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_{j} \\textrm{MLP}(\\E_{ji}) + \\b where \\textrm{MLP} is a multi-layer perceptron that outputs an edge-specific weight as a function of edge attributes. Input Node features of shape ([batch], N, F) ; Binary adjacency matrices of shape ([batch], N, N) ; Edge features. In single mode, shape (num_edges, S) ; in batch mode, shape (batch, N, N, S) . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers representing the hidden neurons of the kernel-generating network; 'root': if False, the layer will not consider the root node for computing the message passing (first term in equation above), but only the neighbours. activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GraphAttention spektral.layers.GraphAttention(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer (GAT) as presented by Velickovic et al. (2017) . Mode : single, disjoint, mixed, batch. This layer expects dense inputs when working in batch mode. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian: \\Z = \\mathbf{\\alpha}\\X\\W + \\b where \\mathbf{\\alpha}_{ij} = \\frac{ \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j] \\right) \\right) } {\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k] \\right) \\right) } where \\a \\in \\mathbb{R}^{2F'} is a trainable attention kernel. Dropout is also applied to \\alpha before computing \\Z . Parallel attention heads are computed in parallel and their results are aggregated by concatenation or average. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Node features with the same shape as the input, but with the last dimension changed to channels ; if return_attn_coef=True , a list with the attention coefficients for each attention head. Each attention coefficient matrix has shape ([batch], N, N) . Arguments channels : number of output channels; attn_heads : number of attention heads to use; concat_heads : bool, whether to concatenate the output of the attention heads instead of averaging; dropout_rate : internal dropout rate for attention coefficients; return_attn_coef : if True, return the attention coefficients for the given input (one N x N matrix for each head). activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; attn_kernel_initializer : initializer for the attention weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; attn_kernel_regularizer : regularization applied to the attention kernels; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; attn_kernel_constraint : constraint applied to the attention kernels; bias_constraint : constraint applied to the bias vector. [source] GraphConvSkip spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A simple convolutional layer with a skip connection. Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b where \\A does not have self-loops (unlike in GraphConv). Input Node features of shape ([batch], N, F) ; Normalized adjacency matrix of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_adjacency . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] APPNP spektral.layers.APPNP(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . This layer computes: \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\ \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} + \\alpha \\Z^{(0)}, where \\alpha is the teleport probability and \\textrm{MLP} is a multi-layer perceptron. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; alpha : teleport probability during propagation; propagations : number of propagation steps; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GINConv spektral.layers.GINConv(channels, epsilon=None, mlp_hidden=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) as presented by Xu et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\Z_i = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_j \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; epsilon : unnamed parameter, see Xu et al. (2018) , and the equation above. By setting epsilon=None , the parameter will be learned (default behaviour). If given as a value, the parameter will stay fixed. mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] DiffusionConv spektral.layers.DiffusionConv(channels, num_diffusion_steps=6, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None, activation='tanh') Applies Graph Diffusion Convolution as descibed by Li et al. (2016) Mode : single, disjoint, mixed, batch. This layer expects a dense adjacency matrix. Given a number of diffusion steps K and a row normalized adjacency matrix \\hat \\A , this layer calculates the q'th channel as: \\mathbf{H}_{~:,~q} = \\sigma\\left( \\sum_{f=1}^{F} \\left( \\sum_{k=0}^{K-1}\\theta_k {\\hat \\A}^k \\right) \\X_{~:,~f} \\right) Input Node features of shape ([batch], N, F) ; Normalized adjacency or attention coef. matrix \\hat \\A of shape ([batch], N, N) ; Use DiffusionConvolution.preprocess to normalize. Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; num_diffusion_steps : How many diffusion steps to consider. K in paper. activation : activation function \\sigma ; ( \\tanh by default) kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] GatedGraphConv spektral.layers.GatedGraphConv(channels, n_layers, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated graph convolutional layer as presented by Li et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer repeatedly applies a GRU cell L times to the node attributes \\begin{align} & \\h^{(0)}_i = \\X_i \\| \\mathbf{0} \\\\ & \\m^{(l)}_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\h^{(l - 1)}_j \\W \\\\ & \\h^{(l)}_i = \\textrm{GRU} \\left(\\m^{(l)}_i, \\h^{(l - 1)}_i \\right) \\\\ & \\Z_i = h^{(L)}_i \\end{align} where \\textrm{GRU} is the GRU cell. Input Node features of shape (N, F) ; note that F must be smaller or equal than channels . Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; n_layers : integer, number of iterations with the GRU cell; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] AGNNConv spektral.layers.AGNNConv(trainable=True, activation=None) An Attention-based Graph Neural Network (AGNN) as presented by Thekumparampil et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes: \\Z = \\P\\X where \\P_{ij} = \\frac{ \\exp \\left( \\beta \\cos \\left( \\X_i, \\X_j \\right) \\right) }{ \\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp \\left( \\beta \\cos \\left( \\X_i, \\X_k \\right) \\right) } and \\beta is a trainable parameter. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input. Arguments trainable : boolean, if True, then beta is a trainable parameter. Otherwise, beta is fixed to 1; activation : activation function to use; [source] TAGConv spektral.layers.TAGConv(channels, K=3, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Topology Adaptive Graph Convolutional layer (TAG) as presented by Du et al. (2017) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes: \\Z = \\sum\\limits_{k=0}^{K} \\D^{-1/2}\\A^k\\D^{-1/2}\\X\\W^{(k)} Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; K : the order of the layer (i.e., the layer will consider a K-hop neighbourhood for each node); activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] CrystalConv spektral.layers.CrystalConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Crystal Graph Convolutional layer as presented by Xie & Grossman (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\H_i = \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\sigma \\left( \\z_{ij} \\W^{(f)} + \\b^{(f)} \\right) \\odot \\g \\left( \\z_{ij} \\W^{(s)} + \\b^{(s)} \\right) where \\z_{ij} = \\X_i \\| \\X_j \\| \\E_{ij} , \\sigma is a sigmoid activation, and g is the activation function (defined by the activation argument). Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Edge features of shape (num_edges, S) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] EdgeConv spektral.layers.EdgeConv(channels, mlp_hidden=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An Edge Convolutional layer as presented by Wang et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\Z_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}\\big( \\X_i \\| \\X_j - \\X_i \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] MessagePassing spektral.layers.MessagePassing(aggregate='sum') A general class for message passing as presented by Gilmer et al. (2017) . Mode : single, disjoint. This layer and all of its extensions expect a sparse adjacency matrix. This layer computes: \\Z_i = \\gamma \\left( \\X_i, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi \\left(\\X_i, \\X_j, \\E_{j,i} \\right) \\right), where \\gamma is a differentiable update function, \\phi is a differentiable message function, \\square is a permutation-invariant function to aggregate the messages (like the sum or the average), and \\E_{ij} is the edge attribute of edge i-j. By extending this class, it is possible to create any message-passing layer in single/disjoint mode. API: propagate(X, A, E=None, **kwargs) : propagate the messages and computes embeddings for each node in the graph. kwargs will be propagated as keyword arguments to message() , aggregate() and update() . message(X, **kwargs) : computes messages, equivalent to \\phi in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. Use self.get_i() and self.get_j() to gather the elements using the indices i or j of the adjacency matrix (e.g, self.get_j(X) will get the features of the neighbours). aggregate(messages, **kwargs) : aggregates the messages, equivalent to \\square in the definition. The behaviour of this function can also be controlled using the aggregate keyword in the constructor of the layer (supported aggregations: sum, mean, max, min, prod). Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. update(embeddings, **kwargs) : updates the aggregated messages to obtain the final node embeddings, equivalent to \\gamma in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. Arguments : aggregate : string or callable, an aggregate function. This flag can be used to control the behaviour of aggregate() wihtout re-implementing it. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. If callable, the function must have the signature foo(updates, indices, N) and return a rank 2 tensor with shape (N, ...) .","title":"Convolutional Layers"},{"location":"layers/convolution/#convolutional-layers","text":"The message-passing layers from these papers are available in Spektral: Semi-Supervised Classification with Graph Convolutional Networks Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering Inductive Representation Learning on Large Graphs Graph Neural Networks with convolutional ARMA filters Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs Graph Attention Networks Predict then Propagate: Graph Neural Networks meet Personalized PageRank How Powerful are Graph Neural Networks? Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting Gated Graph Sequence Neural Networks Attention-based Graph Neural Network for Semi-supervised Learning Topology Adaptive Graph Convolutional Networks Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties Dynamic Graph CNN for Learning on Point Clouds Notation: N : number of nodes in the graph; F : dimension of the node attributes (i.e., each node has an attribute in \\mathbb{R}^F ); S : dimension of the edge attributes (i.e., each edge has an attribute in \\mathbb{R}^S ); \\A \\in \\{0, 1\\}^{N \\times N} : binary adjacency matrix; \\X \\in \\mathbb{R}^{ N \\times F } : node attributes matrix; \\E \\in \\mathbb{R}^{ N \\times N \\times S } : edge attributes matrix; \\D = \\textrm{diag} ( \\sum\\limits_{j=0} \\A_{ij} ) : degree matrix; \\W, \\V : trainable kernels; \\b : trainable bias vector; \\mathcal{N}(i) : the one-hop neighbourhood of node i ; F' : dimension of the node attributes after a message-passing layer; [source]","title":"Convolutional layers"},{"location":"layers/convolution/#graphconv","text":"spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer (GCN) as presented by Kipf & Welling (2016) . Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b where \\hat \\A = \\A + \\I is the adjacency matrix with added self-loops and \\hat\\D is its degree matrix. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphConv"},{"location":"layers/convolution/#chebconv","text":"spektral.layers.ChebConv(channels, K=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)} + \\b^{(k)}, where \\T^{(0)}, ..., \\T^{(K - 1)} are Chebyshev polynomials of \\tilde \\L defined as \\T^{(0)} = \\X \\\\ \\T^{(1)} = \\tilde \\L \\X \\\\ \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)}, where \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; A list of K Chebyshev polynomials of shape [([batch], N, N), ..., ([batch], N, N)] ; can be computed with spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : order of the Chebyshev polynomials; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"ChebConv"},{"location":"layers/convolution/#graphsageconv","text":"spektral.layers.GraphSageConv(channels, aggregate_op='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSAGE layer as presented by Hamilton et al. (2017) . Mode : single, disjoint. This layer computes: \\Z = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\ \\Z = \\frac{\\Z}{\\|\\Z\\|} where \\textrm{AGGREGATE} is a function to aggregate a node's neighbourhood. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; aggregate_op : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphSageConv"},{"location":"layers/convolution/#armaconv","text":"spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA _K filters, as presented by Bianchi et al. (2019) . Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)}, where K is the order of the ARMA _K filter, and where: \\bar \\X_k^{(t + 1)} = \\sigma \\left(\\tilde \\L \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right) is a recursive approximation of an ARMA _1 filter, where \\bar \\X^{(0)} = \\X and \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I is the normalized Laplacian with a rescaled spectrum. Input Node features of shape ([batch], N, F) ; Normalized and rescaled Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_laplacian and spektral.utils.convolution.rescale_laplacian . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; order : order of the full ARMA _K filter, i.e., the number of parallel stacks in the layer; iterations : number of iterations to compute each ARMA _1 approximation; share_weights : share the weights in each ARMA _1 stack. gcn_activation : activation function to use to compute each ARMA _1 stack; dropout_rate : dropout rate for skip connection; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"ARMAConv"},{"location":"layers/convolution/#edgeconditionedconv","text":"spektral.layers.EdgeConditionedConv(channels, kernel_network=None, root=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer (ECC) as presented by Simonovsky & Komodakis (2017) . Mode : single, disjoint, batch. Notes : - In single mode, if the adjacency matrix is dense it will be converted to a SparseTensor automatically (which is an expensive operation). For each node i , this layer computes: \\Z_i = \\X_{i} \\W_{\\textrm{root}} + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_{j} \\textrm{MLP}(\\E_{ji}) + \\b where \\textrm{MLP} is a multi-layer perceptron that outputs an edge-specific weight as a function of edge attributes. Input Node features of shape ([batch], N, F) ; Binary adjacency matrices of shape ([batch], N, N) ; Edge features. In single mode, shape (num_edges, S) ; in batch mode, shape (batch, N, N, S) . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers representing the hidden neurons of the kernel-generating network; 'root': if False, the layer will not consider the root node for computing the message passing (first term in equation above), but only the neighbours. activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"EdgeConditionedConv"},{"location":"layers/convolution/#graphattention","text":"spektral.layers.GraphAttention(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer (GAT) as presented by Velickovic et al. (2017) . Mode : single, disjoint, mixed, batch. This layer expects dense inputs when working in batch mode. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian: \\Z = \\mathbf{\\alpha}\\X\\W + \\b where \\mathbf{\\alpha}_{ij} = \\frac{ \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j] \\right) \\right) } {\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp\\left( \\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k] \\right) \\right) } where \\a \\in \\mathbb{R}^{2F'} is a trainable attention kernel. Dropout is also applied to \\alpha before computing \\Z . Parallel attention heads are computed in parallel and their results are aggregated by concatenation or average. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Node features with the same shape as the input, but with the last dimension changed to channels ; if return_attn_coef=True , a list with the attention coefficients for each attention head. Each attention coefficient matrix has shape ([batch], N, N) . Arguments channels : number of output channels; attn_heads : number of attention heads to use; concat_heads : bool, whether to concatenate the output of the attention heads instead of averaging; dropout_rate : internal dropout rate for attention coefficients; return_attn_coef : if True, return the attention coefficients for the given input (one N x N matrix for each head). activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; attn_kernel_initializer : initializer for the attention weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; attn_kernel_regularizer : regularization applied to the attention kernels; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; attn_kernel_constraint : constraint applied to the attention kernels; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphAttention"},{"location":"layers/convolution/#graphconvskip","text":"spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A simple convolutional layer with a skip connection. Mode : single, disjoint, mixed, batch. This layer computes: \\Z = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b where \\A does not have self-loops (unlike in GraphConv). Input Node features of shape ([batch], N, F) ; Normalized adjacency matrix of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.normalized_adjacency . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphConvSkip"},{"location":"layers/convolution/#appnp","text":"spektral.layers.APPNP(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . This layer computes: \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\ \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} + \\alpha \\Z^{(0)}, where \\alpha is the teleport probability and \\textrm{MLP} is a multi-layer perceptron. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Modified Laplacian of shape ([batch], N, N) ; can be computed with spektral.utils.convolution.localpooling_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; alpha : teleport probability during propagation; propagations : number of propagation steps; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"APPNP"},{"location":"layers/convolution/#ginconv","text":"spektral.layers.GINConv(channels, epsilon=None, mlp_hidden=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) as presented by Xu et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\Z_i = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_j \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; epsilon : unnamed parameter, see Xu et al. (2018) , and the equation above. By setting epsilon=None , the parameter will be learned (default behaviour). If given as a value, the parameter will stay fixed. mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GINConv"},{"location":"layers/convolution/#diffusionconv","text":"spektral.layers.DiffusionConv(channels, num_diffusion_steps=6, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None, activation='tanh') Applies Graph Diffusion Convolution as descibed by Li et al. (2016) Mode : single, disjoint, mixed, batch. This layer expects a dense adjacency matrix. Given a number of diffusion steps K and a row normalized adjacency matrix \\hat \\A , this layer calculates the q'th channel as: \\mathbf{H}_{~:,~q} = \\sigma\\left( \\sum_{f=1}^{F} \\left( \\sum_{k=0}^{K-1}\\theta_k {\\hat \\A}^k \\right) \\X_{~:,~f} \\right) Input Node features of shape ([batch], N, F) ; Normalized adjacency or attention coef. matrix \\hat \\A of shape ([batch], N, N) ; Use DiffusionConvolution.preprocess to normalize. Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; num_diffusion_steps : How many diffusion steps to consider. K in paper. activation : activation function \\sigma ; ( \\tanh by default) kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"DiffusionConv"},{"location":"layers/convolution/#gatedgraphconv","text":"spektral.layers.GatedGraphConv(channels, n_layers, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated graph convolutional layer as presented by Li et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer repeatedly applies a GRU cell L times to the node attributes \\begin{align} & \\h^{(0)}_i = \\X_i \\| \\mathbf{0} \\\\ & \\m^{(l)}_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\h^{(l - 1)}_j \\W \\\\ & \\h^{(l)}_i = \\textrm{GRU} \\left(\\m^{(l)}_i, \\h^{(l - 1)}_i \\right) \\\\ & \\Z_i = h^{(L)}_i \\end{align} where \\textrm{GRU} is the GRU cell. Input Node features of shape (N, F) ; note that F must be smaller or equal than channels . Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; n_layers : integer, number of iterations with the GRU cell; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GatedGraphConv"},{"location":"layers/convolution/#agnnconv","text":"spektral.layers.AGNNConv(trainable=True, activation=None) An Attention-based Graph Neural Network (AGNN) as presented by Thekumparampil et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes: \\Z = \\P\\X where \\P_{ij} = \\frac{ \\exp \\left( \\beta \\cos \\left( \\X_i, \\X_j \\right) \\right) }{ \\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp \\left( \\beta \\cos \\left( \\X_i, \\X_k \\right) \\right) } and \\beta is a trainable parameter. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input. Arguments trainable : boolean, if True, then beta is a trainable parameter. Otherwise, beta is fixed to 1; activation : activation function to use; [source]","title":"AGNNConv"},{"location":"layers/convolution/#tagconv","text":"spektral.layers.TAGConv(channels, K=3, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Topology Adaptive Graph Convolutional layer (TAG) as presented by Du et al. (2017) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes: \\Z = \\sum\\limits_{k=0}^{K} \\D^{-1/2}\\A^k\\D^{-1/2}\\X\\W^{(k)} Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; K : the order of the layer (i.e., the layer will consider a K-hop neighbourhood for each node); activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"TAGConv"},{"location":"layers/convolution/#crystalconv","text":"spektral.layers.CrystalConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Crystal Graph Convolutional layer as presented by Xie & Grossman (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\H_i = \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\sigma \\left( \\z_{ij} \\W^{(f)} + \\b^{(f)} \\right) \\odot \\g \\left( \\z_{ij} \\W^{(s)} + \\b^{(s)} \\right) where \\z_{ij} = \\X_i \\| \\X_j \\| \\E_{ij} , \\sigma is a sigmoid activation, and g is the activation function (defined by the activation argument). Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Edge features of shape (num_edges, S) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"CrystalConv"},{"location":"layers/convolution/#edgeconv","text":"spektral.layers.EdgeConv(channels, mlp_hidden=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An Edge Convolutional layer as presented by Wang et al. (2018) . Mode : single, disjoint. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\Z_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}\\big( \\X_i \\| \\X_j - \\X_i \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function to use; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"EdgeConv"},{"location":"layers/convolution/#messagepassing","text":"spektral.layers.MessagePassing(aggregate='sum') A general class for message passing as presented by Gilmer et al. (2017) . Mode : single, disjoint. This layer and all of its extensions expect a sparse adjacency matrix. This layer computes: \\Z_i = \\gamma \\left( \\X_i, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi \\left(\\X_i, \\X_j, \\E_{j,i} \\right) \\right), where \\gamma is a differentiable update function, \\phi is a differentiable message function, \\square is a permutation-invariant function to aggregate the messages (like the sum or the average), and \\E_{ij} is the edge attribute of edge i-j. By extending this class, it is possible to create any message-passing layer in single/disjoint mode. API: propagate(X, A, E=None, **kwargs) : propagate the messages and computes embeddings for each node in the graph. kwargs will be propagated as keyword arguments to message() , aggregate() and update() . message(X, **kwargs) : computes messages, equivalent to \\phi in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. Use self.get_i() and self.get_j() to gather the elements using the indices i or j of the adjacency matrix (e.g, self.get_j(X) will get the features of the neighbours). aggregate(messages, **kwargs) : aggregates the messages, equivalent to \\square in the definition. The behaviour of this function can also be controlled using the aggregate keyword in the constructor of the layer (supported aggregations: sum, mean, max, min, prod). Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. update(embeddings, **kwargs) : updates the aggregated messages to obtain the final node embeddings, equivalent to \\gamma in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. Arguments : aggregate : string or callable, an aggregate function. This flag can be used to control the behaviour of aggregate() wihtout re-implementing it. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. If callable, the function must have the signature foo(updates, indices, N) and return a rank 2 tensor with shape (N, ...) .","title":"MessagePassing"},{"location":"layers/pooling/","text":"Pooling layers The pooling layers from these papers are available in Spektral: Hierarchical Graph Representation Learning with Differentiable Pooling Mincut pooling in Graph Neural Networks Graph U-Nets Self-Attention Graph Pooling Gated Graph Sequence Neural Networks Additionally, sum, average, and max global pooling are implemented, as well as a simple global weighted sum pooling where weights are calculated with an attention mechanism. See the convolutional layers page for the notation. [source] DiffPool spektral.layers.DiffPool(k, channels=None, return_mask=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A DiffPool layer as presented by Ying et al. (2018) . Mode : batch. This layer computes a soft clustering \\S of the input graphs using a GNN, and reduces graphs as follows: \\S = \\textrm{GNN}(\\A, \\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where GNN consists of one GraphConv layer with softmax activation. Two auxiliary loss terms are also added to the model: the link prediction loss \\big\\| \\A - \\S\\S^\\top \\big\\|_F and the entropy loss - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S). The layer also applies a 1-layer GCN to the input features, and returns the updated graph signal (the number of output channels is controlled by the channels parameter). The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, channels) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; channels : number of output channels (if None, the number of output channels is assumed to be the same as the input); return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] MinCutPool spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_mask=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A minCUT pooling layer as presented by Bianchi et al. (2019) . Mode : batch. This layer computes a soft clustering \\S of the input graphs using a MLP, and reduces graphs as follows: \\S = \\textrm{MLP}(\\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where MLP is a multi-layer perceptron with softmax output. Two auxiliary loss terms are also added to the model: the minCUT loss - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) } and the orthogonality loss \\left\\| \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F} - \\frac{\\I_K}{\\sqrt{K}} \\right\\|_F. The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, F) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP used to compute cluster assignments (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] TopKPool spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A gPool/Top-K layer as presented by Gao & Ji (2019) and Cangea et al. (2018) . Mode : single, disjoint. This layer computes the following operations: \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\p is a learnable parameter vector of size F . K is defined for each graph as a fraction of the number of nodes. Note that the the gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). This layer temporarily makes the adjacency matrix dense in order to compute \\A' . If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] SAGPool spektral.layers.SAGPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A self-attention graph pooling layer as presented by Lee et al. (2019) . Mode : single, disjoint. This layer computes the following operations: \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\textrm{GNN} consists of one GraphConv layer with no activation. K is defined for each graph as a fraction of the number of nodes. This layer temporarily makes the adjacency matrix dense in order to compute \\A' . If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] GlobalSumPool spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments None. [source] GlobalAvgPool spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments None. [source] GlobalMaxPool spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments None. [source] GlobalAttentionPool spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer as presented by Li et al. (2017) . This layer computes: \\X' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i where \\sigma is the sigmoid activation function. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, channels) (if single mode, shape will be (1, channels) ). Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vectors; kernel_regularizer : regularization applied to the kernel matrices; bias_regularizer : regularization applied to the bias vectors; kernel_constraint : constraint applied to the kernel matrices; bias_constraint : constraint applied to the bias vectors. [source] GlobalAttnSumPool spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. This layer computes: \\alpha = \\textrm{softmax}( \\X \\a); \\\\ \\X' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i where \\a \\in \\mathbb{R}^F is a trainable vector. Note that the softmax is applied across nodes, and not across features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments attn_kernel_initializer : initializer for the attention weights; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix; [source] SortPool spektral.layers.SortPool(k) A SortPool layer as described by Zhang et al . This layers takes a graph signal \\mathbf{X} and returns the topmost k rows according to the last column. If \\mathbf{X} has less than k rows, the result is zero-padded to k. Mode : single, disjoint, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, k, F) (if single mode, shape will be (1, k, F) ). Arguments k : integer, number of nodes to keep;","title":"Pooling Layers"},{"location":"layers/pooling/#pooling-layers","text":"The pooling layers from these papers are available in Spektral: Hierarchical Graph Representation Learning with Differentiable Pooling Mincut pooling in Graph Neural Networks Graph U-Nets Self-Attention Graph Pooling Gated Graph Sequence Neural Networks Additionally, sum, average, and max global pooling are implemented, as well as a simple global weighted sum pooling where weights are calculated with an attention mechanism. See the convolutional layers page for the notation. [source]","title":"Pooling layers"},{"location":"layers/pooling/#diffpool","text":"spektral.layers.DiffPool(k, channels=None, return_mask=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A DiffPool layer as presented by Ying et al. (2018) . Mode : batch. This layer computes a soft clustering \\S of the input graphs using a GNN, and reduces graphs as follows: \\S = \\textrm{GNN}(\\A, \\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where GNN consists of one GraphConv layer with softmax activation. Two auxiliary loss terms are also added to the model: the link prediction loss \\big\\| \\A - \\S\\S^\\top \\big\\|_F and the entropy loss - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S). The layer also applies a 1-layer GCN to the input features, and returns the updated graph signal (the number of output channels is controlled by the channels parameter). The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, channels) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; channels : number of output channels (if None, the number of output channels is assumed to be the same as the input); return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"DiffPool"},{"location":"layers/pooling/#mincutpool","text":"spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_mask=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A minCUT pooling layer as presented by Bianchi et al. (2019) . Mode : batch. This layer computes a soft clustering \\S of the input graphs using a MLP, and reduces graphs as follows: \\S = \\textrm{MLP}(\\X); \\\\ \\A' = \\S^\\top \\A \\S; \\X' = \\S^\\top \\X; where MLP is a multi-layer perceptron with softmax output. Two auxiliary loss terms are also added to the model: the minCUT loss - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) } and the orthogonality loss \\left\\| \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F} - \\frac{\\I_K}{\\sqrt{K}} \\right\\|_F. The layer can be used without a supervised loss, to compute node clustering simply by minimizing the two auxiliary losses. Input Node features of shape ([batch], N, F) ; Binary adjacency matrix of shape ([batch], N, N) ; Output Reduced node features of shape ([batch], K, F) ; Reduced adjacency matrix of shape ([batch], K, K) ; If return_mask=True , the soft clustering matrix of shape ([batch], N, K) . Arguments k : number of nodes to keep; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP used to compute cluster assignments (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; return_mask : boolean, whether to return the cluster assignment matrix; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"MinCutPool"},{"location":"layers/pooling/#topkpool","text":"spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A gPool/Top-K layer as presented by Gao & Ji (2019) and Cangea et al. (2018) . Mode : single, disjoint. This layer computes the following operations: \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\p is a learnable parameter vector of size F . K is defined for each graph as a fraction of the number of nodes. Note that the the gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). This layer temporarily makes the adjacency matrix dense in order to compute \\A' . If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"TopKPool"},{"location":"layers/pooling/#sagpool","text":"spektral.layers.SAGPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A self-attention graph pooling layer as presented by Lee et al. (2019) . Mode : single, disjoint. This layer computes the following operations: \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\textrm{GNN} consists of one GraphConv layer with no activation. K is defined for each graph as a fraction of the number of nodes. This layer temporarily makes the adjacency matrix dense in order to compute \\A' . If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from sparse to dense and back to sparse is an expensive operation. Input Node features of shape (N, F) ; Binary adjacency matrix of shape (N, N) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Reduced node features of shape (ratio * N, F) ; Reduced adjacency matrix of shape (ratio * N, ratio * N) ; Reduced graph IDs of shape (ratio * N, ) (only in disjoint mode); If return_mask=True , the binary pooling mask of shape (ratio * N, ) . Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"SAGPool"},{"location":"layers/pooling/#globalsumpool","text":"spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments None. [source]","title":"GlobalSumPool"},{"location":"layers/pooling/#globalavgpool","text":"spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments None. [source]","title":"GlobalAvgPool"},{"location":"layers/pooling/#globalmaxpool","text":"spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments None. [source]","title":"GlobalMaxPool"},{"location":"layers/pooling/#globalattentionpool","text":"spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer as presented by Li et al. (2017) . This layer computes: \\X' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i where \\sigma is the sigmoid activation function. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, channels) (if single mode, shape will be (1, channels) ). Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vectors; kernel_regularizer : regularization applied to the kernel matrices; bias_regularizer : regularization applied to the bias vectors; kernel_constraint : constraint applied to the kernel matrices; bias_constraint : constraint applied to the bias vectors. [source]","title":"GlobalAttentionPool"},{"location":"layers/pooling/#globalattnsumpool","text":"spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. This layer computes: \\alpha = \\textrm{softmax}( \\X \\a); \\\\ \\X' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i where \\a \\in \\mathbb{R}^F is a trainable vector. Note that the softmax is applied across nodes, and not across features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, F) (if single mode, shape will be (1, F) ). Arguments attn_kernel_initializer : initializer for the attention weights; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix; [source]","title":"GlobalAttnSumPool"},{"location":"layers/pooling/#sortpool","text":"spektral.layers.SortPool(k) A SortPool layer as described by Zhang et al . This layers takes a graph signal \\mathbf{X} and returns the topmost k rows according to the last column. If \\mathbf{X} has less than k rows, the result is zero-padded to k. Mode : single, disjoint, batch. Input Node features of shape ([batch], N, F) ; Graph IDs of shape (N, ) (only in disjoint mode); Output Pooled node features of shape (batch, k, F) (if single mode, shape will be (1, k, F) ). Arguments k : integer, number of nodes to keep;","title":"SortPool"},{"location":"utils/conversion/","text":"Conversion nx_to_adj spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices. nx_to_node_features spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_edge_features spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_numpy spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) numpy_to_nx spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"Conversion"},{"location":"utils/conversion/#conversion","text":"","title":"Conversion"},{"location":"utils/conversion/#nx_to_adj","text":"spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices.","title":"nx_to_adj"},{"location":"utils/conversion/#nx_to_node_features","text":"spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_node_features"},{"location":"utils/conversion/#nx_to_edge_features","text":"spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_edge_features"},{"location":"utils/conversion/#nx_to_numpy","text":"spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim)","title":"nx_to_numpy"},{"location":"utils/conversion/#numpy_to_nx","text":"spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"numpy_to_nx"},{"location":"utils/convolution/","text":"Convolution degree_matrix spektral.utils.degree_matrix(A) Computes the degree matrix of the given adjacency matrix. Arguments A : rank 2 array or sparse matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format. degree_power spektral.utils.degree_power(A, k) Computes \\D^{k} from the given adjacency matrix. Useful for computing normalised Laplacian. Arguments A : rank 2 array or sparse matrix. k : exponent to which elevate the degree matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format. normalized_adjacency spektral.utils.normalized_adjacency(A, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either \\D^{-1}\\A or \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix. laplacian spektral.utils.laplacian(A) Computes the Laplacian of the given adjacency matrix as \\D - \\A . Arguments A : rank 2 array or sparse matrix; Return The Laplacian. normalized_laplacian spektral.utils.normalized_laplacian(A, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as \\I - \\D^{-1}\\A or \\I - \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian. rescale_laplacian spektral.utils.rescale_laplacian(L, lmax=None) Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue. Arguments L : rank 2 array or sparse matrix; lmax : if None, compute largest eigenvalue with scipy.linalg.eisgh. If the eigendecomposition fails, lmax is set to 2 automatically. If scalar, use this value as largest eignevalue when rescaling. Return localpooling_filter spektral.utils.localpooling_filter(A, symmetric=True) Computes the graph filter described in Kipf & Welling (2017) . Arguments A : array or sparse matrix with rank 2 or 3; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return Array or sparse matrix with rank 2 or 3, same as A; chebyshev_polynomial spektral.utils.chebyshev_polynomial(X, k) Calculates Chebyshev polynomials of X, up to order k. Arguments X : rank 2 array or sparse matrix; k : the order up to which compute the polynomials, Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial. chebyshev_filter spektral.utils.chebyshev_filter(A, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016) . Arguments A : rank 2 array or sparse matrix; k : integer, the order of the Chebyshev polynomial; symmetric : boolean, whether to normalize the adjacency matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"Convolution"},{"location":"utils/convolution/#convolution","text":"","title":"Convolution"},{"location":"utils/convolution/#degree_matrix","text":"spektral.utils.degree_matrix(A) Computes the degree matrix of the given adjacency matrix. Arguments A : rank 2 array or sparse matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format.","title":"degree_matrix"},{"location":"utils/convolution/#degree_power","text":"spektral.utils.degree_power(A, k) Computes \\D^{k} from the given adjacency matrix. Useful for computing normalised Laplacian. Arguments A : rank 2 array or sparse matrix. k : exponent to which elevate the degree matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format.","title":"degree_power"},{"location":"utils/convolution/#normalized_adjacency","text":"spektral.utils.normalized_adjacency(A, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either \\D^{-1}\\A or \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix.","title":"normalized_adjacency"},{"location":"utils/convolution/#laplacian","text":"spektral.utils.laplacian(A) Computes the Laplacian of the given adjacency matrix as \\D - \\A . Arguments A : rank 2 array or sparse matrix; Return The Laplacian.","title":"laplacian"},{"location":"utils/convolution/#normalized_laplacian","text":"spektral.utils.normalized_laplacian(A, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as \\I - \\D^{-1}\\A or \\I - \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian.","title":"normalized_laplacian"},{"location":"utils/convolution/#rescale_laplacian","text":"spektral.utils.rescale_laplacian(L, lmax=None) Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue. Arguments L : rank 2 array or sparse matrix; lmax : if None, compute largest eigenvalue with scipy.linalg.eisgh. If the eigendecomposition fails, lmax is set to 2 automatically. If scalar, use this value as largest eignevalue when rescaling. Return","title":"rescale_laplacian"},{"location":"utils/convolution/#localpooling_filter","text":"spektral.utils.localpooling_filter(A, symmetric=True) Computes the graph filter described in Kipf & Welling (2017) . Arguments A : array or sparse matrix with rank 2 or 3; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return Array or sparse matrix with rank 2 or 3, same as A;","title":"localpooling_filter"},{"location":"utils/convolution/#chebyshev_polynomial","text":"spektral.utils.chebyshev_polynomial(X, k) Calculates Chebyshev polynomials of X, up to order k. Arguments X : rank 2 array or sparse matrix; k : the order up to which compute the polynomials, Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"chebyshev_polynomial"},{"location":"utils/convolution/#chebyshev_filter","text":"spektral.utils.chebyshev_filter(A, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016) . Arguments A : rank 2 array or sparse matrix; k : integer, the order of the Chebyshev polynomial; symmetric : boolean, whether to normalize the adjacency matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"chebyshev_filter"},{"location":"utils/data/","text":"Data numpy_to_disjoint spektral.utils.numpy_to_disjoint(X_list, A_list, E_list=None) Converts a batch of graphs stored in lists (X, A, and optionally E) to the disjoint mode . Each entry i of the lists should be associated to the same graph, i.e., X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0] . The method also computes the batch index I . Arguments X_list : a list of np.arrays of shape (N, F) ; A_list : a list of np.arrays or sparse matrices of shape (N, N) ; E_list : a list of np.arrays of shape (N, N, S) ; Return X_out : a rank 2 array of shape (n_nodes, F) ; A_out : a rank 2 array of shape (n_nodes, n_nodes) ; E_out : (only if E_list is given) a rank 2 array of shape (n_edges, S) ; I_out : a rank 1 array of shape (n_nodes, ) ; numpy_to_batch spektral.utils.numpy_to_batch(X_list, A_list, E_list=None) Converts a batch of graphs stored in lists (X, A, and optionally E) to the batch mode by zero-padding all X, A and E matrices to have the same node dimensions ( N_max ). Each entry i of the lists should be associated to the same graph, i.e., X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0] . Note that if A_list contains sparse matrices, they will be converted to dense np.arrays, which can be expensice. Arguments X_list : a list of np.arrays of shape (N, F) ; A_list : a list of np.arrays or sparse matrices of shape (N, N) ; E_list : a list of np.arrays of shape (N, N, S) ; Return X_out : a rank 3 array of shape (batch, N_max, F) ; A_out : a rank 2 array of shape (batch, N_max, N_max) ; E_out : (only if E_list if given) a rank 2 array of shape (batch, N_max, N_max, S) ; batch_iterator spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with the same first dimension; batch_size : number of samples in a batch; epochs : number of times to iterate over the data; shuffle : whether to shuffle the data at the beginning of each epoch Return Batches of size batch_size .","title":"Data utils"},{"location":"utils/data/#data","text":"","title":"Data"},{"location":"utils/data/#numpy_to_disjoint","text":"spektral.utils.numpy_to_disjoint(X_list, A_list, E_list=None) Converts a batch of graphs stored in lists (X, A, and optionally E) to the disjoint mode . Each entry i of the lists should be associated to the same graph, i.e., X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0] . The method also computes the batch index I . Arguments X_list : a list of np.arrays of shape (N, F) ; A_list : a list of np.arrays or sparse matrices of shape (N, N) ; E_list : a list of np.arrays of shape (N, N, S) ; Return X_out : a rank 2 array of shape (n_nodes, F) ; A_out : a rank 2 array of shape (n_nodes, n_nodes) ; E_out : (only if E_list is given) a rank 2 array of shape (n_edges, S) ; I_out : a rank 1 array of shape (n_nodes, ) ;","title":"numpy_to_disjoint"},{"location":"utils/data/#numpy_to_batch","text":"spektral.utils.numpy_to_batch(X_list, A_list, E_list=None) Converts a batch of graphs stored in lists (X, A, and optionally E) to the batch mode by zero-padding all X, A and E matrices to have the same node dimensions ( N_max ). Each entry i of the lists should be associated to the same graph, i.e., X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0] . Note that if A_list contains sparse matrices, they will be converted to dense np.arrays, which can be expensice. Arguments X_list : a list of np.arrays of shape (N, F) ; A_list : a list of np.arrays or sparse matrices of shape (N, N) ; E_list : a list of np.arrays of shape (N, N, S) ; Return X_out : a rank 3 array of shape (batch, N_max, F) ; A_out : a rank 2 array of shape (batch, N_max, N_max) ; E_out : (only if E_list if given) a rank 2 array of shape (batch, N_max, N_max, S) ;","title":"numpy_to_batch"},{"location":"utils/data/#batch_iterator","text":"spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with the same first dimension; batch_size : number of samples in a batch; epochs : number of times to iterate over the data; shuffle : whether to shuffle the data at the beginning of each epoch Return Batches of size batch_size .","title":"batch_iterator"},{"location":"utils/misc/","text":"Miscellaneous pad_jagged_array spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a list or np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape add_eye spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix sub_eye spektral.utils.sub_eye(x) Subtracts the identity matrix from the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix add_eye_batch spektral.utils.add_eye_batch(x) Adds the identity matrix to each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array sub_eye_batch spektral.utils.sub_eye_batch(x) Subtracts the identity matrix from each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array add_eye_jagged spektral.utils.add_eye_jagged(x) Adds the identity matrix to each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array sub_eye_jagged spektral.utils.sub_eye_jagged(x) Subtracts the identity matrix from each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array","title":"Miscellaneous"},{"location":"utils/misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"utils/misc/#pad_jagged_array","text":"spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a list or np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape","title":"pad_jagged_array"},{"location":"utils/misc/#add_eye","text":"spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix","title":"add_eye"},{"location":"utils/misc/#sub_eye","text":"spektral.utils.sub_eye(x) Subtracts the identity matrix from the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array or scipy.sparse matrix","title":"sub_eye"},{"location":"utils/misc/#add_eye_batch","text":"spektral.utils.add_eye_batch(x) Adds the identity matrix to each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array","title":"add_eye_batch"},{"location":"utils/misc/#sub_eye_batch","text":"spektral.utils.sub_eye_batch(x) Subtracts the identity matrix from each submatrix of the given rank 3 array. Arguments x : a rank 3 np.array Return A rank 3 np.array","title":"sub_eye_batch"},{"location":"utils/misc/#add_eye_jagged","text":"spektral.utils.add_eye_jagged(x) Adds the identity matrix to each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array","title":"add_eye_jagged"},{"location":"utils/misc/#sub_eye_jagged","text":"spektral.utils.sub_eye_jagged(x) Subtracts the identity matrix from each submatrix of the given rank 3 jagged array. Arguments x : a rank 3 jagged np.array Return A rank 3 jagged np.array","title":"sub_eye_jagged"}]}