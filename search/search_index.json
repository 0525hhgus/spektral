{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Spektral Spektral is a Python library for graph deep learning, based on the Keras API. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the nodes of a network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph convolutional networks (GCN) Chebyshev networks (ChebNets) GraphSage Edge-conditioned convolutions (ECC) Graph attention networks (GAT) ARMA convolutions Approximated personalized propagation of neural predictions (APPNP) Graph isomorphism networks (GIN) You can also find pooling layers (including global readouts and graph coarsening layers), and lots of utilities to apply graph deep learning in your projects. See how to get started with Spektral and have a look at the examples for some project templates. The source code of the project is available on Github . Read the documentation here . Installation Spektral is compatible with Python 3.5+, and is tested on Ubuntu 16.04 and 18.04. Other Linux distros and MacOS should work as well, but Windows is not supported for now. To install the required dependencies on Ubuntu run: $ sudo apt install graphviz libgraphviz-dev libcgraph6 Some features of Spektral also require the following optional dependencies: RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda); dyfunconn , a library to build functional connectivity networks (available through PyPi); The simplest way to install Spektral is from PyPi: $ pip install spektral To install Spektral from source, run this in a terminal: $ git clone https://github.com/danielegrattarola/spektral.git $ cd spektral $ python setup.py install # Or 'pip install .' Note that the setup.py script will not attempt to install a backend for Keras, in order to not mess up any previous installation. It will, however, install Keras and its dependencies via PyPi (which may include the CPU version of TensorFlow). If you are already a Keras user, this should not impact you. If you're just getting started, then you may want to install the GPU version of Tensorflow before installing Spektral. Also note that some features of Spektral may depend explicitly on TensorFlow, although this dependency will be kept to a minimum. Contributing WARNING Spektral is still a work in progress and may change significantly before the first release. The API is not mature enough to be considered stable, but we'll try to keep breaking changes to a minimum. Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework.","title":"Home"},{"location":"#welcome-to-spektral","text":"Spektral is a Python library for graph deep learning, based on the Keras API. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the nodes of a network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph convolutional networks (GCN) Chebyshev networks (ChebNets) GraphSage Edge-conditioned convolutions (ECC) Graph attention networks (GAT) ARMA convolutions Approximated personalized propagation of neural predictions (APPNP) Graph isomorphism networks (GIN) You can also find pooling layers (including global readouts and graph coarsening layers), and lots of utilities to apply graph deep learning in your projects. See how to get started with Spektral and have a look at the examples for some project templates. The source code of the project is available on Github . Read the documentation here .","title":"Welcome to Spektral"},{"location":"#installation","text":"Spektral is compatible with Python 3.5+, and is tested on Ubuntu 16.04 and 18.04. Other Linux distros and MacOS should work as well, but Windows is not supported for now. To install the required dependencies on Ubuntu run: $ sudo apt install graphviz libgraphviz-dev libcgraph6 Some features of Spektral also require the following optional dependencies: RDKit , a library for cheminformatics and molecule manipulation (available through Anaconda); dyfunconn , a library to build functional connectivity networks (available through PyPi); The simplest way to install Spektral is from PyPi: $ pip install spektral To install Spektral from source, run this in a terminal: $ git clone https://github.com/danielegrattarola/spektral.git $ cd spektral $ python setup.py install # Or 'pip install .' Note that the setup.py script will not attempt to install a backend for Keras, in order to not mess up any previous installation. It will, however, install Keras and its dependencies via PyPi (which may include the CPU version of TensorFlow). If you are already a Keras user, this should not impact you. If you're just getting started, then you may want to install the GPU version of Tensorflow before installing Spektral. Also note that some features of Spektral may depend explicitly on TensorFlow, although this dependency will be kept to a minimum.","title":"Installation"},{"location":"#contributing","text":"WARNING Spektral is still a work in progress and may change significantly before the first release. The API is not mature enough to be considered stable, but we'll try to keep breaking changes to a minimum. Spektral is an open source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework.","title":"Contributing"},{"location":"about/","text":"About Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"about/#about","text":"Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"brain/","text":"This module provides some functions to create functional connectivity networks, and requires the dyfunconn library to be installed on the system. get_fc spektral.brain.get_fc(x, band_freq, sampling_freq, samples_per_graph=None, fc_measure='corr', link_cutoff=0.0, percentiles=None, band_freq_hi=(20.0, 45.0), nfft=128, n_overlap=64, nf_mode='mean', self_loops=True, njobs=1) Build functional connectivity networks from the given data stream. Arguments x : numpy array of shape (n_channels, n_samples); band_freq : list with two elements, the band in which to estimate FC; sampling_freq : float, sampling frequency of the stream; samples_per_graph : number of samples to use to generate a graph. By default, the whole stream is used. If provided, 1 + (n_samples / samples_per_graph) will be generated; fc_measure : functional connectivity measure to use. Possible measures are: iplv, icoh, corr, aec, wpli, dwpli, dpli (see documentation of Dyfunconn); link_cutoff : links with absolute FC measure below this value will be removed; percentiles : tuple of two numbers >0 and <100; links with FC measure between the two percentiles will be removed (statistics are calculated for each edge). Note that this option ignores link_cutoff . band_freq_hi : high band used to estimate FC when using 'aec'; nfft : TODO, affects 'wpli' and 'dwpli'; n_overlap : TODO, affects 'wpli' and 'dwpli'; param nf_mode: how to compute node features. Possible modes are: full, mean, energy, ones. self_loops : add self loops to FC networks; njobs : number of processes to use (-1 to use all available cores); Return FC graph(s) in numpy format (note that node features are all ones).","title":"Brain"},{"location":"brain/#get_fc","text":"spektral.brain.get_fc(x, band_freq, sampling_freq, samples_per_graph=None, fc_measure='corr', link_cutoff=0.0, percentiles=None, band_freq_hi=(20.0, 45.0), nfft=128, n_overlap=64, nf_mode='mean', self_loops=True, njobs=1) Build functional connectivity networks from the given data stream. Arguments x : numpy array of shape (n_channels, n_samples); band_freq : list with two elements, the band in which to estimate FC; sampling_freq : float, sampling frequency of the stream; samples_per_graph : number of samples to use to generate a graph. By default, the whole stream is used. If provided, 1 + (n_samples / samples_per_graph) will be generated; fc_measure : functional connectivity measure to use. Possible measures are: iplv, icoh, corr, aec, wpli, dwpli, dpli (see documentation of Dyfunconn); link_cutoff : links with absolute FC measure below this value will be removed; percentiles : tuple of two numbers >0 and <100; links with FC measure between the two percentiles will be removed (statistics are calculated for each edge). Note that this option ignores link_cutoff . band_freq_hi : high band used to estimate FC when using 'aec'; nfft : TODO, affects 'wpli' and 'dwpli'; n_overlap : TODO, affects 'wpli' and 'dwpli'; param nf_mode: how to compute node features. Possible modes are: full, mean, energy, ones. self_loops : add self loops to FC networks; njobs : number of processes to use (-1 to use all available cores); Return FC graph(s) in numpy format (note that node features are all ones).","title":"get_fc"},{"location":"chem/","text":"This module provides some functions to work with molecules, and requires the RDKit library to be installed on the system. numpy_to_rdkit spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule numpy_to_smiles spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule rdkit_to_smiles spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule sdf_to_nx spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs. nx_to_sdf spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format. validate_rdkit spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise get_atomic_symbol spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol get_atomic_num spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118 valid_score spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule novel_score spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule unique_score spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules enable_rdkit_log spektral.chem.enable_rdkit_log() Enables RDkit logging. Return plot_rdkit spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return plot_rdkit_svg_grid spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"Chemistry"},{"location":"chem/#numpy_to_rdkit","text":"spektral.chem.numpy_to_rdkit(adj, nf, ef, sanitize=False) Converts a molecule from numpy to RDKit format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) sanitize : whether to sanitize the molecule after conversion Return An RDKit molecule","title":"numpy_to_rdkit"},{"location":"chem/#numpy_to_smiles","text":"spektral.chem.numpy_to_smiles(adj, nf, ef) Converts a molecule from numpy to SMILES format. Arguments adj : binary numpy array of shape (N, N) nf : numpy array of shape (N, F) ef : numpy array of shape (N, N, S) Return The SMILES string of the molecule","title":"numpy_to_smiles"},{"location":"chem/#rdkit_to_smiles","text":"spektral.chem.rdkit_to_smiles(mol) Returns the SMILES string representing an RDKit molecule. Arguments mol : an RDKit molecule Return The SMILES string of the molecule","title":"rdkit_to_smiles"},{"location":"chem/#sdf_to_nx","text":"spektral.chem.sdf_to_nx(sdf, keep_hydrogen=False) Converts molecules in SDF format to networkx Graphs. Arguments sdf : a list of molecules (or individual molecule) in SDF format. keep_hydrogen : whether to include hydrogen in the representation. Return List of nx.Graphs.","title":"sdf_to_nx"},{"location":"chem/#nx_to_sdf","text":"spektral.chem.nx_to_sdf(graphs) Converts a list of nx.Graphs to the internal SDF format. Arguments graphs : list of nx.Graphs. Return List of molecules in the internal SDF format.","title":"nx_to_sdf"},{"location":"chem/#validate_rdkit","text":"spektral.chem.validate_rdkit(mol) Validates RDKit molecules (single or in a list). Arguments mol : an RDKit molecule or list/np.array thereof Return Boolean array, True if the molecules are chemically valid, False otherwise","title":"validate_rdkit"},{"location":"chem/#get_atomic_symbol","text":"spektral.chem.get_atomic_symbol(number) Given an atomic number (e.g., 6), returns its atomic symbol (e.g., 'C') Arguments number : int <= 118 Return String, atomic symbol","title":"get_atomic_symbol"},{"location":"chem/#get_atomic_num","text":"spektral.chem.get_atomic_num(symbol) Given an atomic symbol (e.g., 'C'), returns its atomic number (e.g., 6) Arguments symbol : string, atomic symbol Return Int <= 118","title":"get_atomic_num"},{"location":"chem/#valid_score","text":"spektral.chem.valid_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing the validity of each molecule. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Boolean array with the validity for each molecule","title":"valid_score"},{"location":"chem/#novel_score","text":"spektral.chem.novel_score(molecules, smiles, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns a boolean array representing valid and novel molecules with respect to the list of smiles provided (a molecule is novel if its SMILES is not in the list). Arguments molecules : list of molecules (RDKit or numpy format) smiles : list or set of smiles strings against which to check for novelty from_numpy : whether the molecules are in numpy format Return Boolean array with the novelty for each valid molecule","title":"novel_score"},{"location":"chem/#unique_score","text":"spektral.chem.unique_score(molecules, from_numpy=False) For a given list of molecules (RDKit or numpy format), returns the fraction of unique and valid molecules w.r.t. to the number of valid molecules. Arguments molecules : list of molecules (RDKit or numpy format) from_numpy : whether the molecules are in numpy format Return Fraction of unique valid molecules w.r.t. to valid molecules","title":"unique_score"},{"location":"chem/#enable_rdkit_log","text":"spektral.chem.enable_rdkit_log() Enables RDkit logging. Return","title":"enable_rdkit_log"},{"location":"chem/#plot_rdkit","text":"spektral.chem.plot_rdkit(mol, filename=None) Plots an RDKit molecule in Matplotlib Arguments mol : an RDKit molecule filename : save the image with the given filename Return","title":"plot_rdkit"},{"location":"chem/#plot_rdkit_svg_grid","text":"spektral.chem.plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None) Plots a grid of RDKit molecules in SVG. Arguments mols : a list of RDKit molecules mols_per_row : size of the grid filename : save an image with the given filename kwargs : additional arguments for RDKit.Chem.Draw.MolsToGridImage Return The SVG as a string","title":"plot_rdkit_svg_grid"},{"location":"data/","text":"Representing graphs Spektral uses a matrix-based representation for manipulating graphs and feeding them to neural networks. This approach is one of the most commonly used in the literature on graph neural networks, and it's perfect to perform parallel computations on GPU. WARNING Support for edge attributes is not fully implemented in Spektral. A graph is generally represented by three matrices: A \\in \\mathbb{R}^{N \\times N} , a square adjacency matrix where A_{ij} > 0 if there is a connection between nodes i and j , and A_{ij} = 0 otherwise; X \\in \\mathbb{R}^{N \\times F} , a matrix encoding node attributes, where each row represents the F -dimensional attribute vector of a node; E \\in \\mathbb{R}^{N \\times N \\times S} , a matrix encoding edge attributes, where each entry represents the S -dimensional attribute vector of an edge; Some frameworks (like the graph networks proposed by Battaglia et al.) also include a feature vector describing the global state of the graph, but this is not supported by Spektral for now. In code, and in this documentation, we use the following convention to refer to the formulation above: A is the adjacency matrix, N is the number of nodes; X is the node attributes matrix, F is the size of the edge attributes; E is the edge attributes matrix, S is the size of the edge attributes; See the table below for how these matrices are represented in Numpy. Modes In Spektral, some functionalities are implemented to work on a single graph, while others consider batches of graphs. To understand the difference between the two settings, consider the difference between classifying the nodes of a citation network, and classifying the chemical properties of molecules. For the citation network, we are interested in the individual nodes and the connections between them. Node and edge attributes are specific to each individual network, and we are usually not interested in training models that work on different networks. The nodes of themselves are our data. On the other hand, when working with molecules in a dataset, we are in a much more familiar setting. Each molecule is a sample of our dataset, and the atoms and bonds that make up the molecules are the constituent part of each data point (like pixels in images). In this case, we are interested in finding patterns that describe the properties of the molecules in general. The two settings require us to do things that are conceptually similar, but that need some minor adjustments in how the data is processed by our graph neural networks. This is why Spektral makes these differences explicit. In practice, we actually distinguish between three main modes of operation: single , where we have a single graph, with fixed topology and attributes; batch , where we have a set of different graphs, each with its own topology and attributes; mixed , where we have a graph with fixed topology, but a set of different attributes (usually called graph signals ); this can be seen as a particular case of the batch mode, but it is handled separately in Spektral to improve memory efficiency. We also have the graph batch mode, which is a simple trick to represent a batch of graphs in single mode. This requires an additional data structure to keep track of the graphs, and is explained in detail at the end of this section. The difference between the three main modes can be easily seen in how A , X , and E have different shapes in each case: Mode A.shape X.shape E.shape Single (N, N) (N, F) (N, N, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) (batch, N, N, S) Single mode In single mode the data describes a single graph. Three very popular datasets in this setting are the citation networks, Cora, Citeseer, and Pubmed. To load a citation network, you can use the built-in loader: In [1]: from spektral.datasets import citation Using TensorFlow backend. In [2]: A, X, _, _, _, _, _, _ = citation.load_data('cora') Loading cora dataset In [3]: A.shape Out[3]: (2708, 2708) In [4]: X.shape Out[4]: (2708, 1433) When training GNNs in single mode, we cannot batch and shuffle the data along the first axis, and the whole graph must be fed to the model at each step (see the node classification example ). Batch mode In batch mode , the matrices will have a batch dimension first. There are several benchmark datasets with this structure, including the very popular Benchmark Data Sets for Graph Kernels . A batch mode dataset that can be loaded natively by Spektral is the QM9 chemical database of small molecules: In [1]: from spektral.datasets import qm9 Using TensorFlow backend. In [2]: A, X, E, _ = qm9.load_data() Loading QM9 dataset. Reading SDF 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133885/133885 [00:29<00:00, 4579.22it/s] In [3]: A.shape Out[3]: (133885, 9, 9) In [4]: X.shape Out[4]: (133885, 9, 6) In [5]: E.shape Out[5]: (133885, 9, 9, 1) Note that the graphs in QM9 have variable order (i.e., different N for each graph), and that by default load_data() pads them with zeros in order to store the data in Numpy arrays. See the graph batch mode section for an alternative to zero-padding. Mixed mode In mixed mode we consider a single adjacency matrix, and different node and edge attributes matrices. An example of a mixed mode dataset is the MNIST random grid proposed by Defferrard et al. : In [1]: from spektral.datasets import mnist Using TensorFlow backend. In [2]: X, _, _, _, _, _, A = mnist.load_data() In [3]: A.shape Out[3]: (784, 784) In [4]: X.shape Out[4]: (50000, 784, 1) Graph batch mode When dealing with graphs with a variable number of nodes, representing a group of graphs in batch mode requires padding A , X , and E to a fixed dimension. In order to avoid this issue, a common approach is to represent a batch of graphs as a single disjoint union, and then using this \"supergraph\" in single mode. The disjoint union of a batch of graphs is a graph where: A is a block diagonal matrix, constructed from the adjacency matrices of the batch; X is obtained by stacking the node attributes of the batch; E is a block diagonal tensor of rank 3, obtained from the edge attributes; In order to keep track of different graphs in the disjoint union, we use an additional array of integers I , that maps each node to a graph with a progressive, zero-based indexing. Utilities for creating the disjoint union of a list of graphs are provided in spektral.utils.data : In [1]: from spektral.utils.data import Batch Using TensorFlow backend. In [2]: A_list = [np.ones((2, 2))] * 3 In [3]: X_list = [np.random.normal(size=(2, 4))] * 3 In [4]: b = Batch(A_list, X_list) In [5]: b.A.todense() Out[5]: matrix([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1.]]) In [6]: b.X Out[6]: array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]) In [7]: b.I Out[7]: array([0, 0, 1, 1, 2, 2]) In [8]: b.get('AXI') Out[8]: (<6x6 sparse matrix of type '<class 'numpy.float64'>' with 12 stored elements in COOrdinate format>, array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]), array([0, 0, 1, 1, 2, 2])) Convolutional layers that work in single mode will work for this type of data representation, without any modification. Pooling layers, on the other hand, require the batch indices vector in order to know which nodes to pool together. Global pooling layers will consume I and reduce the graphs to single vectors. Standard pooling layers will return a reduced version of I along with the reduced graphs. Conversion methods To provide better compatibility with other libraries, Spektral has methods to convert graphs between the matrix representation ( 'numpy' ) and other formats. The 'networkx' format represents graphs using the Networkx library, which can then be used to convert the graphs to other formats like .dot and edge lists. Conversion utils between 'numpy' and 'networkx' are provided in spektral.utils.conversion . Molecules When working with molecules, some specific formats can be used to represent the graphs. The 'sdf' format is an internal representation format used to store an SDF file as a dictionary. A molecule in 'sdf' format will look like this: {'atoms': [{'atomic_num': 7, 'charge': 0, 'coords': array([-0.0299, 1.2183, 0.2994]), 'index': 0, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}, ..., {'atomic_num': 1, 'charge': 0, 'coords': array([ 0.6896, -2.3002, -0.1042]), 'index': 14, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}], 'bonds': [{'end_atom': 13, 'info': array([0, 0, 0]), 'start_atom': 4, 'stereo': 0, 'type': 1}, ..., {'end_atom': 8, 'info': array([0, 0, 0]), 'start_atom': 7, 'stereo': 0, 'type': 3}], 'comment': '', 'data': [''], 'details': '-OEChem-03231823253D', 'n_atoms': 15, 'n_bonds': 15, 'name': 'gdb_54964', 'properties': []} The 'rdkit' format uses the RDKit library to represent molecules, and offers several methods to manipulate molecules with a chemistry-oriented approach. The 'smiles' format represents molecules as strings, and can be used as a space-efficient way to store molecules or perform quick checks on a dataset (e.g., counting the unique number of molecules in a dataset is quicker if all molecules are converted to SMILES first). The spektral.chem and spektral.utils modules offer conversion methods between all of these formats, although some conversions may need more than one step (e.g., 'sdf' to 'networkx' to 'numpy' to 'smiles' ). NOTE Support for direct conversion between all formats will be added eventually.","title":"Data representation"},{"location":"data/#representing-graphs","text":"Spektral uses a matrix-based representation for manipulating graphs and feeding them to neural networks. This approach is one of the most commonly used in the literature on graph neural networks, and it's perfect to perform parallel computations on GPU. WARNING Support for edge attributes is not fully implemented in Spektral. A graph is generally represented by three matrices: A \\in \\mathbb{R}^{N \\times N} , a square adjacency matrix where A_{ij} > 0 if there is a connection between nodes i and j , and A_{ij} = 0 otherwise; X \\in \\mathbb{R}^{N \\times F} , a matrix encoding node attributes, where each row represents the F -dimensional attribute vector of a node; E \\in \\mathbb{R}^{N \\times N \\times S} , a matrix encoding edge attributes, where each entry represents the S -dimensional attribute vector of an edge; Some frameworks (like the graph networks proposed by Battaglia et al.) also include a feature vector describing the global state of the graph, but this is not supported by Spektral for now. In code, and in this documentation, we use the following convention to refer to the formulation above: A is the adjacency matrix, N is the number of nodes; X is the node attributes matrix, F is the size of the edge attributes; E is the edge attributes matrix, S is the size of the edge attributes; See the table below for how these matrices are represented in Numpy.","title":"Representing graphs"},{"location":"data/#modes","text":"In Spektral, some functionalities are implemented to work on a single graph, while others consider batches of graphs. To understand the difference between the two settings, consider the difference between classifying the nodes of a citation network, and classifying the chemical properties of molecules. For the citation network, we are interested in the individual nodes and the connections between them. Node and edge attributes are specific to each individual network, and we are usually not interested in training models that work on different networks. The nodes of themselves are our data. On the other hand, when working with molecules in a dataset, we are in a much more familiar setting. Each molecule is a sample of our dataset, and the atoms and bonds that make up the molecules are the constituent part of each data point (like pixels in images). In this case, we are interested in finding patterns that describe the properties of the molecules in general. The two settings require us to do things that are conceptually similar, but that need some minor adjustments in how the data is processed by our graph neural networks. This is why Spektral makes these differences explicit. In practice, we actually distinguish between three main modes of operation: single , where we have a single graph, with fixed topology and attributes; batch , where we have a set of different graphs, each with its own topology and attributes; mixed , where we have a graph with fixed topology, but a set of different attributes (usually called graph signals ); this can be seen as a particular case of the batch mode, but it is handled separately in Spektral to improve memory efficiency. We also have the graph batch mode, which is a simple trick to represent a batch of graphs in single mode. This requires an additional data structure to keep track of the graphs, and is explained in detail at the end of this section. The difference between the three main modes can be easily seen in how A , X , and E have different shapes in each case: Mode A.shape X.shape E.shape Single (N, N) (N, F) (N, N, S) Batch (batch, N, N) (batch, N, F) (batch, N, N, S) Mixed (N, N) (batch, N, F) (batch, N, N, S)","title":"Modes"},{"location":"data/#single-mode","text":"In single mode the data describes a single graph. Three very popular datasets in this setting are the citation networks, Cora, Citeseer, and Pubmed. To load a citation network, you can use the built-in loader: In [1]: from spektral.datasets import citation Using TensorFlow backend. In [2]: A, X, _, _, _, _, _, _ = citation.load_data('cora') Loading cora dataset In [3]: A.shape Out[3]: (2708, 2708) In [4]: X.shape Out[4]: (2708, 1433) When training GNNs in single mode, we cannot batch and shuffle the data along the first axis, and the whole graph must be fed to the model at each step (see the node classification example ).","title":"Single mode"},{"location":"data/#batch-mode","text":"In batch mode , the matrices will have a batch dimension first. There are several benchmark datasets with this structure, including the very popular Benchmark Data Sets for Graph Kernels . A batch mode dataset that can be loaded natively by Spektral is the QM9 chemical database of small molecules: In [1]: from spektral.datasets import qm9 Using TensorFlow backend. In [2]: A, X, E, _ = qm9.load_data() Loading QM9 dataset. Reading SDF 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133885/133885 [00:29<00:00, 4579.22it/s] In [3]: A.shape Out[3]: (133885, 9, 9) In [4]: X.shape Out[4]: (133885, 9, 6) In [5]: E.shape Out[5]: (133885, 9, 9, 1) Note that the graphs in QM9 have variable order (i.e., different N for each graph), and that by default load_data() pads them with zeros in order to store the data in Numpy arrays. See the graph batch mode section for an alternative to zero-padding.","title":"Batch mode"},{"location":"data/#mixed-mode","text":"In mixed mode we consider a single adjacency matrix, and different node and edge attributes matrices. An example of a mixed mode dataset is the MNIST random grid proposed by Defferrard et al. : In [1]: from spektral.datasets import mnist Using TensorFlow backend. In [2]: X, _, _, _, _, _, A = mnist.load_data() In [3]: A.shape Out[3]: (784, 784) In [4]: X.shape Out[4]: (50000, 784, 1)","title":"Mixed mode"},{"location":"data/#graph-batch-mode","text":"When dealing with graphs with a variable number of nodes, representing a group of graphs in batch mode requires padding A , X , and E to a fixed dimension. In order to avoid this issue, a common approach is to represent a batch of graphs as a single disjoint union, and then using this \"supergraph\" in single mode. The disjoint union of a batch of graphs is a graph where: A is a block diagonal matrix, constructed from the adjacency matrices of the batch; X is obtained by stacking the node attributes of the batch; E is a block diagonal tensor of rank 3, obtained from the edge attributes; In order to keep track of different graphs in the disjoint union, we use an additional array of integers I , that maps each node to a graph with a progressive, zero-based indexing. Utilities for creating the disjoint union of a list of graphs are provided in spektral.utils.data : In [1]: from spektral.utils.data import Batch Using TensorFlow backend. In [2]: A_list = [np.ones((2, 2))] * 3 In [3]: X_list = [np.random.normal(size=(2, 4))] * 3 In [4]: b = Batch(A_list, X_list) In [5]: b.A.todense() Out[5]: matrix([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1.]]) In [6]: b.X Out[6]: array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]) In [7]: b.I Out[7]: array([0, 0, 1, 1, 2, 2]) In [8]: b.get('AXI') Out[8]: (<6x6 sparse matrix of type '<class 'numpy.float64'>' with 12 stored elements in COOrdinate format>, array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]), array([0, 0, 1, 1, 2, 2])) Convolutional layers that work in single mode will work for this type of data representation, without any modification. Pooling layers, on the other hand, require the batch indices vector in order to know which nodes to pool together. Global pooling layers will consume I and reduce the graphs to single vectors. Standard pooling layers will return a reduced version of I along with the reduced graphs.","title":"Graph batch mode"},{"location":"data/#conversion-methods","text":"To provide better compatibility with other libraries, Spektral has methods to convert graphs between the matrix representation ( 'numpy' ) and other formats. The 'networkx' format represents graphs using the Networkx library, which can then be used to convert the graphs to other formats like .dot and edge lists. Conversion utils between 'numpy' and 'networkx' are provided in spektral.utils.conversion .","title":"Conversion methods"},{"location":"data/#molecules","text":"When working with molecules, some specific formats can be used to represent the graphs. The 'sdf' format is an internal representation format used to store an SDF file as a dictionary. A molecule in 'sdf' format will look like this: {'atoms': [{'atomic_num': 7, 'charge': 0, 'coords': array([-0.0299, 1.2183, 0.2994]), 'index': 0, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}, ..., {'atomic_num': 1, 'charge': 0, 'coords': array([ 0.6896, -2.3002, -0.1042]), 'index': 14, 'info': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'iso': 0}], 'bonds': [{'end_atom': 13, 'info': array([0, 0, 0]), 'start_atom': 4, 'stereo': 0, 'type': 1}, ..., {'end_atom': 8, 'info': array([0, 0, 0]), 'start_atom': 7, 'stereo': 0, 'type': 3}], 'comment': '', 'data': [''], 'details': '-OEChem-03231823253D', 'n_atoms': 15, 'n_bonds': 15, 'name': 'gdb_54964', 'properties': []} The 'rdkit' format uses the RDKit library to represent molecules, and offers several methods to manipulate molecules with a chemistry-oriented approach. The 'smiles' format represents molecules as strings, and can be used as a space-efficient way to store molecules or perform quick checks on a dataset (e.g., counting the unique number of molecules in a dataset is quicker if all molecules are converted to SMILES first). The spektral.chem and spektral.utils modules offer conversion methods between all of these formats, although some conversions may need more than one step (e.g., 'sdf' to 'networkx' to 'numpy' to 'smiles' ). NOTE Support for direct conversion between all formats will be added eventually.","title":"Molecules"},{"location":"examples/","text":"Examples This is a collection of example scripts that you can use as template to solve your own tasks: Node classification on citation networks with GCN (single mode) ; Node classification on citation networks with ChebNets (single mode) ; Node classification on citation networks with GAT (single mode) ; Classification of synthetic graphs with GAT (batch mode) ; Regression of molecular properties on QM9 with Edge-Conditioned Convolutions (batch mode) ; Graph signal classification on MNIST (mixed mode) ;","title":"Examples"},{"location":"examples/#examples","text":"This is a collection of example scripts that you can use as template to solve your own tasks: Node classification on citation networks with GCN (single mode) ; Node classification on citation networks with ChebNets (single mode) ; Node classification on citation networks with GAT (single mode) ; Classification of synthetic graphs with GAT (batch mode) ; Regression of molecular properties on QM9 with Edge-Conditioned Convolutions (batch mode) ; Graph signal classification on MNIST (mixed mode) ;","title":"Examples"},{"location":"getting-started/","text":"Getting started Spektral is designed according to the Keras API principles, in order to make things extremely simple for beginners, while maintaining flexibility for experts and researchers. The most important features of Spektral are the layers.convolutional and layers.pooling modules, which offer a number of popular layers to start building graph neural networks (GNNs) right away. Because Spektral is designed as an extension of Keras, you can plug any Spektral layer into an existing Keras Model without modifications. A good starting point is the collection of examples which can be found on Github , and it is also a good idea to read the section on how to represent graphs before starting this tutorial. Node classification on citation networks In this example, we will build a simple Graph Convolutional Network for semi-supervised node classification . This is a simple but challenging task that has caused GNN's recent rise to popularity, and consists in classifying documents in a citation network . In this network, each node represents a document, and node attributes are bag-of-words binary features. A link between two nodes means that one of the two documents cites the other. Finally, each node has a class label. This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels. The datasets.citation module of Spektral allows you to download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as: from spektral.datasets import citation data = citation.load_data('cora') A, X, y_train, y_val, y_test, train_mask, val_mask, test_mask = data N = A.shape[0] F = X.shape[-1] n_classes = y_train.shape[-1] This will load the network's adjacency matrix ( A ) in a Scipy sparse format, the node features ( X ), and the pre-split training, validation, and test labels ( y_train, y_val, y_test ). The loader will also return some boolean masks to know which nodes belong to which set ( train_mask, val_mask, test_mask ). We also saved a couple of values that will be useful later: the number of nodes in the graph ( N ), the size of the node attributes ( F ), and the number of classes in the labels ( n_classes ). Creating a GNN To create a GCN similar to the one in the paper, we will use the GraphConv layer and the functional API of Keras: from spektral.layers import GraphConv from keras.models import Model from keras.layers import Input, Dropout Building the model is no different than building any Keras model, but we will need to provide multiple inputs to the GraphConv layers (namely A and X ): # Model definition X_in = Input(shape=(F, )) # Input layer for X A_in = Input((N, ), sparse=True) # Input layer for A graph_conv_1 = GraphConv(16, activation='relu')([X_in, A_in]) dropout = Dropout(0.5)(graph_conv_1) graph_conv_2 = GraphConv(n_classes, activation='softmax')([dropout, A_in]) # Build model model = Model(inputs=[X_in, A_in], outputs=graph_conv_2) And that's it. We just built our first GNN in Spektral and Keras. Note how we used the same familiar API of Keras for creating the GCN layers, as well as the standard Dropout layer to regularize our model. If we wanted, we could choose our own regularizers and initializers for the weights of GraphConv as well. An important thing to keep in mind is that in this single mode (see the data representation section ), there is no batch dimension. The \"elements\" of our dataset are, in a sense, the node themselves. This is why we omitted the first dimension of X and A in the Input layers ( shape=(F, ) instead of (N, F) , and shape=(N, ) instead of (N, N) ). This will become clearer later. Training the GNN Before training the model, we have to do a simple pre-processing of the adjacency matrix, in order to scale the weight (i.e., the importance) of a node's connections according to the node's degree. In other words, the more a node is connected, the less relative importance those connections have (plus some other minor considerations that you can find in the original GCN paper ). This is simply achieved by doing: from spektral import utils A = utils.localpooling_filter(A) which will give us a normalized adjacency matrix that we can use to train the GCN. What's left now for us is to compile and train our model: model.compile(optimizer='adam', loss='categorical_crossentropy', weighted_metrics=['acc']) model.summary() Note that we used the weighted_metrics argument instead of the usual metrics . This is due to the particular semi-supervised problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later). We are now ready to train the model: # Train model validation_data = ([X, A], y_val, val_mask) model.fit([X, A], y_train, sample_weight=train_mask, epochs=100, batch_size=N, validation_data=validation_data, shuffle=False) # Shuffling data means shuffling the whole graph There are a couple of things to note here. First, we trained our model using the native fit() method of Keras. No modifications needed. Second, we have set batch_size=N and shuffle=False . This is because we are working in single mode , meaning that Keras will interpret the first dimension of our adjacency and node attributes matrices as the \"batch\" dimension. If left to its own devices, Keras will automatically try to split our graph into batches of 32, and shuffle the batches at each epoch. For us, that means that the graph would get mixed and cut beyond repair, and the model would not be able to learn. This is why we tell Keras to use a batch size of N (the whole graph) and to not shuffle the nodes between epochs. This would not be necessary if we were working in batch mode instead, with many different graphs in our dataset. Finally, we used train_mask and val_mask as sample_weight . This results in the training nodes being assigned a weight of 1 during training, and the nodes outside the training set being assigned a weight of 0. The same holds for the validation and test sets. This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X and A for both phases? The only thing that changes is the mask and targets. This is also why we used the weighted_metrics flag when compiling the model. Evaluating the model Again, this is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training ( shuffle is False by default in evaluate() ): # Evaluate model eval_results = model.evaluate([X, A], y_test, sample_weight=test_mask, batch_size=N) print('Done.\\n' 'Test loss: {}\\n' 'Test accuracy: {}'.format(*eval_results)) Done! Our model has been defined, trained, and evaluated. Go create! If you made it to this point, you are ready to use Spektral to create your own models. If you want to build a GNN for a specific task, chances are that the things you need to define the model and pre-process the data are already part of Spektral. Check the examples for some ideas and practical tips. Remember to read the data representation section to learn about how GNNs can be used to solve different problems. Make sure to check the documentation, and leave a comment on Github if you have a feature that you want to see implemented.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Spektral is designed according to the Keras API principles, in order to make things extremely simple for beginners, while maintaining flexibility for experts and researchers. The most important features of Spektral are the layers.convolutional and layers.pooling modules, which offer a number of popular layers to start building graph neural networks (GNNs) right away. Because Spektral is designed as an extension of Keras, you can plug any Spektral layer into an existing Keras Model without modifications. A good starting point is the collection of examples which can be found on Github , and it is also a good idea to read the section on how to represent graphs before starting this tutorial.","title":"Getting started"},{"location":"getting-started/#node-classification-on-citation-networks","text":"In this example, we will build a simple Graph Convolutional Network for semi-supervised node classification . This is a simple but challenging task that has caused GNN's recent rise to popularity, and consists in classifying documents in a citation network . In this network, each node represents a document, and node attributes are bag-of-words binary features. A link between two nodes means that one of the two documents cites the other. Finally, each node has a class label. This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels. The datasets.citation module of Spektral allows you to download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as: from spektral.datasets import citation data = citation.load_data('cora') A, X, y_train, y_val, y_test, train_mask, val_mask, test_mask = data N = A.shape[0] F = X.shape[-1] n_classes = y_train.shape[-1] This will load the network's adjacency matrix ( A ) in a Scipy sparse format, the node features ( X ), and the pre-split training, validation, and test labels ( y_train, y_val, y_test ). The loader will also return some boolean masks to know which nodes belong to which set ( train_mask, val_mask, test_mask ). We also saved a couple of values that will be useful later: the number of nodes in the graph ( N ), the size of the node attributes ( F ), and the number of classes in the labels ( n_classes ).","title":"Node classification on citation networks"},{"location":"getting-started/#creating-a-gnn","text":"To create a GCN similar to the one in the paper, we will use the GraphConv layer and the functional API of Keras: from spektral.layers import GraphConv from keras.models import Model from keras.layers import Input, Dropout Building the model is no different than building any Keras model, but we will need to provide multiple inputs to the GraphConv layers (namely A and X ): # Model definition X_in = Input(shape=(F, )) # Input layer for X A_in = Input((N, ), sparse=True) # Input layer for A graph_conv_1 = GraphConv(16, activation='relu')([X_in, A_in]) dropout = Dropout(0.5)(graph_conv_1) graph_conv_2 = GraphConv(n_classes, activation='softmax')([dropout, A_in]) # Build model model = Model(inputs=[X_in, A_in], outputs=graph_conv_2) And that's it. We just built our first GNN in Spektral and Keras. Note how we used the same familiar API of Keras for creating the GCN layers, as well as the standard Dropout layer to regularize our model. If we wanted, we could choose our own regularizers and initializers for the weights of GraphConv as well. An important thing to keep in mind is that in this single mode (see the data representation section ), there is no batch dimension. The \"elements\" of our dataset are, in a sense, the node themselves. This is why we omitted the first dimension of X and A in the Input layers ( shape=(F, ) instead of (N, F) , and shape=(N, ) instead of (N, N) ). This will become clearer later.","title":"Creating a GNN"},{"location":"getting-started/#training-the-gnn","text":"Before training the model, we have to do a simple pre-processing of the adjacency matrix, in order to scale the weight (i.e., the importance) of a node's connections according to the node's degree. In other words, the more a node is connected, the less relative importance those connections have (plus some other minor considerations that you can find in the original GCN paper ). This is simply achieved by doing: from spektral import utils A = utils.localpooling_filter(A) which will give us a normalized adjacency matrix that we can use to train the GCN. What's left now for us is to compile and train our model: model.compile(optimizer='adam', loss='categorical_crossentropy', weighted_metrics=['acc']) model.summary() Note that we used the weighted_metrics argument instead of the usual metrics . This is due to the particular semi-supervised problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later). We are now ready to train the model: # Train model validation_data = ([X, A], y_val, val_mask) model.fit([X, A], y_train, sample_weight=train_mask, epochs=100, batch_size=N, validation_data=validation_data, shuffle=False) # Shuffling data means shuffling the whole graph There are a couple of things to note here. First, we trained our model using the native fit() method of Keras. No modifications needed. Second, we have set batch_size=N and shuffle=False . This is because we are working in single mode , meaning that Keras will interpret the first dimension of our adjacency and node attributes matrices as the \"batch\" dimension. If left to its own devices, Keras will automatically try to split our graph into batches of 32, and shuffle the batches at each epoch. For us, that means that the graph would get mixed and cut beyond repair, and the model would not be able to learn. This is why we tell Keras to use a batch size of N (the whole graph) and to not shuffle the nodes between epochs. This would not be necessary if we were working in batch mode instead, with many different graphs in our dataset. Finally, we used train_mask and val_mask as sample_weight . This results in the training nodes being assigned a weight of 1 during training, and the nodes outside the training set being assigned a weight of 0. The same holds for the validation and test sets. This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X and A for both phases? The only thing that changes is the mask and targets. This is also why we used the weighted_metrics flag when compiling the model.","title":"Training the GNN"},{"location":"getting-started/#evaluating-the-model","text":"Again, this is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training ( shuffle is False by default in evaluate() ): # Evaluate model eval_results = model.evaluate([X, A], y_test, sample_weight=test_mask, batch_size=N) print('Done.\\n' 'Test loss: {}\\n' 'Test accuracy: {}'.format(*eval_results)) Done! Our model has been defined, trained, and evaluated.","title":"Evaluating the model"},{"location":"getting-started/#go-create","text":"If you made it to this point, you are ready to use Spektral to create your own models. If you want to build a GNN for a specific task, chances are that the things you need to define the model and pre-process the data are already part of Spektral. Check the examples for some ideas and practical tips. Remember to read the data representation section to learn about how GNNs can be used to solve different problems. Make sure to check the documentation, and leave a comment on Github if you have a feature that you want to see implemented.","title":"Go create!"},{"location":"datasets/citation/","text":"load_data spektral.datasets.citation.load_data(dataset_name='cora', normalize_features=True) Loads a citation dataset using the public splits as defined in Kipf & Welling (2016) . Arguments dataset_name : name of the dataset to load ('cora', 'citeseer', or 'pubmed'); normalize_features : if True, the node features are normalized; Return The citation network in numpy format, with train, test, and validation splits for the targets and masks.","title":"Citation"},{"location":"datasets/citation/#load_data","text":"spektral.datasets.citation.load_data(dataset_name='cora', normalize_features=True) Loads a citation dataset using the public splits as defined in Kipf & Welling (2016) . Arguments dataset_name : name of the dataset to load ('cora', 'citeseer', or 'pubmed'); normalize_features : if True, the node features are normalized; Return The citation network in numpy format, with train, test, and validation splits for the targets and masks.","title":"load_data"},{"location":"datasets/delaunay/","text":"generate_data spektral.datasets.delaunay.generate_data(return_type='numpy', classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None) Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Note that this function is basically deprecated and will change soon. Arguments return_type : 'numpy' or 'networkx', data format to return; classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; Return If return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"Delaunay"},{"location":"datasets/delaunay/#generate_data","text":"spektral.datasets.delaunay.generate_data(return_type='numpy', classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.0, support_high=10.0, drift_amount=1.0, one_hot_labels=True, support=None, seed=None) Generates a dataset of Delaunay triangulations as described by Zambon et al. (2017) . Note that this function is basically deprecated and will change soon. Arguments return_type : 'numpy' or 'networkx', data format to return; classes : indices of the classes to load (integer, or list of integers between 0 and 20); n_samples_in_class : number of generated samples per class; n_nodes : number of nodes in a graph; support_low : lower bound of the uniform distribution from which the support is generated; support_high : upper bound of the uniform distribution from which the support is generated; drift_amount : coefficient to control the amount of change between classes; one_hot_labels : one-hot encode dataset labels; support : custom support to use instead of generating it randomly; seed : random numpy seed; Return If return_type='numpy' , the adjacency matrix, node features, and an array containing labels; if return_type='networkx' , a list of graphs in Networkx format, and an array containing labels;","title":"generate_data"},{"location":"datasets/mnist/","text":"load_data spektral.datasets.mnist.load_data() Loads the MNIST dataset and the associated grid. This code is largely taken from Micha\u00ebl Defferrard's Github . Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"MNIST"},{"location":"datasets/mnist/#load_data","text":"spektral.datasets.mnist.load_data() Loads the MNIST dataset and the associated grid. This code is largely taken from Micha\u00ebl Defferrard's Github . Return X_train, y_train: training node features and labels; X_val, y_val: validation node features and labels; X_test, y_test: test node features and labels; A: adjacency matrix of the grid;","title":"load_data"},{"location":"datasets/qm9/","text":"load_data spektral.datasets.qm9.load_data(return_type='numpy', nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None) Loads the QM9 molecules dataset. Arguments return_type : 'networkx', 'numpy', or 'sdf', data format to return; nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in order). Return If return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"QM9"},{"location":"datasets/qm9/#load_data","text":"spektral.datasets.qm9.load_data(return_type='numpy', nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False, amount=None) Loads the QM9 molecules dataset. Arguments return_type : 'networkx', 'numpy', or 'sdf', data format to return; nf_keys : list or str, node features to return (see qm9.NODE_FEATURES for available features); ef_keys : list or str, edge features to return (see qm9.EDGE_FEATURES for available features); auto_pad : if return_type='numpy' , zero pad graph matrices to have the same number of nodes; self_loops : if return_type='numpy' , add self loops to adjacency matrices; amount : the amount of molecules to return (in order). Return If return_type='numpy' , the adjacency matrix, node features, edge features, and a Pandas dataframe containing labels; if return_type='networkx' , a list of graphs in Networkx format, and a dataframe containing labels; if return_type='sdf' , a list of molecules in the internal SDF format and a dataframe containing labels.","title":"load_data"},{"location":"layers/base/","text":"[source] InnerProduct spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = xx^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments trainable_kernel : add a trainable square matrix between the inner product (i.e., x.dot(w).dot(x.T) ); activation : activation function to use; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel; [source] MinkowskiProduct spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None, activity_regularizer=None) Computes the hyperbolic inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = x \\, \\begin{pmatrix} I_{d\\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\,x^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments input_dim_1 : first dimension of the input tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use; activity_regularizer : regularization applied to the output; [source] CCMProjection spektral.layers.CCMProjection(r=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Projects a tensor to a CCM depending on the value of r . Optionally, r can be learned via backpropagation. Input tensor of shape (batch_size, input_dim) . Output tensor of shape (batch_size, input_dim) , where each sample along the 0th axis is projected to the CCM. Arguments r : radius of the CCM. If r is a number, then use it as fixed radius. If r='spherical' , use a trainable weight as radius, with a positivity constraint. If r='hyperbolic' , use a trainable weight as radius, with a negativity constraint. If r=None , use a trainable weight as radius, with no constraints (points will be projected to the correct manifold based on the sign of the weight). kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix. [source] CCMMembership spektral.layers.CCMMembership(r=1.0, mode='average', sigma=1.0) Computes the membership of the given points to a constant-curvature manifold of radius r , as: \\mu(x) = \\mathrm{exp}\\left(\\cfrac{-\\big( \\langle \\vec x, \\vec x \\rangle - r^2 \\big)^2}{2\\sigma^2}\\right). If r=0 , then \\mu(x) = 1 . If more than one radius is given, inputs are evenly split across the last dimension and membership is computed for each radius-slice pair. The output membership is returned according to the mode option. Input tensor of shape (batch_size, input_dim) ; Output tensor of shape (batch_size, output_size) , where output_size is computed according to the mode option;. Arguments r : int ot list, radia of the CCMs. mode : 'average' to return the average membership across CCMs, or 'concat' to return the membership for each CCM concatenated; sigma : spread of the membership curve;","title":"Base Layers"},{"location":"layers/base/#innerproduct","text":"spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = xx^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments trainable_kernel : add a trainable square matrix between the inner product (i.e., x.dot(w).dot(x.T) ); activation : activation function to use; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel; [source]","title":"InnerProduct"},{"location":"layers/base/#minkowskiproduct","text":"spektral.layers.MinkowskiProduct(input_dim_1=None, activation=None, activity_regularizer=None) Computes the hyperbolic inner product between elements of a given 2d tensor x : \\langle x, x \\rangle = x \\, \\begin{pmatrix} I_{d\\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\,x^T. Mode : single. Input rank 2 tensor of shape (input_dim_1, input_dim_2) (e.g. node features of shape (num_nodes, num_features) ); Output rank 2 tensor of shape (input_dim_1, input_dim_1) Arguments input_dim_1 : first dimension of the input tensor; set this if you encounter issues with shapes in your model, in order to provide an explicit output shape for your layer. activation : activation function to use; activity_regularizer : regularization applied to the output; [source]","title":"MinkowskiProduct"},{"location":"layers/base/#ccmprojection","text":"spektral.layers.CCMProjection(r=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Projects a tensor to a CCM depending on the value of r . Optionally, r can be learned via backpropagation. Input tensor of shape (batch_size, input_dim) . Output tensor of shape (batch_size, input_dim) , where each sample along the 0th axis is projected to the CCM. Arguments r : radius of the CCM. If r is a number, then use it as fixed radius. If r='spherical' , use a trainable weight as radius, with a positivity constraint. If r='hyperbolic' , use a trainable weight as radius, with a negativity constraint. If r=None , use a trainable weight as radius, with no constraints (points will be projected to the correct manifold based on the sign of the weight). kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; kernel_constraint : constraint applied to the kernel matrix. [source]","title":"CCMProjection"},{"location":"layers/base/#ccmmembership","text":"spektral.layers.CCMMembership(r=1.0, mode='average', sigma=1.0) Computes the membership of the given points to a constant-curvature manifold of radius r , as: \\mu(x) = \\mathrm{exp}\\left(\\cfrac{-\\big( \\langle \\vec x, \\vec x \\rangle - r^2 \\big)^2}{2\\sigma^2}\\right). If r=0 , then \\mu(x) = 1 . If more than one radius is given, inputs are evenly split across the last dimension and membership is computed for each radius-slice pair. The output membership is returned according to the mode option. Input tensor of shape (batch_size, input_dim) ; Output tensor of shape (batch_size, output_size) , where output_size is computed according to the mode option;. Arguments r : int ot list, radia of the CCMs. mode : 'average' to return the average membership across CCMs, or 'concat' to return the membership for each CCM concatenated; sigma : spread of the membership curve;","title":"CCMMembership"},{"location":"layers/convolution/","text":"[source] GraphConv spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) . Mode : single, mixed, batch. This layer computes: Z = \\sigma( \\tilde{A} XW + b) where X is the node features matrix, \\tilde{A} is the normalized Laplacian, W is the convolution kernel, b is a bias vector, and \\sigma is the activation function. Input Node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized Laplacian of shape (n_nodes, n_nodes) (with optional batch dimension); see spektral.utils.convolution.localpooling_filter . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.localpooling_filter(A) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, ), sparse=True) output = GraphConv(channels)([X_in, fltr_in]) [source] ChebConv spektral.layers.ChebConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, mixed, batch. Given a list of Chebyshev polynomials T = [T_{1}, ..., T_{K}] , this layer computes: Z = \\sigma( \\sum \\limits_{k=1}^{K} T_{k} X W + b) where X is the node features matrix, W is the convolution kernel, b is the bias vector, and \\sigma is the activation function. Input Node features of shape (n_nodes, n_features) (with optional batch dimension); A list of Chebyshev polynomials of shape (num_nodes, num_nodes) (with optional batch dimension); see spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.chebyshev_filter(A, K) # Model definition X_in = Input(shape=(F, )) fltr_in = [Input((N, ), sparse=True) for _ in range(K + 1)] output = ChebConv(channels)([X_in] + fltr_in) [source] GraphSageConv spektral.layers.GraphSageConv(channels, aggregate_method='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSage layer as presented by Hamilton et al. (2017) . Mode : single. This layer computes: Z = \\sigma \\big( \\big[ \\textrm{AGGREGATE}(X) \\| X \\big] W + b \\big) where X is the node features matrix, W is a trainable kernel, b is a bias vector, and \\sigma is the activation function. \\textrm{AGGREGATE} is an aggregation function as described in the original paper, that works by aggregating each node's neighbourhood according to some rule. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (n_nodes, n_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; aggregate_method : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage X_in = Input(shape=(F, )) A_in = Input((N, ), sparse=True) output = GraphSageConv(channels)([X_in, A_in]) [source] ARMAConv spektral.layers.ARMAConv(channels, T=1, K=1, recurrent=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA(K, K-1) filters, as presented by Bianchi et al. (2019) . Mode : single, mixed, batch. This layer computes: Z = \\frac{1}{K}\\sum \\limits_{k=1}^K \\bar{X}_k^{(T)}, where K is the order of the ARMA(K, K-1) filter, and where: \\bar{X}_k^{(t + 1)} = \\sigma\\left(\\tilde{L}\\bar{X}^{(t)}W^{(t)} + XV^{(t)}\\right) is a graph convolutional skip layer implementing a recursive approximation of an ARMA(1, 0) filter, \\tilde{L} is normalized graph Laplacian with a rescaled spectrum, \\bar{X}^{(0)} = X , and W, V are trainable kernels. Input node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized Laplacian of shape (n_nodes, n_nodes) (with optional batch dimension); see the ARMA node classification example Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; T : depth of each ARMA(1, 0) approximation (number of recursive updates); K : order of the full ARMA(K, K-1) filter (combination of K ARMA(1, 0) filters); recurrent : whether to share each head's weights like a recurrent net; gcn_activation : activation function to use to compute the ARMA filter; dropout_rate : dropout rate for Laplacian and output layer; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.normalized_adjacency(A) # Model definition X_in= Input(shape=(F, ), sparse=True) fltr_in = Input((N, )) output = ARMAConv(channels)([X_in, fltr_in]) [source] EdgeConditionedConv spektral.layers.EdgeConditionedConv(channels, kernel_network=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer as presented by Simonovsky and Komodakis (2017) . Mode : batch. For each node i , this layer computes: Z_i = \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} F(E_{ji}) X_{j} + b where \\mathcal{N}(i) represents the one-step neighbourhood of node i , F is a neural network that outputs the convolution kernel as a function of edge attributes, E is the edge attributes matrix, and b is a bias vector. Input node features of shape (batch, n_nodes, n_node_features) ; adjacency matrices of shape (batch, n_nodes, num_nodes) ; edge features of shape (batch, n_nodes, n_nodes, n_edge_features) ; Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers describing the hidden structure of the kernel-generating network (i.e., the ReLU layers before the linear output); activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage X_in = Input(shape=(N, F)) A_in = Input(shape=(N, N)) E_in = Input(shape=(N, N, S)) output = EdgeConditionedConv(channels)([X_in, A_in, E_in]) [source] GraphAttention spektral.layers.GraphAttention(channels, attn_heads=1, attn_heads_reduction='concat', dropout_rate=0.5, activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer as presented by Velickovic et al. (2017) . Mode : single, mixed, batch. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian. Input node features of shape (n_nodes, n_features) (with optional batch dimension); adjacency matrices of shape (n_nodes, n_nodes) (with optional batch dimension); Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; attn_heads : number of attention heads to use; attn_heads_reduction : how to reduce the outputs of the attention heads (can be either 'concat' or 'average'); dropout_rate : internal dropout rate; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; attn_kernel_initializer : initializer for the attention kernel matrices; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrices; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrices; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations A = utils.add_eye(A).toarray() # Add self-loops # Model definition X_in = Input(shape=(F, )) A_in = Input((N, )) output = GraphAttention(channels)([X_in, A_in]) [source] GraphConvSkip spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) , with the addition of a skip connection. Mode : single, mixed, batch. This layer computes: Z = \\sigma(A X W_1 + X W_2 + b) where X is the node features matrix, A is the normalized laplacian, W_1 and W_2 are the convolution kernels, b is a bias vector, and \\sigma is the activation function. Input node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized adjacency matrix of shape (n_nodes, n_nodes) (with optional batch dimension); see spektral.utils.convolution.normalized_adjacency . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.normalized_adjacency(A) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, ), sparse=True) output = GraphConvSkip(channels)([X_in, fltr_in]) [source] APPNP spektral.layers.APPNP(channels, mlp_channels, alpha=0.2, H=1, K=1, mlp_activation='relu', dropout_rate=0.0, activation='softmax', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . Mode : single, mixed, batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized adjacency matrix of shape (n_nodes, n_nodes) (with optional batch dimension); see spektral.utils.convolution.normalized_adjacency . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_channels : integer, number of hidden units for the MLP layers; alpha : teleport probability; H : number of MLP layers; K : number of power iterations; mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations I = sp.identity(A.shape[0], dtype=A.dtype) fltr = utils.normalize_adjacency(A + I) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, )) output = APPNP(channels, mlp_channels)([X_in, fltr_in]) [source] GINConv spektral.layers.GINConv(channels, mlp_channels=16, n_hidden_layers=0, epsilon=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) as presented by Xu et al. (2018) . Mode : single. This layer computes for each node i : Z_i = \\textrm{MLP} ( (1 + \\epsilon) \\cdot X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} X_j) where X is the node features matrix and \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized and rescaled Laplacian of shape (n_nodes, n_nodes) (with optional batch dimension); Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_channels : integer, number of channels in the inner MLP; n_hidden_layers : integer, number of hidden layers in the MLP (default 0) epsilon : unnamed parameter, see Xu et al. (2018) . In practice, it is safe to leave it to 0. mlp_activation : activation function for the MLP, activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.normalized_laplacian(A) fltr = utils.rescale_laplacian(X, lmax=2) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, ), sparse=True) output = GINConv(channels)([X_in, fltr_in])","title":"Convolutional Layers"},{"location":"layers/convolution/#graphconv","text":"spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) . Mode : single, mixed, batch. This layer computes: Z = \\sigma( \\tilde{A} XW + b) where X is the node features matrix, \\tilde{A} is the normalized Laplacian, W is the convolution kernel, b is a bias vector, and \\sigma is the activation function. Input Node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized Laplacian of shape (n_nodes, n_nodes) (with optional batch dimension); see spektral.utils.convolution.localpooling_filter . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.localpooling_filter(A) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, ), sparse=True) output = GraphConv(channels)([X_in, fltr_in]) [source]","title":"GraphConv"},{"location":"layers/convolution/#chebconv","text":"spektral.layers.ChebConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer as presented by Defferrard et al. (2016) . Mode : single, mixed, batch. Given a list of Chebyshev polynomials T = [T_{1}, ..., T_{K}] , this layer computes: Z = \\sigma( \\sum \\limits_{k=1}^{K} T_{k} X W + b) where X is the node features matrix, W is the convolution kernel, b is the bias vector, and \\sigma is the activation function. Input Node features of shape (n_nodes, n_features) (with optional batch dimension); A list of Chebyshev polynomials of shape (num_nodes, num_nodes) (with optional batch dimension); see spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.chebyshev_filter(A, K) # Model definition X_in = Input(shape=(F, )) fltr_in = [Input((N, ), sparse=True) for _ in range(K + 1)] output = ChebConv(channels)([X_in] + fltr_in) [source]","title":"ChebConv"},{"location":"layers/convolution/#graphsageconv","text":"spektral.layers.GraphSageConv(channels, aggregate_method='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSage layer as presented by Hamilton et al. (2017) . Mode : single. This layer computes: Z = \\sigma \\big( \\big[ \\textrm{AGGREGATE}(X) \\| X \\big] W + b \\big) where X is the node features matrix, W is a trainable kernel, b is a bias vector, and \\sigma is the activation function. \\textrm{AGGREGATE} is an aggregation function as described in the original paper, that works by aggregating each node's neighbourhood according to some rule. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (n_nodes, n_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; aggregate_method : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage X_in = Input(shape=(F, )) A_in = Input((N, ), sparse=True) output = GraphSageConv(channels)([X_in, A_in]) [source]","title":"GraphSageConv"},{"location":"layers/convolution/#armaconv","text":"spektral.layers.ARMAConv(channels, T=1, K=1, recurrent=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer with ARMA(K, K-1) filters, as presented by Bianchi et al. (2019) . Mode : single, mixed, batch. This layer computes: Z = \\frac{1}{K}\\sum \\limits_{k=1}^K \\bar{X}_k^{(T)}, where K is the order of the ARMA(K, K-1) filter, and where: \\bar{X}_k^{(t + 1)} = \\sigma\\left(\\tilde{L}\\bar{X}^{(t)}W^{(t)} + XV^{(t)}\\right) is a graph convolutional skip layer implementing a recursive approximation of an ARMA(1, 0) filter, \\tilde{L} is normalized graph Laplacian with a rescaled spectrum, \\bar{X}^{(0)} = X , and W, V are trainable kernels. Input node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized Laplacian of shape (n_nodes, n_nodes) (with optional batch dimension); see the ARMA node classification example Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; T : depth of each ARMA(1, 0) approximation (number of recursive updates); K : order of the full ARMA(K, K-1) filter (combination of K ARMA(1, 0) filters); recurrent : whether to share each head's weights like a recurrent net; gcn_activation : activation function to use to compute the ARMA filter; dropout_rate : dropout rate for Laplacian and output layer; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.normalized_adjacency(A) # Model definition X_in= Input(shape=(F, ), sparse=True) fltr_in = Input((N, )) output = ARMAConv(channels)([X_in, fltr_in]) [source]","title":"ARMAConv"},{"location":"layers/convolution/#edgeconditionedconv","text":"spektral.layers.EdgeConditionedConv(channels, kernel_network=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer as presented by Simonovsky and Komodakis (2017) . Mode : batch. For each node i , this layer computes: Z_i = \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} F(E_{ji}) X_{j} + b where \\mathcal{N}(i) represents the one-step neighbourhood of node i , F is a neural network that outputs the convolution kernel as a function of edge attributes, E is the edge attributes matrix, and b is a bias vector. Input node features of shape (batch, n_nodes, n_node_features) ; adjacency matrices of shape (batch, n_nodes, num_nodes) ; edge features of shape (batch, n_nodes, n_nodes, n_edge_features) ; Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers describing the hidden structure of the kernel-generating network (i.e., the ReLU layers before the linear output); activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage X_in = Input(shape=(N, F)) A_in = Input(shape=(N, N)) E_in = Input(shape=(N, N, S)) output = EdgeConditionedConv(channels)([X_in, A_in, E_in]) [source]","title":"EdgeConditionedConv"},{"location":"layers/convolution/#graphattention","text":"spektral.layers.GraphAttention(channels, attn_heads=1, attn_heads_reduction='concat', dropout_rate=0.5, activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A graph attention layer as presented by Velickovic et al. (2017) . Mode : single, mixed, batch. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian. Input node features of shape (n_nodes, n_features) (with optional batch dimension); adjacency matrices of shape (n_nodes, n_nodes) (with optional batch dimension); Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; attn_heads : number of attention heads to use; attn_heads_reduction : how to reduce the outputs of the attention heads (can be either 'concat' or 'average'); dropout_rate : internal dropout rate; activation : activation function to use; use_bias : boolean, whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; attn_kernel_initializer : initializer for the attention kernel matrices; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrices; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrices; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations A = utils.add_eye(A).toarray() # Add self-loops # Model definition X_in = Input(shape=(F, )) A_in = Input((N, )) output = GraphAttention(channels)([X_in, A_in]) [source]","title":"GraphAttention"},{"location":"layers/convolution/#graphconvskip","text":"spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer as presented by Kipf & Welling (2016) , with the addition of a skip connection. Mode : single, mixed, batch. This layer computes: Z = \\sigma(A X W_1 + X W_2 + b) where X is the node features matrix, A is the normalized laplacian, W_1 and W_2 are the convolution kernels, b is a bias vector, and \\sigma is the activation function. Input node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized adjacency matrix of shape (n_nodes, n_nodes) (with optional batch dimension); see spektral.utils.convolution.normalized_adjacency . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.normalized_adjacency(A) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, ), sparse=True) output = GraphConvSkip(channels)([X_in, fltr_in]) [source]","title":"GraphConvSkip"},{"location":"layers/convolution/#appnp","text":"spektral.layers.APPNP(channels, mlp_channels, alpha=0.2, H=1, K=1, mlp_activation='relu', dropout_rate=0.0, activation='softmax', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer implementing the APPNP operator, as presented by Klicpera et al. (2019) . Mode : single, mixed, batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized adjacency matrix of shape (n_nodes, n_nodes) (with optional batch dimension); see spektral.utils.convolution.normalized_adjacency . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_channels : integer, number of hidden units for the MLP layers; alpha : teleport probability; H : number of MLP layers; K : number of power iterations; mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations I = sp.identity(A.shape[0], dtype=A.dtype) fltr = utils.normalize_adjacency(A + I) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, )) output = APPNP(channels, mlp_channels)([X_in, fltr_in]) [source]","title":"APPNP"},{"location":"layers/convolution/#ginconv","text":"spektral.layers.GINConv(channels, mlp_channels=16, n_hidden_layers=0, epsilon=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) as presented by Xu et al. (2018) . Mode : single. This layer computes for each node i : Z_i = \\textrm{MLP} ( (1 + \\epsilon) \\cdot X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} X_j) where X is the node features matrix and \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (n_nodes, n_features) (with optional batch dimension); Normalized and rescaled Laplacian of shape (n_nodes, n_nodes) (with optional batch dimension); Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_channels : integer, number of channels in the inner MLP; n_hidden_layers : integer, number of hidden layers in the MLP (default 0) epsilon : unnamed parameter, see Xu et al. (2018) . In practice, it is safe to leave it to 0. mlp_activation : activation function for the MLP, activation : activation function to use; use_bias : whether to add a bias to the linear transformation; kernel_initializer : initializer for the kernel matrix; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. Usage # Load data A, X, _, _, _, _, _, _ = citation.load_data('cora') # Preprocessing operations fltr = utils.normalized_laplacian(A) fltr = utils.rescale_laplacian(X, lmax=2) # Model definition X_in = Input(shape=(F, )) fltr_in = Input((N, ), sparse=True) output = GINConv(channels)([X_in, fltr_in])","title":"GINConv"},{"location":"layers/pooling/","text":"[source] TopKPool spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None) A gPool/Top-K layer as presented by Gao & Ji (2017) and Cangea et al. . This layer computes the following operations: y = \\cfrac{Xp}{\\| p \\|}; \\;\\;\\;\\; \\textrm{idx} = \\textrm{rank}(y, k); \\;\\;\\;\\; \\bar X = (X \\odot \\textrm{tanh}(y))_{\\textrm{idx}}; \\;\\;\\;\\; \\bar A = A^2_{\\textrm{idx}, \\textrm{idx}} where \\textrm{rank}(y, k) returns the indices of the top k values of y , and p is a learnable parameter vector of size F . Note that the the gating operation \\textrm{tanh}(y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). The original paper by Gao & Ji used a tanh as well, but was later updated to use a sigmoid activation. Due to the lack of sparse-sparse matrix multiplication support, this layer temporarily makes the adjacency matrix dense in order to compute A^2 (needed to preserve connectivity after pooling). If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from dense to sparse and viceversa is a costly operation. Mode : single, graph batch. Input node features of shape (n_nodes, n_features) ; adjacency matrix of shape (n_nodes, n_nodes) ; (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output reduced node features of shape (n_graphs * k, n_features) ; reduced adjacency matrix of shape (n_graphs * k, n_graphs * k) ; reduced graph IDs with shape (n_graphs * k, ) (graph batch mode); Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; [source] GlobalSumPool spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments None. [source] GlobalAvgPool spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments None. [source] GlobalMaxPool spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments None. [source] GlobalAttentionPool spektral.layers.GlobalAttentionPool(channels=32, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer as presented by Li et al. (2017) . Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1), and last dimension changed to channels . Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source] GlobalAttnSumPool spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments attn_kernel_initializer : initializer for the attention kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix;","title":"Pooling Layers"},{"location":"layers/pooling/#topkpool","text":"spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None) A gPool/Top-K layer as presented by Gao & Ji (2017) and Cangea et al. . This layer computes the following operations: y = \\cfrac{Xp}{\\| p \\|}; \\;\\;\\;\\; \\textrm{idx} = \\textrm{rank}(y, k); \\;\\;\\;\\; \\bar X = (X \\odot \\textrm{tanh}(y))_{\\textrm{idx}}; \\;\\;\\;\\; \\bar A = A^2_{\\textrm{idx}, \\textrm{idx}} where \\textrm{rank}(y, k) returns the indices of the top k values of y , and p is a learnable parameter vector of size F . Note that the the gating operation \\textrm{tanh}(y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). The original paper by Gao & Ji used a tanh as well, but was later updated to use a sigmoid activation. Due to the lack of sparse-sparse matrix multiplication support, this layer temporarily makes the adjacency matrix dense in order to compute A^2 (needed to preserve connectivity after pooling). If memory is not an issue, considerable speedups can be achieved by using dense graphs directly. Converting a graph from dense to sparse and viceversa is a costly operation. Mode : single, graph batch. Input node features of shape (n_nodes, n_features) ; adjacency matrix of shape (n_nodes, n_nodes) ; (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output reduced node features of shape (n_graphs * k, n_features) ; reduced adjacency matrix of shape (n_graphs * k, n_graphs * k) ; reduced graph IDs with shape (n_graphs * k, ) (graph batch mode); Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_mask : boolean, whether to return the binary mask used for pooling; sigmoid_gating : boolean, use a sigmoid gating activation instead of a tanh; kernel_initializer : initializer for the kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; [source]","title":"TopKPool"},{"location":"layers/pooling/#globalsumpool","text":"spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments None. [source]","title":"GlobalSumPool"},{"location":"layers/pooling/#globalavgpool","text":"spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments None. [source]","title":"GlobalAvgPool"},{"location":"layers/pooling/#globalmaxpool","text":"spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments None. [source]","title":"GlobalMaxPool"},{"location":"layers/pooling/#globalattentionpool","text":"spektral.layers.GlobalAttentionPool(channels=32, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer as presented by Li et al. (2017) . Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1), and last dimension changed to channels . Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the kernel matrix; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the kernel matrix; bias_constraint : constraint applied to the bias vector. [source]","title":"GlobalAttentionPool"},{"location":"layers/pooling/#globalattnsumpool","text":"spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. Mode : single, mixed, batch, graph batch. Input node features of shape (n_nodes, n_features) (with optional batch dimension); (optional) graph IDs of shape (n_nodes, ) (graph batch mode); Output tensor like node features, but without node dimension (except for single mode, where the node dimension is preserved and set to 1). Arguments attn_kernel_initializer : initializer for the attention kernel matrix; kernel_regularizer : regularization applied to the kernel matrix; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix;","title":"GlobalAttnSumPool"},{"location":"utils/conversion/","text":"nx_to_adj spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices. nx_to_node_features spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_edge_features spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices nx_to_numpy spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) numpy_to_nx spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"Conversion"},{"location":"utils/conversion/#nx_to_adj","text":"spektral.utils.nx_to_adj(graphs) Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices of shape (num_graphs, num_nodes, num_nodes) . Arguments graphs : a nx.Graph, or list of nx.Graphs. Return A rank 3 np.array of adjacency matrices.","title":"nx_to_adj"},{"location":"utils/conversion/#nx_to_node_features","text":"spektral.utils.nx_to_node_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 3 np.array of node features matrices of shape (num_graphs, num_nodes, num_features) . Optionally applies a post-processing function to each individual attribute in the nx Graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index node attributes in the nx Graphs. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_node_features"},{"location":"utils/conversion/#nx_to_edge_features","text":"spektral.utils.nx_to_edge_features(graphs, keys, post_processing=None) Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices of shape (num_graphs, num_nodes, num_nodes, num_features) . Optionally applies a post-processing function to each attribute in the nx graphs. Arguments graphs : a nx.Graph, or a list of nx.Graphs; keys : a list of keys with which to index edge attributes. post_processing : a list of functions with which to post process each attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return A rank 3 np.array of feature matrices","title":"nx_to_edge_features"},{"location":"utils/conversion/#nx_to_numpy","text":"spektral.utils.nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None, ef_keys=None, nf_postprocessing=None, ef_postprocessing=None) Converts a list of nx.Graphs to numpy format (adjacency, node attributes, and edge attributes matrices). Arguments graphs : a nx.Graph, or list of nx.Graphs; auto_pad : whether to zero-pad all matrices to have graphs with the same dimension (set this to true if you don't want to deal with manual batching for different-size graphs. self_loops : whether to add self-loops to the graphs. nf_keys : a list of keys with which to index node attributes. If None, returns None as node attributes matrix. ef_keys : a list of keys with which to index edge attributes. If None, returns None as edge attributes matrix. nf_postprocessing : a list of functions with which to post process each node attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. ef_postprocessing : a list of functions with which to post process each edge attribute associated to a key. None can be passed as post-processing function to leave the attribute unchanged. Return adjacency matrices of shape (num_samples, num_nodes, num_nodes) node attributes matrices of shape (num_samples, num_nodes, node_features_dim) edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim)","title":"nx_to_numpy"},{"location":"utils/conversion/#numpy_to_nx","text":"spektral.utils.numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None) Converts graphs in numpy format to a list of nx.Graphs. Arguments adj : adjacency matrices of shape (num_samples, num_nodes, num_nodes) . If there is only one sample, the first dimension can be dropped. node_features : optional node attributes matrices of shape (num_samples, num_nodes, node_features_dim) . If there is only one sample, the first dimension can be dropped. edge_features : optional edge attributes matrices of shape (num_samples, num_nodes, num_nodes, edge_features_dim) If there is only one sample, the first dimension can be dropped. nf_name : optional name to assign to node attributes in the nx.Graphs ef_name : optional name to assign to edge attributes in the nx.Graphs Return A list of nx.Graphs (or a single nx.Graph is there is only one sample)","title":"numpy_to_nx"},{"location":"utils/convolution/","text":"degree spektral.utils.degree(adj) Computes the degree matrix of the given adjacency matrix. Arguments adj : rank 2 array or sparse matrix Return The degree matrix in sparse DIA format degree_power spektral.utils.degree_power(adj, pow) Computes D^{p} from the given adjacency matrix. Useful for computing normalised Laplacians. Arguments adj : rank 2 array or sparse matrix pow : exponent to which elevate the degree matrix Return The exponentiated degree matrix in sparse DIA format normalized_adjacency spektral.utils.normalized_adjacency(adj, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either D^{-1}A or D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix. laplacian spektral.utils.laplacian(adj) Computes the Laplacian of the given adjacency matrix as D - A . Arguments adj : rank 2 array or sparse matrix; Return The Laplacian. normalized_laplacian spektral.utils.normalized_laplacian(adj, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as I - D^{-1}A or I - D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian. localpooling_filter spektral.utils.localpooling_filter(adj, symmetric=True) Computes the local pooling filter from the given adjacency matrix, as described by Kipf & Welling (2017). Arguments adj : a np.array or scipy.sparse matrix of rank 2 or 3; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return The filter matrix, as dense np.array. chebyshev_filter spektral.utils.chebyshev_filter(adj, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016). Arguments adj : a np.array or scipy.sparse matrix; k : integer, the order up to which to compute the Chebyshev polynomials; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return A list of k+1 filter matrices, as np.arrays.","title":"Convolution"},{"location":"utils/convolution/#degree","text":"spektral.utils.degree(adj) Computes the degree matrix of the given adjacency matrix. Arguments adj : rank 2 array or sparse matrix Return The degree matrix in sparse DIA format","title":"degree"},{"location":"utils/convolution/#degree_power","text":"spektral.utils.degree_power(adj, pow) Computes D^{p} from the given adjacency matrix. Useful for computing normalised Laplacians. Arguments adj : rank 2 array or sparse matrix pow : exponent to which elevate the degree matrix Return The exponentiated degree matrix in sparse DIA format","title":"degree_power"},{"location":"utils/convolution/#normalized_adjacency","text":"spektral.utils.normalized_adjacency(adj, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either D^{-1}A or D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix.","title":"normalized_adjacency"},{"location":"utils/convolution/#laplacian","text":"spektral.utils.laplacian(adj) Computes the Laplacian of the given adjacency matrix as D - A . Arguments adj : rank 2 array or sparse matrix; Return The Laplacian.","title":"laplacian"},{"location":"utils/convolution/#normalized_laplacian","text":"spektral.utils.normalized_laplacian(adj, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as I - D^{-1}A or I - D^{-1/2}AD^{-1/2} (symmetric normalization). Arguments adj : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian.","title":"normalized_laplacian"},{"location":"utils/convolution/#localpooling_filter","text":"spektral.utils.localpooling_filter(adj, symmetric=True) Computes the local pooling filter from the given adjacency matrix, as described by Kipf & Welling (2017). Arguments adj : a np.array or scipy.sparse matrix of rank 2 or 3; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return The filter matrix, as dense np.array.","title":"localpooling_filter"},{"location":"utils/convolution/#chebyshev_filter","text":"spektral.utils.chebyshev_filter(adj, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016). Arguments adj : a np.array or scipy.sparse matrix; k : integer, the order up to which to compute the Chebyshev polynomials; symmetric : boolean, whether to normalize the matrix as D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} or as D^{-1}A ; Return A list of k+1 filter matrices, as np.arrays.","title":"chebyshev_filter"},{"location":"utils/data/","text":"[source] Batch spektral.utils.Batch(A_list, X_list) Converts a batch of graphs stored in lists to the graph batch format . Input A_list , list of adjacency matrices of shape (N, N) ; X_list , list of node attributes matrices of shape (N, F) ; Properties A : returns the block diagonal adjacency matrix; X : returns the stacked node attributes; I : returns the graph indices mapping each node to a graph (numbering is relative to the batch). Usage In [1]: from spektral.utils.data import Batch Using TensorFlow backend. In [2]: A_list = [np.ones((2, 2))] * 3 In [3]: X_list = [np.random.normal(size=(2, 4))] * 3 In [4]: b = Batch(A_list, X_list) In [5]: b.A.todense() Out[5]: matrix([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1.]]) In [6]: b.X Out[6]: array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]) In [7]: b.I Out[7]: array([0, 0, 1, 1, 2, 2]) In [8]: b.get('AXI') Out[8]: (<6x6 sparse matrix of type '<class 'numpy.float64'>' with 12 stored elements in COOrdinate format>, array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]), array([0, 0, 1, 1, 2, 2])) get spektral.utils.get(attr) Splits a strings and returns the associated matrices in order, e.g., data = b.get('AXI') is equivalent to data = b.A, b.X, b.I . Arguments attr : a string (possible literals can be seen with b.attr_map.keys() . Return A tuple with the requested matrices.","title":"Data"},{"location":"utils/data/#batch","text":"spektral.utils.Batch(A_list, X_list) Converts a batch of graphs stored in lists to the graph batch format . Input A_list , list of adjacency matrices of shape (N, N) ; X_list , list of node attributes matrices of shape (N, F) ; Properties A : returns the block diagonal adjacency matrix; X : returns the stacked node attributes; I : returns the graph indices mapping each node to a graph (numbering is relative to the batch). Usage In [1]: from spektral.utils.data import Batch Using TensorFlow backend. In [2]: A_list = [np.ones((2, 2))] * 3 In [3]: X_list = [np.random.normal(size=(2, 4))] * 3 In [4]: b = Batch(A_list, X_list) In [5]: b.A.todense() Out[5]: matrix([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 1., 1., 0., 0.], [0., 0., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1.]]) In [6]: b.X Out[6]: array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]) In [7]: b.I Out[7]: array([0, 0, 1, 1, 2, 2]) In [8]: b.get('AXI') Out[8]: (<6x6 sparse matrix of type '<class 'numpy.float64'>' with 12 stored elements in COOrdinate format>, array([[-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459], [-0.85196709, -1.66795384, -1.14046868, 0.4735151 ], [ 0.14221143, -0.76473164, -1.05635638, 1.45961459]]), array([0, 0, 1, 1, 2, 2]))","title":"Batch"},{"location":"utils/data/#get","text":"spektral.utils.get(attr) Splits a strings and returns the associated matrices in order, e.g., data = b.get('AXI') is equivalent to data = b.A, b.X, b.I . Arguments attr : a string (possible literals can be seen with b.attr_map.keys() . Return A tuple with the requested matrices.","title":"get"},{"location":"utils/io/","text":"load_binary spektral.utils.load_binary(filename) Loads a pickled file. Arguments filename : a string or file-like object Return The loaded object dump_binary spektral.utils.dump_binary(obj, filename) Pickles and saves an object to file. Arguments obj : the object to save filename : a string or file-like object load_csv spektral.utils.load_csv(filename) Loads a csv file with pandas. Arguments filename : a string or file-like object Return The loaded csv dump_csv spektral.utils.dump_csv(df, filename, convert=False) Dumps a pd.DataFrame to csv. Arguments df : the pd.DataFrame to save or equivalent object filename : a string or file-like object convert : whether to attempt to convert the given object to pd.DataFrame before saving the csv. load_dot spektral.utils.load_dot(filename, force_graph=True) Loads a graph saved in .dot format. Arguments filename : a string or file-like object force_graph : whether to force a conversion to nx.Graph after loading. This may be useful in the case of .dot files being loaded as nx.MultiGraph. Return The loaded graph dump_dot spektral.utils.dump_dot(obj, filename) Dumps a nx.Graph to .dot file Arguments obj : the nx.Graph (or equivalent) to save filename : a string or file-like object load_npy spektral.utils.load_npy(filename) Loads a file saved by np.save. Arguments filename : a string or file-like object Return The loaded object dump_npy spektral.utils.dump_npy(obj, filename, zipped=False) Saves an object to file using the numpy format. Arguments obj : the object to save filename : a string or file-like object zipped : boolean, whether to save the object in the zipped format .npz rather than .npy load_txt spektral.utils.load_txt(filename) Loads a txt file using np.genfromtxt. Arguments filename : a string or file-like object Return The loaded object dump_txt spektral.utils.dump_txt(obj, filename) Saves an object to text file using np.savetxt. Arguments obj : the object to save filename : a string or file-like object load_sdf spektral.utils.load_sdf(filename, amount=None) Load an .sdf file and return a list of molecules in the internal SDF format. Arguments filename : target SDF file amount : only load the first amount molecules from the file Return A list of molecules in the internal SDF format (see documentation).","title":"Input/Ouput"},{"location":"utils/io/#load_binary","text":"spektral.utils.load_binary(filename) Loads a pickled file. Arguments filename : a string or file-like object Return The loaded object","title":"load_binary"},{"location":"utils/io/#dump_binary","text":"spektral.utils.dump_binary(obj, filename) Pickles and saves an object to file. Arguments obj : the object to save filename : a string or file-like object","title":"dump_binary"},{"location":"utils/io/#load_csv","text":"spektral.utils.load_csv(filename) Loads a csv file with pandas. Arguments filename : a string or file-like object Return The loaded csv","title":"load_csv"},{"location":"utils/io/#dump_csv","text":"spektral.utils.dump_csv(df, filename, convert=False) Dumps a pd.DataFrame to csv. Arguments df : the pd.DataFrame to save or equivalent object filename : a string or file-like object convert : whether to attempt to convert the given object to pd.DataFrame before saving the csv.","title":"dump_csv"},{"location":"utils/io/#load_dot","text":"spektral.utils.load_dot(filename, force_graph=True) Loads a graph saved in .dot format. Arguments filename : a string or file-like object force_graph : whether to force a conversion to nx.Graph after loading. This may be useful in the case of .dot files being loaded as nx.MultiGraph. Return The loaded graph","title":"load_dot"},{"location":"utils/io/#dump_dot","text":"spektral.utils.dump_dot(obj, filename) Dumps a nx.Graph to .dot file Arguments obj : the nx.Graph (or equivalent) to save filename : a string or file-like object","title":"dump_dot"},{"location":"utils/io/#load_npy","text":"spektral.utils.load_npy(filename) Loads a file saved by np.save. Arguments filename : a string or file-like object Return The loaded object","title":"load_npy"},{"location":"utils/io/#dump_npy","text":"spektral.utils.dump_npy(obj, filename, zipped=False) Saves an object to file using the numpy format. Arguments obj : the object to save filename : a string or file-like object zipped : boolean, whether to save the object in the zipped format .npz rather than .npy","title":"dump_npy"},{"location":"utils/io/#load_txt","text":"spektral.utils.load_txt(filename) Loads a txt file using np.genfromtxt. Arguments filename : a string or file-like object Return The loaded object","title":"load_txt"},{"location":"utils/io/#dump_txt","text":"spektral.utils.dump_txt(obj, filename) Saves an object to text file using np.savetxt. Arguments obj : the object to save filename : a string or file-like object","title":"dump_txt"},{"location":"utils/io/#load_sdf","text":"spektral.utils.load_sdf(filename, amount=None) Load an .sdf file and return a list of molecules in the internal SDF format. Arguments filename : target SDF file amount : only load the first amount molecules from the file Return A list of molecules in the internal SDF format (see documentation).","title":"load_sdf"},{"location":"utils/misc/","text":"batch_iterator spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with equal first dimension. batch_size : number of samples in a batch epochs : number of times to iterate over the data shuffle : whether to shuffle the data at the beginning of each epoch :yield: a batch of samples (or tuple of batches if X had more than one array). set_trainable spektral.utils.set_trainable(model, toset) Sets the trainable parameters of a Keras model and all its layers to toset. Arguments model : a Keras Model toset : boolean Return None pad_jagged_array spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape add_eye spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above sub_eye spektral.utils.sub_eye(x) Subtracts the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above","title":"Miscellaneous"},{"location":"utils/misc/#batch_iterator","text":"spektral.utils.batch_iterator(data, batch_size=32, epochs=1, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with equal first dimension. batch_size : number of samples in a batch epochs : number of times to iterate over the data shuffle : whether to shuffle the data at the beginning of each epoch :yield: a batch of samples (or tuple of batches if X had more than one array).","title":"batch_iterator"},{"location":"utils/misc/#set_trainable","text":"spektral.utils.set_trainable(model, toset) Sets the trainable parameters of a Keras model and all its layers to toset. Arguments model : a Keras Model toset : boolean Return None","title":"set_trainable"},{"location":"utils/misc/#pad_jagged_array","text":"spektral.utils.pad_jagged_array(x, target_shape, dtype=<class 'float'>) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a np.array of dtype object, containing np.arrays of varying dimensions target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched (note that the creation of the output array may fail if the result is again a jagged array). dtype : the dtype of the returned np.array Return A zero-padded np.array of shape (X.shape[0], ) + target_shape","title":"pad_jagged_array"},{"location":"utils/misc/#add_eye","text":"spektral.utils.add_eye(x) Adds the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above","title":"add_eye"},{"location":"utils/misc/#sub_eye","text":"spektral.utils.sub_eye(x) Subtracts the identity matrix to the given matrix. Arguments x : a rank 2 np.array or scipy.sparse matrix Return A rank 2 np.array as described above","title":"sub_eye"},{"location":"utils/plotting/","text":"plot_numpy spektral.utils.plot_numpy(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Converts a graph in matrix format (i.e. with adjacency matrix, node features matrix, and edge features matrix) to the Networkx format, then plots it with plot_nx(). Arguments adj : np.array, adjacency matrix of the graph node_features : np.array, node features matrix of the graph edge_features : np.array, edge features matrix of the graph nf_name : name to assign to the node features ef_name : name to assign to the edge features layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None plot_nx spektral.utils.plot_nx(nx_graph, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Plot the given Networkx graph. Arguments nx_graph : a Networkx graph nf_name : name of the relevant node feature to plot ef_name : name of the relevant edgee feature to plot layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None","title":"Plotting"},{"location":"utils/plotting/#plot_numpy","text":"spektral.utils.plot_numpy(adj, node_features=None, edge_features=None, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Converts a graph in matrix format (i.e. with adjacency matrix, node features matrix, and edge features matrix) to the Networkx format, then plots it with plot_nx(). Arguments adj : np.array, adjacency matrix of the graph node_features : np.array, node features matrix of the graph edge_features : np.array, edge features matrix of the graph nf_name : name to assign to the node features ef_name : name to assign to the edge features layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None","title":"plot_numpy"},{"location":"utils/plotting/#plot_nx","text":"spektral.utils.plot_nx(nx_graph, nf_name=None, ef_name=None, layout='spring_layout', labels=True, node_color='r', node_size=300) Plot the given Networkx graph. Arguments nx_graph : a Networkx graph nf_name : name of the relevant node feature to plot ef_name : name of the relevant edgee feature to plot layout : type of layout for networkx labels : plot labels node_color : color for the plotted nodes node_size : size of the plotted nodes Return None","title":"plot_nx"}]}