<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Daniele Grattarola">
  <link rel="canonical" href="https://graphneural.network/layers/pooling/">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Pooling Layers - Spektral</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../../stylesheets/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Pooling Layers";
    var mkdocs_page_input_path = "layers/pooling.md";
    var mkdocs_page_url = "/layers/pooling/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-125823175-1', 'auto');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Spektral</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Tutorials</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../getting-started/">Getting started</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../data/">Data representation</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../examples/">Examples</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Layers</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../convolution/">Convolutional Layers</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Pooling Layers</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#diffpool">DiffPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mincutpool">MinCutPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#topkpool">TopKPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#sagpool">SAGPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#globalsumpool">GlobalSumPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#globalavgpool">GlobalAvgPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#globalmaxpool">GlobalMaxPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#globalattentionpool">GlobalAttentionPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#globalattnsumpool">GlobalAttnSumPool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#sortpool">SortPool</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../base/">Base Layers</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Data</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../datasets/">Datasets</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../utils/data/">Data utils</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Utils</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../utils/convolution/">Convolution</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../utils/misc/">Miscellaneous</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../utils/conversion/">Conversion</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../chem/">Chemistry</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Other</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Spektral</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Pooling Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/danielegrattarola/spektral/edit/master/docs/sources/layers/pooling.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="pooling-layers">Pooling layers</h2>
<p>The pooling layers from these papers are available in Spektral:</p>
<ul>
<li><a href="https://arxiv.org/abs/1806.08804">Hierarchical Graph Representation Learning with Differentiable Pooling</a></li>
<li><a href="https://arxiv.org/abs/1907.00481">Mincut pooling in Graph Neural Networks</a></li>
<li><a href="http://proceedings.mlr.press/v97/gao19a/gao19a.pdf">Graph U-Nets</a></li>
<li><a href="https://arxiv.org/abs/1904.08082">Self-Attention Graph Pooling</a></li>
<li><a href="https://arxiv.org/abs/1511.05493">Gated Graph Sequence Neural Networks</a></li>
</ul>
<p>Additionally, sum, average, and max global pooling are implemented, as well as 
a simple global weighted sum pooling where weights are calculated with an 
attention mechanism. </p>
<p>See <a href="/layers/convolution">the convolutional layers page</a> for the notation. </p>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/diff_pool.py#L10">[source]</a></span></p>
<h4 id="diffpool">DiffPool</h4>
<pre><code class="python">spektral.layers.DiffPool(k, channels=None, return_mask=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None)
</code></pre>

<p>A DiffPool layer as presented by
<a href="https://arxiv.org/abs/1806.08804">Ying et al. (2018)</a>.</p>
<p><strong>Mode</strong>: batch.</p>
<p>This layer computes a soft clustering <script type="math/tex">\S</script> of the input graphs using a GNN,
and reduces graphs as follows:</p>
<p>
<script type="math/tex; mode=display">
\S = \textrm{GNN}(\A, \X); \\
\A' = \S^\top \A \S; \X' = \S^\top \X;
</script>
</p>
<p>where GNN consists of one GraphConv layer with softmax activation.
Two auxiliary loss terms are also added to the model: the <em>link prediction
loss</em>
<script type="math/tex; mode=display">
\big\| \A - \S\S^\top \big\|_F
</script>
and the <em>entropy loss</em>
<script type="math/tex; mode=display">
- \frac{1}{N} \sum\limits_{i = 1}^{N} \S \log (\S).
</script>
</p>
<p>The layer also applies a 1-layer GCN to the input features, and returns
the updated graph signal (the number of output channels is controlled by
the <code>channels</code> parameter).
The layer can be used without a supervised loss, to compute node clustering
simply by minimizing the two auxiliary losses.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>([batch], N, N)</code>;</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Reduced node features of shape <code>([batch], K, channels)</code>;</li>
<li>Reduced adjacency matrix of shape <code>([batch], K, K)</code>;</li>
<li>If <code>return_mask=True</code>, the soft clustering matrix of shape <code>([batch], N, K)</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>k</code>: number of nodes to keep;</li>
<li><code>channels</code>: number of output channels (if None, the number of output
channels is assumed to be the same as the input);</li>
<li><code>return_mask</code>: boolean, whether to return the cluster assignment matrix;</li>
<li><code>kernel_initializer</code>: initializer for the weights;</li>
<li><code>kernel_regularizer</code>: regularization applied to the weights;</li>
<li><code>kernel_constraint</code>: constraint applied to the weights;</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/mincut_pool.py#L9">[source]</a></span></p>
<h4 id="mincutpool">MinCutPool</h4>
<pre><code class="python">spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_mask=False, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A minCUT pooling layer as presented by
<a href="https://arxiv.org/abs/1907.00481">Bianchi et al. (2019)</a>.</p>
<p><strong>Mode</strong>: batch.</p>
<p>This layer computes a soft clustering <script type="math/tex">\S</script> of the input graphs using a MLP,
and reduces graphs as follows:</p>
<p>
<script type="math/tex; mode=display">
\S = \textrm{MLP}(\X); \\
\A' = \S^\top \A \S; \X' = \S^\top \X;
</script>
</p>
<p>where MLP is a multi-layer perceptron with softmax output.
Two auxiliary loss terms are also added to the model: the <em>minCUT loss</em>
<script type="math/tex; mode=display">
- \frac{ \mathrm{Tr}(\S^\top \A \S) }{ \mathrm{Tr}(\S^\top \D \S) }
</script>
and the <em>orthogonality loss</em>
<script type="math/tex; mode=display">
\left\|
\frac{\S^\top \S}{\| \S^\top \S \|_F}
- \frac{\I_K}{\sqrt{K}}
\right\|_F.
</script>
</p>
<p>The layer can be used without a supervised loss, to compute node clustering
simply by minimizing the two auxiliary losses.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>([batch], N, N)</code>;</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Reduced node features of shape <code>([batch], K, F)</code>;</li>
<li>Reduced adjacency matrix of shape <code>([batch], K, K)</code>;</li>
<li>If <code>return_mask=True</code>, the soft clustering matrix of shape <code>([batch], N, K)</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>k</code>: number of nodes to keep;</li>
<li><code>mlp_hidden</code>: list of integers, number of hidden units for each hidden
layer in the MLP used to compute cluster assignments (if None, the MLP has
only the output layer);</li>
<li><code>mlp_activation</code>: activation for the MLP layers;</li>
<li><code>return_mask</code>: boolean, whether to return the cluster assignment matrix;</li>
<li><code>kernel_initializer</code>: initializer for the weights;</li>
<li><code>kernel_regularizer</code>: regularization applied to the weights;</li>
<li><code>kernel_constraint</code>: constraint applied to the weights;</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/topk_pool.py#L8">[source]</a></span></p>
<h4 id="topkpool">TopKPool</h4>
<pre><code class="python">spektral.layers.TopKPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None)
</code></pre>

<p>A gPool/Top-K layer as presented by
<a href="http://proceedings.mlr.press/v97/gao19a/gao19a.pdf">Gao &amp; Ji (2019)</a> and
<a href="https://arxiv.org/abs/1811.01287">Cangea et al. (2018)</a>.</p>
<p><strong>Mode</strong>: single, disjoint.</p>
<p>This layer computes the following operations:</p>
<p>
<script type="math/tex; mode=display">
\y = \frac{\X\p}{\|\p\|}; \;\;\;\;
\i = \textrm{rank}(\y, K); \;\;\;\;
\X' = (\X \odot \textrm{tanh}(\y))_\i; \;\;\;\;
\A' = \A_{\i, \i}
</script>
</p>
<p>where <script type="math/tex"> \textrm{rank}(\y, K) </script> returns the indices of the top K values of
<script type="math/tex">\y</script>, and <script type="math/tex">\p</script> is a learnable parameter vector of size <script type="math/tex">F</script>. <script type="math/tex">K</script> is
defined for each graph as a fraction of the number of nodes.
Note that the the gating operation <script type="math/tex">\textrm{tanh}(\y)</script> (Cangea et al.)
can be replaced with a sigmoid (Gao &amp; Ji).</p>
<p>This layer temporarily makes the adjacency matrix dense in order to compute
<script type="math/tex">\A'</script>.
If memory is not an issue, considerable speedups can be achieved by using
dense graphs directly.
Converting a graph from sparse to dense and back to sparse is an expensive
operation.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>(N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>(N, N)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Reduced node features of shape <code>(ratio * N, F)</code>;</li>
<li>Reduced adjacency matrix of shape <code>(ratio * N, ratio * N)</code>;</li>
<li>Reduced graph IDs of shape <code>(ratio * N, )</code> (only in disjoint mode);</li>
<li>If <code>return_mask=True</code>, the binary pooling mask of shape <code>(ratio * N, )</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>ratio</code>: float between 0 and 1, ratio of nodes to keep in each graph;</li>
<li><code>return_mask</code>: boolean, whether to return the binary mask used for pooling;</li>
<li><code>sigmoid_gating</code>: boolean, use a sigmoid gating activation instead of a
tanh;</li>
<li><code>kernel_initializer</code>: initializer for the weights;</li>
<li><code>kernel_regularizer</code>: regularization applied to the weights;</li>
<li><code>kernel_constraint</code>: constraint applied to the weights;</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/sag_pool.py#L6">[source]</a></span></p>
<h4 id="sagpool">SAGPool</h4>
<pre><code class="python">spektral.layers.SAGPool(ratio, return_mask=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None)
</code></pre>

<p>A self-attention graph pooling layer as presented by
<a href="https://arxiv.org/abs/1904.08082">Lee et al. (2019)</a>.</p>
<p><strong>Mode</strong>: single, disjoint.</p>
<p>This layer computes the following operations:</p>
<p>
<script type="math/tex; mode=display">
\y = \textrm{GNN}(\A, \X); \;\;\;\;
\i = \textrm{rank}(\y, K); \;\;\;\;
\X' = (\X \odot \textrm{tanh}(\y))_\i; \;\;\;\;
\A' = \A_{\i, \i}
</script>
</p>
<p>where <script type="math/tex"> \textrm{rank}(\y, K) </script> returns the indices of the top K values of
<script type="math/tex">\y</script>, and <script type="math/tex">\textrm{GNN}</script> consists of one GraphConv layer with no
activation. <script type="math/tex">K</script> is defined for each graph as a fraction of the number of
nodes.</p>
<p>This layer temporarily makes the adjacency matrix dense in order to compute
<script type="math/tex">\A'</script>.
If memory is not an issue, considerable speedups can be achieved by using
dense graphs directly.
Converting a graph from sparse to dense and back to sparse is an expensive
operation.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>(N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>(N, N)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Reduced node features of shape <code>(ratio * N, F)</code>;</li>
<li>Reduced adjacency matrix of shape <code>(ratio * N, ratio * N)</code>;</li>
<li>Reduced graph IDs of shape <code>(ratio * N, )</code> (only in disjoint mode);</li>
<li>If <code>return_mask=True</code>, the binary pooling mask of shape <code>(ratio * N, )</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>ratio</code>: float between 0 and 1, ratio of nodes to keep in each graph;</li>
<li><code>return_mask</code>: boolean, whether to return the binary mask used for pooling;</li>
<li><code>sigmoid_gating</code>: boolean, use a sigmoid gating activation instead of a
tanh;</li>
<li><code>kernel_initializer</code>: initializer for the weights;</li>
<li><code>kernel_regularizer</code>: regularization applied to the weights;</li>
<li><code>kernel_constraint</code>: constraint applied to the weights;</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/global_pool.py#L51">[source]</a></span></p>
<h4 id="globalsumpool">GlobalSumPool</h4>
<pre><code class="python">spektral.layers.GlobalSumPool()
</code></pre>

<p>A global sum pooling layer. Pools a graph by computing the sum of its node
features.</p>
<p><strong>Mode</strong>: single, mixed, batch, disjoint.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Pooled node features of shape <code>([batch], F)</code> (if single mode, shape will
be <code>(1, F)</code>).</li>
</ul>
<p><strong>Arguments</strong></p>
<p>None.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/global_pool.py#L80">[source]</a></span></p>
<h4 id="globalavgpool">GlobalAvgPool</h4>
<pre><code class="python">spektral.layers.GlobalAvgPool()
</code></pre>

<p>An average pooling layer. Pools a graph by computing the average of its node
features.</p>
<p><strong>Mode</strong>: single, mixed, batch, disjoint.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Pooled node features of shape <code>([batch], F)</code> (if single mode, shape will
be <code>(1, F)</code>).</li>
</ul>
<p><strong>Arguments</strong></p>
<p>None.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/global_pool.py#L109">[source]</a></span></p>
<h4 id="globalmaxpool">GlobalMaxPool</h4>
<pre><code class="python">spektral.layers.GlobalMaxPool()
</code></pre>

<p>A max pooling layer. Pools a graph by computing the maximum of its node
features.</p>
<p><strong>Mode</strong>: single, mixed, batch, disjoint.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Pooled node features of shape <code>([batch], F)</code> (if single mode, shape will
be <code>(1, F)</code>).</li>
</ul>
<p><strong>Arguments</strong></p>
<p>None.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/global_pool.py#L138">[source]</a></span></p>
<h4 id="globalattentionpool">GlobalAttentionPool</h4>
<pre><code class="python">spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A gated attention global pooling layer as presented by
<a href="https://arxiv.org/abs/1511.05493">Li et al. (2017)</a>.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\X' = \sum\limits_{i=1}^{N} (\sigma(\X \W_1 + \b_1) \odot (\X \W_2 + \b_2))_i
</script>
where <script type="math/tex">\sigma</script> is the sigmoid activation function.</p>
<p><strong>Mode</strong>: single, mixed, batch, disjoint.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Pooled node features of shape <code>([batch], channels)</code> (if single mode,
shape will be <code>(1, channels)</code>).</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: integer, number of output channels;</li>
<li><code>bias_initializer</code>: initializer for the bias vectors;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrices;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vectors;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrices;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vectors.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/global_pool.py#L250">[source]</a></span></p>
<h4 id="globalattnsumpool">GlobalAttnSumPool</h4>
<pre><code class="python">spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None)
</code></pre>

<p>A node-attention global pooling layer. Pools a graph by learning attention
coefficients to sum node features.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\alpha = \textrm{softmax}( \X \a); \\
\X' = \sum\limits_{i=1}^{N} \alpha_i \cdot \X_i
</script>
where <script type="math/tex">\a \in \mathbb{R}^F</script> is a trainable vector. Note that the softmax
is applied across nodes, and not across features.</p>
<p><strong>Mode</strong>: single, mixed, batch, disjoint.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Graph IDs of shape <code>(N, )</code> (only in disjoint mode);</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Pooled node features of shape <code>([batch], F)</code> (if single mode, shape will
be <code>(1, F)</code>).</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>attn_kernel_initializer</code>: initializer for the attention weights;</li>
<li><code>attn_kernel_regularizer</code>: regularization applied to the attention kernel
matrix;</li>
<li><code>attn_kernel_constraint</code>: constraint applied to the attention kernel
matrix;</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/pooling/global_pool.py#L345">[source]</a></span></p>
<h4 id="sortpool">SortPool</h4>
<pre><code class="python">spektral.layers.SortPool(k)
</code></pre>

<p>SortPool layer pooling the top <script type="math/tex">k</script> most relevant nodes as described by
<a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">Zhang et al.</a></p>
<p>This layers takes a graph signal <script type="math/tex">\mathbf{X}</script> and sorts the rows by the
elements of its last column. It then keeps the top <script type="math/tex">k</script> rows.
Should <script type="math/tex">\mathbf{X}</script> have less than <script type="math/tex">k</script> rows, it sorts and then adds rows
full of zeros until <script type="math/tex">\mathbf{X}</script> has <script type="math/tex">k</script> rows.</p>
<p><strong>Mode</strong>: single, batch.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Pooled node features of shape <code>([batch], k, F)</code>;</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>k</code>: number of nodes to keep;</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../base/" class="btn btn-neutral float-right" title="Base Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../convolution/" class="btn btn-neutral" title="Convolutional Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/danielegrattarola/spektral/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../convolution/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../base/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../js/macros.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
