<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Daniele Grattarola">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Convolutional Layers - Spektral</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="../../stylesheets/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Convolutional Layers";
    var mkdocs_page_input_path = "layers/convolution.md";
    var mkdocs_page_url = "/spektral/layers/convolution/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-125823175-1', 'auto');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Spektral</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../getting-started/">Getting started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../data/">Data representation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Convolutional Layers</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#graphconv">GraphConv</a></li>
    

    <li class="toctree-l3"><a href="#chebconv">ChebConv</a></li>
    

    <li class="toctree-l3"><a href="#graphsageconv">GraphSageConv</a></li>
    

    <li class="toctree-l3"><a href="#armaconv">ARMAConv</a></li>
    

    <li class="toctree-l3"><a href="#edgeconditionedconv">EdgeConditionedConv</a></li>
    

    <li class="toctree-l3"><a href="#graphattention">GraphAttention</a></li>
    

    <li class="toctree-l3"><a href="#graphconvskip">GraphConvSkip</a></li>
    

    <li class="toctree-l3"><a href="#appnp">APPNP</a></li>
    

    <li class="toctree-l3"><a href="#ginconv">GINConv</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../pooling/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../base/">Base Layers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Datasets</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../datasets/citation/">Citation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../datasets/delaunay/">Delaunay</a>
                </li>
                <li class="">
                    
    <a class="" href="../../datasets/graphsage/">GraphSage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../datasets/qm9/">QM9</a>
                </li>
                <li class="">
                    
    <a class="" href="../../datasets/mnist/">MNIST</a>
                </li>
                <li class="">
                    
    <a class="" href="../../datasets/tud/">TU Dortmund</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../chem/">Chemistry</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../brain/">Brain</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Utils</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../utils/data/">Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../utils/io/">Input/Ouput</a>
                </li>
                <li class="">
                    
    <a class="" href="../../utils/conversion/">Conversion</a>
                </li>
                <li class="">
                    
    <a class="" href="../../utils/convolution/">Convolution</a>
                </li>
                <li class="">
                    
    <a class="" href="../../utils/misc/">Miscellaneous</a>
                </li>
                <li class="">
                    
    <a class="" href="../../utils/plotting/">Plotting</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../about/">About</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Spektral</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Convolutional Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/danielegrattarola/spektral/edit/master/docs/sources/layers/convolution.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>The message-passing layers from these papers are available in Spektral:</p>
<ul>
<li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1606.09375">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a></li>
<li><a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a></li>
<li><a href="https://arxiv.org/abs/1704.02901">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs</a></li>
<li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></li>
<li><a href="https://arxiv.org/abs/1901.01343">Graph Neural Networks with convolutional ARMA filters</a></li>
<li><a href="https://arxiv.org/abs/1810.05997">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</a></li>
<li><a href="https://arxiv.org/abs/1810.00826">How Powerful are Graph Neural Networks?</a></li>
</ul>
<p>Notation:</p>
<ul>
<li>
<script type="math/tex"> N </script>: number of nodes in the graph;</li>
<li>
<script type="math/tex"> F </script>: dimension of the node attributes (i.e., each node has an attribute in <script type="math/tex"> \mathbb{R}^F </script>);</li>
<li>
<script type="math/tex"> S </script>: dimension of the edge attributes (i.e., each edge has an attribute in <script type="math/tex"> \mathbb{R}^S </script>);</li>
<li>
<script type="math/tex"> \A \in \{0, 1\}^{N \times N}</script>: binary adjacency matrix;</li>
<li>
<script type="math/tex"> \X \in \mathbb{R}^{ N \times F } </script>: node attributes matrix;</li>
<li>
<script type="math/tex"> \E \in \mathbb{R}^{ N \times N \times S } </script>: edge attributes matrix;</li>
<li>
<script type="math/tex"> \D = \textrm{diag} ( \sum\limits_{j=0} \A_{ij} )</script>: degree matrix;</li>
<li>
<script type="math/tex"> \W, \V </script>: trainable kernels;</li>
<li>
<script type="math/tex"> \b </script>: trainable bias vector;</li>
<li>
<script type="math/tex"> \mathcal{N}(i) </script>: the one-hop neighbourhood of node <script type="math/tex">i</script>; </li>
<li>
<script type="math/tex"> F' </script>: dimension of the node attributes after a message-passing layer;</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L10">[source]</a></span></p>
<h3 id="graphconv">GraphConv</h3>
<pre><code class="python">spektral.layers.GraphConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A graph convolutional layer (GCN) as presented by
<a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling (2016)</a>.</p>
<p><strong>Mode</strong>: single, mixed, batch.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\Z = \hat \D^{-1/2} \hat \A \hat \D^{-1/2} \X \W + \b
</script>
where <script type="math/tex"> \hat \A = \A + \I </script> is the adjacency matrix with added self-loops
and <script type="math/tex">\hat\D</script> is its degree matrix.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Modified Laplacian of shape <code>([batch], N, N)</code>; can be computed with
<code>spektral.utils.convolution.localpooling_filter</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape as the input, but with the last
dimension changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;  </li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L129">[source]</a></span></p>
<h3 id="chebconv">ChebConv</h3>
<pre><code class="python">spektral.layers.ChebConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A Chebyshev convolutional layer as presented by
<a href="https://arxiv.org/abs/1606.09375">Defferrard et al. (2016)</a>.</p>
<p><strong>Mode</strong>: single, mixed, batch.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\Z = \sum \limits_{k=1}^{K} \T_{k} \X \W  + \b,
</script>
where <script type="math/tex"> \T = [ \T_{1}, ..., \T_{K}] </script> is a list of Chebyshev polynomials
defined as
<script type="math/tex; mode=display">
\T_0 = \X \\
\T_1 = \tilde \L \X \\
\T_{k \ge 2} = 2 \cdot \tilde \L \T_{k-1} - \T_{k-2},
</script>
where
<script type="math/tex; mode=display">
\tilde \L =  \frac{2}{\lambda_{max}} \cdot (\I - \D^{-1/2} \A \D^{-1/2}) - \I
</script>
is the normalized Laplacian with a rescaled spectrum.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>A list of K Chebyshev polynomials of shape
<code>[([batch], N, N), ..., ([batch], N, N)]</code>; can be computed with
<code>spektral.utils.convolution.chebyshev_filter</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape of the input, but with the last
dimension changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: boolean, whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;  </li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L242">[source]</a></span></p>
<h3 id="graphsageconv">GraphSageConv</h3>
<pre><code class="python">spektral.layers.GraphSageConv(channels, aggregate_method='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A GraphSAGE layer as presented by
<a href="https://arxiv.org/abs/1706.02216">Hamilton et al. (2017)</a>.</p>
<p><strong>Mode</strong>: single.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\Z = \big[ \textrm{AGGREGATE}(\X) \| \X \big] \W + \b; \\
\Z = \frac{\Z}{\|\Z\|}
</script>
where <script type="math/tex"> \textrm{AGGREGATE} </script> is a function to aggregate a node's
neighbourhood. The supported aggregation methods are: sum, mean,
max, min, and product.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>(N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>(N, N)</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape as the input, but with the last
dimension changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>aggregate_method</code>: str, aggregation method to use (<code>'sum'</code>, <code>'mean'</code>,
<code>'max'</code>, <code>'min'</code>, <code>'prod'</code>);</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L363">[source]</a></span></p>
<h3 id="armaconv">ARMAConv</h3>
<pre><code class="python">spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A graph convolutional layer with ARMA<script type="math/tex">_K</script> filters, as presented by
<a href="https://arxiv.org/abs/1901.01343">Bianchi et al. (2019)</a>.</p>
<p><strong>Mode</strong>: single, mixed, batch.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\Z = \frac{1}{K} \sum\limits_{k=1}^K \bar\X_k^{(T)},
</script>
where <script type="math/tex">K</script> is the order of the ARMA<script type="math/tex">_K</script> filter, and where:
<script type="math/tex; mode=display">
\bar \X_k^{(t + 1)} =
\sigma \left(\tilde \L \bar \X^{(t)} \W^{(t)} + \X \V^{(t)} \right)
</script>
is a recursive approximation of an ARMA<script type="math/tex">_1</script> filter, where
<script type="math/tex"> \bar \X^{(0)} = \X </script>
and
<script type="math/tex; mode=display">
\tilde \L =  \frac{2}{\lambda_{max}} \cdot (\I - \D^{-1/2} \A \D^{-1/2}) - \I
</script>
is the normalized Laplacian with a rescaled spectrum.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Normalized and rescaled Laplacian of shape <code>([batch], N, N)</code>; can be
computed with <code>spektral.utils.convolution.normalized_laplacian</code> and
<code>spektral.utils.convolution.rescale_laplacian</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape as the input, but with the last
dimension changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>order</code>: order of the full ARMA<script type="math/tex">_K</script> filter, i.e., the number of parallel
stacks in the layer;</li>
<li><code>iterations</code>: number of iterations to compute each ARMA<script type="math/tex">_1</script> approximation;</li>
<li><code>share_weights</code>: share the weights in each ARMA<script type="math/tex">_1</script> stack.</li>
<li><code>gcn_activation</code>: activation function to use to compute each ARMA<script type="math/tex">_1</script>
stack;</li>
<li><code>dropout_rate</code>: dropout rate for skip connection;</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L667">[source]</a></span></p>
<h3 id="edgeconditionedconv">EdgeConditionedConv</h3>
<pre><code class="python">spektral.layers.EdgeConditionedConv(channels, kernel_network=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>An edge-conditioned convolutional layer (ECC) as presented by
<a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis (2017)</a>.</p>
<p><strong>Mode</strong>: single, batch.</p>
<p><strong>This layer expects dense inputs.</strong></p>
<p>For each node <script type="math/tex"> i </script>, this layer computes:
<script type="math/tex; mode=display">
\Z_i =  \frac{1}{\mathcal{N}(i)} \sum\limits_{j \in \mathcal{N}(i)} \textrm{MLP}(\E_{ji}) \X_{j} + \b
</script>
where <script type="math/tex">\textrm{MLP}</script> is a multi-layer perceptron that outputs the
convolutional kernel <script type="math/tex">\W</script> as a function of edge attributes.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Binary adjacency matrices with self-loops, of shape <code>([batch], N, N)</code>;</li>
<li>Edge features of shape <code>([batch], N, N, S)</code>;</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>node features with the same shape of the input, but the last dimension
changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: integer, number of output channels;</li>
<li><code>kernel_network</code>: a list of integers describing the hidden structure of
the kernel-generating network (i.e., the ReLU layers before the linear
output);</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: boolean, whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;  </li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L822">[source]</a></span></p>
<h3 id="graphattention">GraphAttention</h3>
<pre><code class="python">spektral.layers.GraphAttention(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None)
</code></pre>

<p>A graph attention layer (GAT) as presented by
<a href="https://arxiv.org/abs/1710.10903">Velickovic et al. (2017)</a>.</p>
<p><strong>Mode</strong>: single, mixed, batch.</p>
<p><strong>This layer expects dense inputs.</strong></p>
<p>This layer computes a convolution similar to <code>layers.GraphConv</code>, but
uses the attention mechanism to weight the adjacency matrix instead of
using the normalized Laplacian:
<script type="math/tex; mode=display">
\Z = \mathbf{\alpha}\X\W + \b
</script>
where
<script type="math/tex; mode=display">
\mathbf{\alpha}_{ij} =
\frac{
\exp\left(
\mathrm{LeakyReLU}\left(
\a^{\top} [(\X\W)_i \, \| \, (\X\W)_j]
\right)
\right)
}
{\sum\limits_{k \in \mathcal{N}(i) \cup \{ i \}}
\exp\left(
\mathrm{LeakyReLU}\left(
\a^{\top} [(\X\W)_i \, \| \, (\X\W)_k]
\right)
\right)
}
</script>
where <script type="math/tex">\a \in \mathbb{R}^{2F'}</script> is a trainable attention kernel.
Dropout is also applied to <script type="math/tex">\alpha</script> before computing <script type="math/tex">\Z</script>.
Parallel attention heads are computed in parallel and their results are
aggregated by concatenation or average.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>([batch], N, N)</code>;</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape as the input, but with the last
dimension changed to <code>channels</code>;</li>
<li>if <code>return_attn_coef=True</code>, a list with the attention coefficients for
each attention head. Each attention coefficient matrix has shape
<code>([batch], N, N)</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>attn_heads</code>: number of attention heads to use;</li>
<li><code>concat_heads</code>: bool, whether to concatenate the output of the attention
heads instead of averaging;</li>
<li><code>dropout_rate</code>: internal dropout rate for attention coefficients;</li>
<li><code>return_attn_coef</code>: if True, return the attention coefficients for
the given input (one N x N matrix for each head).</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>attn_kernel_initializer</code>: initializer for the attention kernels;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;</li>
<li><code>attn_kernel_regularizer</code>: regularization applied to the attention kernels;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>attn_kernel_constraint</code>: constraint applied to the attention kernels;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L1071">[source]</a></span></p>
<h3 id="graphconvskip">GraphConvSkip</h3>
<pre><code class="python">spektral.layers.GraphConvSkip(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A simple convolutional layer with a skip connection.</p>
<p><strong>Mode</strong>: single, mixed, batch.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\Z = \D^{-1/2} \A \D^{-1/2} \X \W_1 + \X \W_2 + \b
</script>
where <script type="math/tex"> \A </script> does not have self-loops (unlike in GraphConv).</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Normalized adjacency matrix of shape <code>([batch], N, N)</code>; can be computed
with <code>spektral.utils.convolution.normalized_adjacency</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape as the input, but with the last
dimension changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L1177">[source]</a></span></p>
<h3 id="appnp">APPNP</h3>
<pre><code class="python">spektral.layers.APPNP(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A graph convolutional layer implementing the APPNP operator, as presented by
<a href="https://arxiv.org/abs/1810.05997">Klicpera et al. (2019)</a>.</p>
<p>This layer computes:
<script type="math/tex; mode=display">
\Z^{(0)} = \textrm{MLP}(\X); \\
\Z^{(K)} = (1 - \alpha) \hat \D^{-1/2} \hat \A \hat \D^{-1/2} \Z^{(K - 1)} +
\alpha \Z^{(0)},
</script>
where <script type="math/tex">\alpha</script> is the <em>teleport</em> probability and <script type="math/tex">\textrm{MLP}</script> is a
multi-layer perceptron.</p>
<p><strong>Mode</strong>: single, mixed, batch.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Modified Laplacian of shape <code>([batch], N, N)</code>; can be computed with
<code>spektral.utils.convolution.localpooling_filter</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape as the input, but with the last
dimension changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: number of output channels;</li>
<li><code>alpha</code>: teleport probability during propagation;</li>
<li><code>propagations</code>: number of propagation steps;</li>
<li><code>mlp_hidden</code>: list of integers, number of hidden units for each hidden
layer in the MLP (if None, the MLP has only one layer);</li>
<li><code>mlp_activation</code>: activation for the MLP layers;</li>
<li><code>dropout_rate</code>: dropout rate for Laplacian and MLP layers;</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional.py#L1349">[source]</a></span></p>
<h3 id="ginconv">GINConv</h3>
<pre><code class="python">spektral.layers.GINConv(channels, mlp_channels=16, n_hidden_layers=0, epsilon=None, mlp_activation='relu', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>A Graph Isomorphism Network (GIN) as presented by
<a href="https://arxiv.org/abs/1810.00826">Xu et al. (2018)</a>.</p>
<p><strong>Mode</strong>: single.</p>
<p><strong>This layer expects sparse inputs.</strong></p>
<p>This layer computes for each node <script type="math/tex">i</script>:
<script type="math/tex; mode=display">
\Z_i = \textrm{MLP}\big( (1 + \epsilon) \cdot \X_i + \sum\limits_{j \in \mathcal{N}(i)} \X_j \big)
</script>
where <script type="math/tex">\textrm{MLP}</script> is a multi-layer perceptron.</p>
<p><strong>Input</strong></p>
<ul>
<li>Node features of shape <code>([batch], N, F)</code>;</li>
<li>Binary adjacency matrix of shape <code>([batch], N, N)</code>.</li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>Node features with the same shape of the input, but the last dimension
changed to <code>channels</code>.</li>
</ul>
<p><strong>Arguments</strong></p>
<ul>
<li><code>channels</code>: integer, number of output channels;</li>
<li><code>mlp_channels</code>: integer, number of channels in the inner MLP;</li>
<li><code>n_hidden_layers</code>: integer, number of hidden layers in the MLP (default 0)</li>
<li><code>epsilon</code>: unnamed parameter, see
<a href="https://arxiv.org/abs/1810.00826">Xu et al. (2018)</a>, and the equation above.
This parameter can be learned by setting <code>epsilon=None</code>, or it can be set
to a constant value, which is what happens by default (0). In practice, it
is safe to leave it to 0.</li>
<li><code>mlp_activation</code>: activation function for the MLP,</li>
<li><code>activation</code>: activation function to use;</li>
<li><code>use_bias</code>: whether to add a bias to the linear transformation;</li>
<li><code>kernel_initializer</code>: initializer for the kernel matrix;</li>
<li><code>bias_initializer</code>: initializer for the bias vector;</li>
<li><code>kernel_regularizer</code>: regularization applied to the kernel matrix;</li>
<li><code>bias_regularizer</code>: regularization applied to the bias vector;</li>
<li><code>activity_regularizer</code>: regularization applied to the output;</li>
<li><code>kernel_constraint</code>: constraint applied to the kernel matrix;</li>
<li><code>bias_constraint</code>: constraint applied to the bias vector.</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../pooling/" class="btn btn-neutral float-right" title="Pooling Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../examples/" class="btn btn-neutral" title="Examples"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/danielegrattarola/spektral/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../../examples/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../pooling/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../js/macros.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
